=============training from sratch============
Program ID: 20937

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt', '1', '/home/hxie1/data/OvarianCancerCT/pixelSize223/numpy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '0', '2']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
            8    at July 30th 03:00, 2019:
                    add learning rate print;
                    use convStride =2;
                    add filter number by 2 times along deeper layers.
            9    at July 30th, 10:13, 2019:
                    add MaxPool2d in stage1;
                    add final filters to 2048.
            10   at July 30th, 15:23, 2019
                    final conv layer filter number: 1024
            11   at Aug 10th, 2019:
                    A. Add new patient data; and exclude non-standard patient data;
                    B. test the k-th fold,  validation on the (k+1)th fold;
                    C. new inputsize: 231*251*251 with pixels size 3*2*2 mm
                    D. window level shresthold [0,300]
                    E. put data padding in to converting from nrrd to numpy;
                    F. Add STN network as first laye of network
                    G. change input data into gaussian normalization in slice by slice; (x-mean)/std;
            12   Aug 12th, 2019
                    A. fix the bug that SITk reads int imape, and normalization into interger error;
                    B  test without SPN.           
            13   Aug 13th, 2019
                    A change input data into Gausssian distribution with non-zero mean,
                      it will make the padding zero do not occupy meaning of gaussian distribution.        
                    B add filters in the ResNeXt network to 128 at first stage, and reduce batchSize to 3. 
                    C Add STN at the beginning of the ResNeXt network
            14   Aug 14th, 2019
                    A. Fix the Bug the STN convert all image into 0 problem;
                    B. change DataTransform: change shear into [-30, 30], and add scale[0.6,1.2]
                    C  put STN at teh begginning of the network;
                    D  change optimizer from SGD to Adam; and change learning rate decay with gamma=0.5 per 20 steps.                                             
            15   Aug 16th, 2019
                    A The affine matrix in the STN is divided by its spectral norm;
                    B All conv layer add spectralNorm, while all FC layer do not add spectral Norm;
                    C reduce initial LR at 0.0001, and decay step to 30.
            16   Aug 17th, 2019
                    A  Cancel STN;
                    B  Add LeakyReLU;
                    C  before final FC layer, add ReLU and LocalResponseNorm   
            17   Aug 17th, 2019:10:44am
                    A enable STN;
                    B STN support LeakyReLU and localResponseNorm  
            18   Aug 17th, 2019 14:38 pm
                    A  del STN at beginining;
                    B  put STN at final 2 layer before FC. 
            19  Aug 17th 18:09 2019
                    A add surgical result file support;                    
            20  Aug 18th 08:00 am 2019
                    A  change all bias in Conv2d, and Linear into True.   
            21  Aug 19th 10:33am 2019
                    A initalize the bias in the final Linear layer as 0.3; (1-2*0.65 = -0.3)
                    16:17 pm:
                    B  add inductive bias 0.3 in the network forward function. 
                    C  initial LR =0.1, with decay steps =30                          
            
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   total 220 patients, 5-fold cross validation, test 45, validation 45, and training 130.  

Training strategy: 

          

Program starting Time: 2019-08-20 10:23:31.937941
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt/20190820_102331

Info: this is the 0th fold leave for test in the 5-fold cross-validation.

Infor: In all data of 220 files, label 0 has 77 files,
	  and label 1 has 143 files, where positive response rate = 0.65 in full data
5-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.
training dataset:	 total 130 files, where 1 has 85.0 with rate of 0.6538461538461539
validation dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
test dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
Info: batchSize = 3

=== Network trains from scratch ====
Network has total 331,776,449 parameters.
This predict optimal response.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	LearningRate		TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	1.0000e-01		47.5183	0.6000	0.6471	0.5111		3.9467	0.3556	0.0000	1.0000		5.0632	0.3556	0.0000	1.0000
5	1.0000e-01		2.8389	0.5385	0.5529	0.5111		6.8097	0.3556	0.0000	1.0000		6.7137	0.3556	0.0000	1.0000
10	1.0000e-01		1.3772	0.5154	0.5059	0.5333		1.2272	0.6444	1.0000	0.0000		1.2188	0.6444	1.0000	0.0000
15	1.0000e-01		0.8093	0.5923	0.6000	0.5778		1.0344	0.6444	1.0000	0.0000		1.0337	0.6444	1.0000	0.0000
20	1.0000e-01		1.1422	0.5385	0.5412	0.5333		1.9123	0.6444	1.0000	0.0000		1.9078	0.6444	1.0000	0.0000
25	1.0000e-01		0.7408	0.5077	0.4824	0.5556		0.5259	0.6444	1.0000	0.0000		0.5260	0.6444	1.0000	0.0000
30	5.0000e-02		0.5617	0.4846	0.4824	0.4889		0.5004	0.6444	1.0000	0.0000		0.5009	0.6444	1.0000	0.0000
35	5.0000e-02		0.5800	0.5000	0.4588	0.5778		0.4883	0.6444	1.0000	0.0000		0.4882	0.6444	1.0000	0.0000
40	5.0000e-02		0.8414	0.5154	0.5059	0.5333		1.0760	0.6444	1.0000	0.0000		1.0810	0.6444	1.0000	0.0000
45	5.0000e-02		0.7422	0.4692	0.4706	0.4667		0.9492	0.6444	1.0000	0.0000		0.9529	0.6444	1.0000	0.0000
50	5.0000e-02		0.6748	0.5308	0.6000	0.4000		0.7783	0.3556	0.0000	1.0000		0.7767	0.3556	0.0000	1.0000
55	5.0000e-02		0.8532	0.5231	0.5059	0.5556		0.5135	0.3556	0.0000	1.0000		0.5134	0.3556	0.0000	1.0000
60	2.5000e-02		0.5571	0.4385	0.5176	0.2889		0.4875	0.6444	1.0000	0.0000		0.4874	0.6444	1.0000	0.0000
65	2.5000e-02		0.5846	0.5385	0.5529	0.5111		0.5919	0.3556	0.0000	1.0000		0.5912	0.3556	0.0000	1.0000
70	2.5000e-02		0.5716	0.5077	0.5294	0.4667		0.5313	0.3556	0.0000	1.0000		0.5308	0.3556	0.0000	1.0000
75	2.5000e-02		0.5188	0.4769	0.5176	0.4000		0.4877	0.6444	1.0000	0.0000		0.4876	0.6444	1.0000	0.0000
80	2.5000e-02		0.7364	0.4923	0.5176	0.4444		1.1969	0.3556	0.0000	1.0000		1.1974	0.3556	0.0000	1.0000
85	2.5000e-02		0.5186	0.6308	0.7294	0.4444		0.5122	0.3556	0.0000	1.0000		0.5104	0.3556	0.0000	1.0000
90	1.2500e-02		0.4898	0.5615	0.6941	0.3111		0.5180	0.3556	0.0000	1.0000		0.5176	0.3556	0.0000	1.0000
95	1.2500e-02		0.4987	0.4538	0.4000	0.5556		0.5590	0.3556	0.0000	1.0000		0.5591	0.3556	0.0000	1.0000
100	1.2500e-02		0.5158	0.5462	0.6000	0.4444		0.4970	0.6444	1.0000	0.0000		0.4971	0.6444	1.0000	0.0000
105	1.2500e-02		0.5224	0.4846	0.4588	0.5333		0.4892	0.3556	0.0000	1.0000		0.4894	0.3556	0.0000	1.0000
110	1.2500e-02		0.5674	0.5154	0.6118	0.3333		0.4986	0.6444	1.0000	0.0000		0.4987	0.6444	1.0000	0.0000
115	1.2500e-02		0.5231	0.4077	0.3529	0.5111		0.4886	0.6444	1.0000	0.0000		0.4886	0.6444	1.0000	0.0000
120	6.2500e-03		0.5194	0.5462	0.5765	0.4889		0.5850	0.3556	0.0000	1.0000		0.5850	0.3556	0.0000	1.0000
125	6.2500e-03		0.4851	0.5462	0.6118	0.4222		0.5524	0.3556	0.0000	1.0000		0.5536	0.3556	0.0000	1.0000
130	6.2500e-03		0.5130	0.4231	0.4353	0.4000		0.4886	0.6444	1.0000	0.0000		0.4886	0.6444	1.0000	0.0000
135	6.2500e-03		0.5133	0.4231	0.4353	0.4000		0.4885	0.6444	1.0000	0.0000		0.4884	0.6444	1.0000	0.0000
140	6.2500e-03		0.5151	0.5077	0.6588	0.2222		0.4881	0.6444	1.0000	0.0000		0.4880	0.6444	1.0000	0.0000
145	6.2500e-03		0.5052	0.4692	0.4471	0.5111		0.5073	0.3556	0.0000	1.0000		0.5075	0.3556	0.0000	1.0000
150	3.1250e-03		0.4970	0.4846	0.4941	0.4667		0.4882	0.3556	0.0000	1.0000		0.4881	0.3556	0.0000	1.0000
155	3.1250e-03		0.5005	0.5000	0.4588	0.5778		0.4904	0.6444	1.0000	0.0000		0.4903	0.6444	1.0000	0.0000
160	3.1250e-03		0.4878	0.5846	0.7294	0.3111		0.4977	0.3556	0.0000	1.0000		0.4976	0.3556	0.0000	1.0000
165	3.1250e-03		0.4881	0.4462	0.3647	0.6000		0.4996	0.6444	1.0000	0.0000		0.4995	0.6444	1.0000	0.0000
170	3.1250e-03		0.4918	0.5538	0.6353	0.4000		0.4880	0.3556	0.0000	1.0000		0.4880	0.3556	0.0000	1.0000
175	3.1250e-03		0.4918	0.5538	0.6706	0.3333		0.4911	0.3556	0.0000	1.0000		0.4910	0.3556	0.0000	1.0000
180	1.5625e-03		0.4891	0.4154	0.2706	0.6889		0.4933	0.6444	1.0000	0.0000		0.4933	0.6444	1.0000	0.0000
185	1.5625e-03		0.5160	0.4231	0.3882	0.4889		0.4879	0.6444	1.0000	0.0000		0.4879	0.6444	1.0000	0.0000
190	1.5625e-03		0.4906	0.5231	0.6706	0.2444		0.4877	0.6444	1.0000	0.0000		0.4876	0.6444	1.0000	0.0000
195	1.5625e-03		0.4923	0.4846	0.4824	0.4889		0.4886	0.6444	1.0000	0.0000		0.4885	0.6444	1.0000	0.0000
200	1.5625e-03		0.5112	0.4846	0.3765	0.6889		0.5083	0.6444	1.0000	0.0000		0.5086	0.6444	1.0000	0.0000
205	1.5625e-03		0.4908	0.4077	0.3176	0.5778		0.4875	0.6444	1.0000	0.0000		0.4875	0.6444	1.0000	0.0000
210	7.8125e-04		0.4902	0.3692	0.3176	0.4667		0.4876	0.3556	0.0000	1.0000		0.4875	0.3556	0.0000	1.0000
215	7.8125e-04		0.4927	0.5000	0.4941	0.5111		0.4909	0.6444	1.0000	0.0000		0.4909	0.6444	1.0000	0.0000
220	7.8125e-04		0.4854	0.5538	0.7294	0.2222		0.4870	0.3556	0.0000	1.0000		0.4869	0.3556	0.0000	1.0000
225	7.8125e-04		0.4905	0.4615	0.5647	0.2667		0.4874	0.3556	0.0000	1.0000		0.4872	0.3556	0.0000	1.0000
230	7.8125e-04		0.4845	0.5846	0.7765	0.2222		0.4870	0.3778	0.2759	0.5625		0.4869	0.5111	0.3448	0.8125
235	7.8125e-04		0.4894	0.6154	0.8941	0.0889		0.4875	0.3556	0.0000	1.0000		0.4874	0.3556	0.0000	1.0000
240	3.9063e-04		0.4837	0.4769	0.6235	0.2000		0.4871	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
245	3.9063e-04		0.4842	0.4308	0.2235	0.8222		0.4873	0.3556	0.0000	1.0000		0.4871	0.3556	0.0000	1.0000
250	3.9063e-04		0.4844	0.3692	0.3294	0.4444		0.4870	0.3556	0.0000	1.0000		0.4869	0.3556	0.0000	1.0000
255	3.9063e-04		0.4858	0.3462	0.0000	1.0000		0.4870	0.3556	0.0000	1.0000		0.4869	0.3556	0.0000	1.0000
260	3.9063e-04		0.4900	0.6538	1.0000	0.0000		0.4877	0.6444	1.0000	0.0000		0.4876	0.6444	1.0000	0.0000
265	3.9063e-04		0.4856	0.6538	1.0000	0.0000		0.4887	0.6444	1.0000	0.0000		0.4886	0.6444	1.0000	0.0000
270	1.9531e-04		0.4896	0.4385	0.4471	0.4222		0.4872	0.6444	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000
275	1.9531e-04		0.4848	0.6538	1.0000	0.0000		0.4875	0.6444	1.0000	0.0000		0.4875	0.6444	1.0000	0.0000
280	1.9531e-04		0.4829	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000
285	1.9531e-04		0.4888	0.6538	1.0000	0.0000		0.4876	0.6444	1.0000	0.0000		0.4875	0.6444	1.0000	0.0000
290	1.9531e-04		0.4844	0.6538	1.0000	0.0000		0.4875	0.6444	1.0000	0.0000		0.4874	0.6444	1.0000	0.0000
295	1.9531e-04		0.4854	0.6000	0.8706	0.0889		0.4870	0.3556	0.0000	1.0000		0.4868	0.3556	0.0000	1.0000
300	9.7656e-05		0.4875	0.6538	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000		0.4869	0.6444	1.0000	0.0000
305	9.7656e-05		0.4875	0.4692	0.5059	0.4000		0.4871	0.6444	1.0000	0.0000		0.4869	0.6444	1.0000	0.0000
310	9.7656e-05		0.4825	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
315	9.7656e-05		0.4825	0.6538	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
320	9.7656e-05		0.4879	0.6538	1.0000	0.0000		0.4873	0.6444	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000
325	9.7656e-05		0.4832	0.4846	0.5529	0.3556		0.4870	0.3556	0.0000	1.0000		0.4869	0.3556	0.0000	1.0000
330	4.8828e-05		0.4824	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
335	4.8828e-05		0.4823	0.6538	1.0000	0.0000		0.4873	0.6444	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000
340	4.8828e-05		0.4825	0.6385	0.9765	0.0000		0.4871	0.6444	1.0000	0.0000		0.4869	0.6444	1.0000	0.0000
345	4.8828e-05		0.4824	0.6538	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000		0.4869	0.6444	1.0000	0.0000
350	4.8828e-05		0.4824	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000
355	4.8828e-05		0.4878	0.6538	1.0000	0.0000		0.4873	0.6444	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000
360	2.4414e-05		0.4824	0.6615	1.0000	0.0222		0.4871	0.6444	1.0000	0.0000		0.4869	0.6444	1.0000	0.0000
365	2.4414e-05		0.4828	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000
370	2.4414e-05		0.4874	0.6538	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
375	2.4414e-05		0.4822	0.6462	0.9882	0.0000		0.4871	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
380	2.4414e-05		0.4822	0.6538	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
385	2.4414e-05		0.4823	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
390	1.2207e-05		0.4876	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000
395	1.2207e-05		0.4823	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
400	1.2207e-05		0.4822	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
405	1.2207e-05		0.4824	0.6538	1.0000	0.0000		0.4873	0.6444	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000
410	1.2207e-05		0.4823	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
415	1.2207e-05		0.4823	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4871	0.6444	1.0000	0.0000
420	6.1035e-06		0.4872	0.6615	1.0000	0.0222		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
425	6.1035e-06		0.4822	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
430	6.1035e-06		0.4820	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
435	6.1035e-06		0.4822	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
440	6.1035e-06		0.4821	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
445	6.1035e-06		0.4873	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
450	3.0518e-06		0.4822	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
455	3.0518e-06		0.4822	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
460	3.0518e-06		0.4822	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
465	3.0518e-06		0.4873	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
470	3.0518e-06		0.4821	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
475	3.0518e-06		0.4822	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
480	1.5259e-06		0.4822	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
485	1.5259e-06		0.4872	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
490	1.5259e-06		0.4873	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
495	1.5259e-06		0.4823	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
500	1.5259e-06		0.4821	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
505	1.5259e-06		0.4873	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
510	7.6294e-07		0.4823	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
515	7.6294e-07		0.4823	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
520	7.6294e-07		0.4822	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
525	7.6294e-07		0.4823	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4869	0.6444	1.0000	0.0000
530	7.6294e-07		0.4821	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
535	7.6294e-07		0.4872	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
540	3.8147e-07		0.4872	0.6538	1.0000	0.0000		0.4872	0.6444	1.0000	0.0000		0.4870	0.6444	1.0000	0.0000
