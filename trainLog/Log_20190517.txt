Program ID 3954

Program command: 
 ['/home/hxie1/Projects/OvarianCancer/TrainSegV.py', '/home/hxie1/temp_netParameters/OvarianCancer/Label0_1', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainImages', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainLabels', '2D', '0,1']

Major program changes: 
                       Restore May 1st 11:51 nework which got test dice 78% for primary
                       ConvDense uses Conv-Bn-ReLU order (CBR)
                       useSkip2Conv = 3 
                       output layer use 1*1 filter. 
                       use boundary loss with weight 0 at beginning, and pretrain CE loss. 
                       special convInput Module    
                       first layer filter = 96
                       use Mixup
                       Boundary Loss supports multi-class, and weight
                       Only  0,1 two classes clasfication for primary
                       Encoder do not use dropout, Decoder use dropout.
                       
                       
            

Program starting Time: 2019-05-17 11:36:32.119988
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/Label0_1

Infor: program test labels: [0, 1]
Infor: program suppressed labels: [2, 3]
Infor: program test labels: [0, 1]
Infor: program suppressed labels: [2, 3]
Info: program does not output training dice.
Building the Segmented Slice Tuple list, which may need 8 mins, please waiting......
Directory of /home/hxie1/data/OvarianCancerCT/Extract_uniform/trainLabels has 164 segmented slices for remained labels [0, 1].
Building the Segmented Slice Tuple list, which may need 8 mins, please waiting......
Directory of /home/hxie1/data/OvarianCancerCT/Extract_uniform/testLabels has 31 segmented slices for remained labels [0, 1].
Info: program uses 2D input.
TrainData Input:  batchSize=8, depth=1, height=281, width=281, NumClassfication=2

TestData Input:  batchSize=8, depth=1, height=281, width=281, NumClassfication=2

Info: program uses Mixup with alpha=0.4, and mixupProb = 0.5.
Network trains from scratch.
Network has total 337066946 parameters.
Info: network dropout rate = 0.3
Infor: Cross Entropy Weight: [1.0260619741432382, 39.37007874015748]

====================Net Architecture===========================
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 96, 281, 281]             960
       BatchNorm2d-2         [-1, 96, 281, 281]             192
              ReLU-3         [-1, 96, 281, 281]               0
    BN_ReLU_Conv2d-4         [-1, 96, 281, 281]               0
            Conv2d-5         [-1, 96, 281, 281]          83,040
       BatchNorm2d-6         [-1, 96, 281, 281]             192
              ReLU-7         [-1, 96, 281, 281]               0
    BN_ReLU_Conv2d-8         [-1, 96, 281, 281]               0
            Conv2d-9         [-1, 96, 281, 281]          83,040
      BatchNorm2d-10         [-1, 96, 281, 281]             192
             ReLU-11         [-1, 96, 281, 281]               0
   BN_ReLU_Conv2d-12         [-1, 96, 281, 281]               0
       Skip2Convs-13         [-1, 96, 281, 281]               0
ConvBuildingBlock-14         [-1, 96, 281, 281]               0
        ConvInput-15         [-1, 96, 281, 281]               0
           Conv2d-16         [-1, 96, 139, 139]         230,496
      BatchNorm2d-17         [-1, 96, 139, 139]             192
             ReLU-18         [-1, 96, 139, 139]               0
   BN_ReLU_Conv2d-19         [-1, 96, 139, 139]               0
           Conv2d-20         [-1, 96, 139, 139]          83,040
      BatchNorm2d-21         [-1, 96, 139, 139]             192
             ReLU-22         [-1, 96, 139, 139]               0
   BN_ReLU_Conv2d-23         [-1, 96, 139, 139]               0
           Conv2d-24         [-1, 96, 139, 139]          83,040
      BatchNorm2d-25         [-1, 96, 139, 139]             192
             ReLU-26         [-1, 96, 139, 139]               0
   BN_ReLU_Conv2d-27         [-1, 96, 139, 139]               0
           Conv2d-28         [-1, 96, 139, 139]          83,040
      BatchNorm2d-29         [-1, 96, 139, 139]             192
             ReLU-30         [-1, 96, 139, 139]               0
   BN_ReLU_Conv2d-31         [-1, 96, 139, 139]               0
       Skip2Convs-32         [-1, 96, 139, 139]               0
ConvBuildingBlock-33         [-1, 96, 139, 139]               0
         Down2dBB-34         [-1, 96, 139, 139]               0
           Conv2d-35          [-1, 192, 69, 69]         166,080
      BatchNorm2d-36          [-1, 192, 69, 69]             384
             ReLU-37          [-1, 192, 69, 69]               0
   BN_ReLU_Conv2d-38          [-1, 192, 69, 69]               0
           Conv2d-39          [-1, 192, 69, 69]         331,968
      BatchNorm2d-40          [-1, 192, 69, 69]             384
             ReLU-41          [-1, 192, 69, 69]               0
   BN_ReLU_Conv2d-42          [-1, 192, 69, 69]               0
           Conv2d-43          [-1, 192, 69, 69]         331,968
      BatchNorm2d-44          [-1, 192, 69, 69]             384
             ReLU-45          [-1, 192, 69, 69]               0
   BN_ReLU_Conv2d-46          [-1, 192, 69, 69]               0
           Conv2d-47          [-1, 192, 69, 69]         331,968
      BatchNorm2d-48          [-1, 192, 69, 69]             384
             ReLU-49          [-1, 192, 69, 69]               0
   BN_ReLU_Conv2d-50          [-1, 192, 69, 69]               0
       Skip2Convs-51          [-1, 192, 69, 69]               0
ConvBuildingBlock-52          [-1, 192, 69, 69]               0
         Down2dBB-53          [-1, 192, 69, 69]               0
           Conv2d-54          [-1, 384, 33, 33]       1,843,584
      BatchNorm2d-55          [-1, 384, 33, 33]             768
             ReLU-56          [-1, 384, 33, 33]               0
   BN_ReLU_Conv2d-57          [-1, 384, 33, 33]               0
           Conv2d-58          [-1, 384, 33, 33]       1,327,488
      BatchNorm2d-59          [-1, 384, 33, 33]             768
             ReLU-60          [-1, 384, 33, 33]               0
   BN_ReLU_Conv2d-61          [-1, 384, 33, 33]               0
           Conv2d-62          [-1, 384, 33, 33]       1,327,488
      BatchNorm2d-63          [-1, 384, 33, 33]             768
             ReLU-64          [-1, 384, 33, 33]               0
   BN_ReLU_Conv2d-65          [-1, 384, 33, 33]               0
           Conv2d-66          [-1, 384, 33, 33]       1,327,488
      BatchNorm2d-67          [-1, 384, 33, 33]             768
             ReLU-68          [-1, 384, 33, 33]               0
   BN_ReLU_Conv2d-69          [-1, 384, 33, 33]               0
       Skip2Convs-70          [-1, 384, 33, 33]               0
ConvBuildingBlock-71          [-1, 384, 33, 33]               0
         Down2dBB-72          [-1, 384, 33, 33]               0
           Conv2d-73          [-1, 768, 15, 15]       7,373,568
      BatchNorm2d-74          [-1, 768, 15, 15]           1,536
             ReLU-75          [-1, 768, 15, 15]               0
   BN_ReLU_Conv2d-76          [-1, 768, 15, 15]               0
           Conv2d-77          [-1, 768, 15, 15]       5,309,184
      BatchNorm2d-78          [-1, 768, 15, 15]           1,536
             ReLU-79          [-1, 768, 15, 15]               0
   BN_ReLU_Conv2d-80          [-1, 768, 15, 15]               0
           Conv2d-81          [-1, 768, 15, 15]       5,309,184
      BatchNorm2d-82          [-1, 768, 15, 15]           1,536
             ReLU-83          [-1, 768, 15, 15]               0
   BN_ReLU_Conv2d-84          [-1, 768, 15, 15]               0
           Conv2d-85          [-1, 768, 15, 15]       5,309,184
      BatchNorm2d-86          [-1, 768, 15, 15]           1,536
             ReLU-87          [-1, 768, 15, 15]               0
   BN_ReLU_Conv2d-88          [-1, 768, 15, 15]               0
       Skip2Convs-89          [-1, 768, 15, 15]               0
ConvBuildingBlock-90          [-1, 768, 15, 15]               0
         Down2dBB-91          [-1, 768, 15, 15]               0
           Conv2d-92           [-1, 1536, 7, 7]      10,618,368
      BatchNorm2d-93           [-1, 1536, 7, 7]           3,072
             ReLU-94           [-1, 1536, 7, 7]               0
   BN_ReLU_Conv2d-95           [-1, 1536, 7, 7]               0
           Conv2d-96           [-1, 1536, 7, 7]      21,235,200
      BatchNorm2d-97           [-1, 1536, 7, 7]           3,072
             ReLU-98           [-1, 1536, 7, 7]               0
   BN_ReLU_Conv2d-99           [-1, 1536, 7, 7]               0
          Conv2d-100           [-1, 1536, 7, 7]      21,235,200
     BatchNorm2d-101           [-1, 1536, 7, 7]           3,072
            ReLU-102           [-1, 1536, 7, 7]               0
  BN_ReLU_Conv2d-103           [-1, 1536, 7, 7]               0
          Conv2d-104           [-1, 1536, 7, 7]      21,235,200
     BatchNorm2d-105           [-1, 1536, 7, 7]           3,072
            ReLU-106           [-1, 1536, 7, 7]               0
  BN_ReLU_Conv2d-107           [-1, 1536, 7, 7]               0
      Skip2Convs-108           [-1, 1536, 7, 7]               0
ConvBuildingBlock-109           [-1, 1536, 7, 7]               0
        Down2dBB-110           [-1, 1536, 7, 7]               0
          Conv2d-111           [-1, 1536, 3, 3]      21,235,200
     BatchNorm2d-112           [-1, 1536, 3, 3]           3,072
            ReLU-113           [-1, 1536, 3, 3]               0
  BN_ReLU_Conv2d-114           [-1, 1536, 3, 3]               0
          Conv2d-115           [-1, 1536, 3, 3]      21,235,200
     BatchNorm2d-116           [-1, 1536, 3, 3]           3,072
            ReLU-117           [-1, 1536, 3, 3]               0
  BN_ReLU_Conv2d-118           [-1, 1536, 3, 3]               0
          Conv2d-119           [-1, 1536, 3, 3]      21,235,200
     BatchNorm2d-120           [-1, 1536, 3, 3]           3,072
            ReLU-121           [-1, 1536, 3, 3]               0
  BN_ReLU_Conv2d-122           [-1, 1536, 3, 3]               0
          Conv2d-123           [-1, 1536, 3, 3]      21,235,200
     BatchNorm2d-124           [-1, 1536, 3, 3]           3,072
            ReLU-125           [-1, 1536, 3, 3]               0
  BN_ReLU_Conv2d-126           [-1, 1536, 3, 3]               0
      Skip2Convs-127           [-1, 1536, 3, 3]               0
ConvBuildingBlock-128           [-1, 1536, 3, 3]               0
        Down2dBB-129           [-1, 1536, 3, 3]               0
 ConvTranspose2d-130           [-1, 1536, 7, 7]      21,235,200
     BatchNorm2d-131           [-1, 1536, 7, 7]           3,072
            ReLU-132           [-1, 1536, 7, 7]               0
 BN_ReLU_ConvT2d-133           [-1, 1536, 7, 7]               0
          Conv2d-134           [-1, 1536, 7, 7]      21,235,200
     BatchNorm2d-135           [-1, 1536, 7, 7]           3,072
            ReLU-136           [-1, 1536, 7, 7]               0
  BN_ReLU_Conv2d-137           [-1, 1536, 7, 7]               0
          Conv2d-138           [-1, 1536, 7, 7]      21,235,200
     BatchNorm2d-139           [-1, 1536, 7, 7]           3,072
            ReLU-140           [-1, 1536, 7, 7]               0
  BN_ReLU_Conv2d-141           [-1, 1536, 7, 7]               0
          Conv2d-142           [-1, 1536, 7, 7]      21,235,200
     BatchNorm2d-143           [-1, 1536, 7, 7]           3,072
            ReLU-144           [-1, 1536, 7, 7]               0
  BN_ReLU_Conv2d-145           [-1, 1536, 7, 7]               0
      Skip2Convs-146           [-1, 1536, 7, 7]               0
ConvBuildingBlock-147           [-1, 1536, 7, 7]               0
          Up2dBB-148           [-1, 1536, 7, 7]               0
       Dropout2d-149           [-1, 1536, 7, 7]               0
 ConvTranspose2d-150          [-1, 768, 15, 15]      21,234,432
     BatchNorm2d-151          [-1, 768, 15, 15]           1,536
            ReLU-152          [-1, 768, 15, 15]               0
 BN_ReLU_ConvT2d-153          [-1, 768, 15, 15]               0
          Conv2d-154          [-1, 768, 15, 15]       5,309,184
     BatchNorm2d-155          [-1, 768, 15, 15]           1,536
            ReLU-156          [-1, 768, 15, 15]               0
  BN_ReLU_Conv2d-157          [-1, 768, 15, 15]               0
          Conv2d-158          [-1, 768, 15, 15]       5,309,184
     BatchNorm2d-159          [-1, 768, 15, 15]           1,536
            ReLU-160          [-1, 768, 15, 15]               0
  BN_ReLU_Conv2d-161          [-1, 768, 15, 15]               0
          Conv2d-162          [-1, 768, 15, 15]       5,309,184
     BatchNorm2d-163          [-1, 768, 15, 15]           1,536
            ReLU-164          [-1, 768, 15, 15]               0
  BN_ReLU_Conv2d-165          [-1, 768, 15, 15]               0
      Skip2Convs-166          [-1, 768, 15, 15]               0
ConvBuildingBlock-167          [-1, 768, 15, 15]               0
          Up2dBB-168          [-1, 768, 15, 15]               0
       Dropout2d-169          [-1, 768, 15, 15]               0
 ConvTranspose2d-170          [-1, 384, 33, 33]      14,745,984
     BatchNorm2d-171          [-1, 384, 33, 33]             768
            ReLU-172          [-1, 384, 33, 33]               0
 BN_ReLU_ConvT2d-173          [-1, 384, 33, 33]               0
          Conv2d-174          [-1, 384, 33, 33]       1,327,488
     BatchNorm2d-175          [-1, 384, 33, 33]             768
            ReLU-176          [-1, 384, 33, 33]               0
  BN_ReLU_Conv2d-177          [-1, 384, 33, 33]               0
          Conv2d-178          [-1, 384, 33, 33]       1,327,488
     BatchNorm2d-179          [-1, 384, 33, 33]             768
            ReLU-180          [-1, 384, 33, 33]               0
  BN_ReLU_Conv2d-181          [-1, 384, 33, 33]               0
          Conv2d-182          [-1, 384, 33, 33]       1,327,488
     BatchNorm2d-183          [-1, 384, 33, 33]             768
            ReLU-184          [-1, 384, 33, 33]               0
  BN_ReLU_Conv2d-185          [-1, 384, 33, 33]               0
      Skip2Convs-186          [-1, 384, 33, 33]               0
ConvBuildingBlock-187          [-1, 384, 33, 33]               0
          Up2dBB-188          [-1, 384, 33, 33]               0
       Dropout2d-189          [-1, 384, 33, 33]               0
 ConvTranspose2d-190          [-1, 192, 69, 69]       3,686,592
     BatchNorm2d-191          [-1, 192, 69, 69]             384
            ReLU-192          [-1, 192, 69, 69]               0
 BN_ReLU_ConvT2d-193          [-1, 192, 69, 69]               0
          Conv2d-194          [-1, 192, 69, 69]         331,968
     BatchNorm2d-195          [-1, 192, 69, 69]             384
            ReLU-196          [-1, 192, 69, 69]               0
  BN_ReLU_Conv2d-197          [-1, 192, 69, 69]               0
          Conv2d-198          [-1, 192, 69, 69]         331,968
     BatchNorm2d-199          [-1, 192, 69, 69]             384
            ReLU-200          [-1, 192, 69, 69]               0
  BN_ReLU_Conv2d-201          [-1, 192, 69, 69]               0
          Conv2d-202          [-1, 192, 69, 69]         331,968
     BatchNorm2d-203          [-1, 192, 69, 69]             384
            ReLU-204          [-1, 192, 69, 69]               0
  BN_ReLU_Conv2d-205          [-1, 192, 69, 69]               0
      Skip2Convs-206          [-1, 192, 69, 69]               0
ConvBuildingBlock-207          [-1, 192, 69, 69]               0
          Up2dBB-208          [-1, 192, 69, 69]               0
       Dropout2d-209          [-1, 192, 69, 69]               0
 ConvTranspose2d-210         [-1, 96, 139, 139]         331,872
     BatchNorm2d-211         [-1, 96, 139, 139]             192
            ReLU-212         [-1, 96, 139, 139]               0
 BN_ReLU_ConvT2d-213         [-1, 96, 139, 139]               0
          Conv2d-214         [-1, 96, 139, 139]          83,040
     BatchNorm2d-215         [-1, 96, 139, 139]             192
            ReLU-216         [-1, 96, 139, 139]               0
  BN_ReLU_Conv2d-217         [-1, 96, 139, 139]               0
          Conv2d-218         [-1, 96, 139, 139]          83,040
     BatchNorm2d-219         [-1, 96, 139, 139]             192
            ReLU-220         [-1, 96, 139, 139]               0
  BN_ReLU_Conv2d-221         [-1, 96, 139, 139]               0
          Conv2d-222         [-1, 96, 139, 139]          83,040
     BatchNorm2d-223         [-1, 96, 139, 139]             192
            ReLU-224         [-1, 96, 139, 139]               0
  BN_ReLU_Conv2d-225         [-1, 96, 139, 139]               0
      Skip2Convs-226         [-1, 96, 139, 139]               0
ConvBuildingBlock-227         [-1, 96, 139, 139]               0
          Up2dBB-228         [-1, 96, 139, 139]               0
       Dropout2d-229         [-1, 96, 139, 139]               0
 ConvTranspose2d-230         [-1, 96, 281, 281]         460,896
     BatchNorm2d-231         [-1, 96, 281, 281]             192
            ReLU-232         [-1, 96, 281, 281]               0
 BN_ReLU_ConvT2d-233         [-1, 96, 281, 281]               0
          Conv2d-234         [-1, 96, 281, 281]          83,040
     BatchNorm2d-235         [-1, 96, 281, 281]             192
            ReLU-236         [-1, 96, 281, 281]               0
  BN_ReLU_Conv2d-237         [-1, 96, 281, 281]               0
          Conv2d-238         [-1, 96, 281, 281]          83,040
     BatchNorm2d-239         [-1, 96, 281, 281]             192
            ReLU-240         [-1, 96, 281, 281]               0
  BN_ReLU_Conv2d-241         [-1, 96, 281, 281]               0
          Conv2d-242         [-1, 96, 281, 281]          83,040
     BatchNorm2d-243         [-1, 96, 281, 281]             192
            ReLU-244         [-1, 96, 281, 281]               0
  BN_ReLU_Conv2d-245         [-1, 96, 281, 281]               0
      Skip2Convs-246         [-1, 96, 281, 281]               0
ConvBuildingBlock-247         [-1, 96, 281, 281]               0
          Up2dBB-248         [-1, 96, 281, 281]               0
       Dropout2d-249         [-1, 96, 281, 281]               0
          Conv2d-250          [-1, 2, 281, 281]             386
================================================================
Total params: 337,066,946
Trainable params: 337,066,946
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.30
Forward/backward pass size (MB): 3049.48
Params size (MB): 1285.81
Estimated Total Size (MB): 4335.59
----------------------------------------------------------------
===================End of Net Architecture =====================

Info: program will use 4 GPUs.
Loss Functions List: FocalCELoss with weight of 1; 	BoundaryLoss with weight of 0; 
Hints: Test Dice_0 is the dice coeff for all non-zero labels
Hints: Test Dice_1 is for primary cancer(green), 	
 test Dice_2 is for metastasis(yellow), 	
 and test Dice_3 is for invaded lymph node(brown).
Hints: Test TPR_0 is the TPR for all non-zero labels
Hints: Test TPR_1 is for primary cancer(green), 	
 TPR_2 is for metastasis(yellow), 	
 and TPR_3 is for invaded lymph node(brown).

Epoch	TrLoss	Dice0	Dice1	TPR_0	TPR_1	TsLoss	Dice0	Dice1	TPR_0	TPR_1
0	0.1059	0.000	0.000	0.000	0.000	0.0882	0.275	0.275	0.997	0.997
1	0.0746	0.000	0.000	0.000	0.000	0.0677	0.310	0.310	0.991	0.991
2	0.0621	0.000	0.000	0.000	0.000	0.0564	0.443	0.443	0.959	0.959
3	0.0672	0.000	0.000	0.000	0.000	0.0960	0.319	0.319	1.000	1.000
4	0.0556	0.000	0.000	0.000	0.000	0.0593	0.419	0.419	0.994	0.994
5	0.0560	0.000	0.000	0.000	0.000	0.0617	0.395	0.395	0.989	0.989
6	0.0593	0.000	0.000	0.000	0.000	0.0612	0.553	0.553	0.936	0.936
7	0.0578	0.000	0.000	0.000	0.000	0.0435	0.564	0.564	0.964	0.964
8	0.0583	0.000	0.000	0.000	0.000	0.0362	0.570	0.570	0.982	0.982
9	0.0578	0.000	0.000	0.000	0.000	0.0517	0.486	0.486	0.968	0.968
10	0.0533	0.000	0.000	0.000	0.000	0.0360	0.543	0.543	0.982	0.982
11	0.0593	0.000	0.000	0.000	0.000	0.0415	0.564	0.564	0.982	0.982
12	0.0529	0.000	0.000	0.000	0.000	0.0454	0.422	0.422	0.997	0.997
13	0.0475	0.000	0.000	0.000	0.000	0.0387	0.472	0.472	0.985	0.985
14	0.0548	0.000	0.000	0.000	0.000	0.0383	0.489	0.489	0.989	0.989
15	0.0514	0.000	0.000	0.000	0.000	0.0399	0.467	0.467	0.991	0.991
16	0.0494	0.000	0.000	0.000	0.000	0.0443	0.541	0.541	0.967	0.967
17	0.0497	0.000	0.000	0.000	0.000	0.0409	0.617	0.617	0.964	0.964
18	0.0525	0.000	0.000	0.000	0.000	0.0510	0.579	0.579	0.957	0.957
19	0.0482	0.000	0.000	0.000	0.000	0.0406	0.477	0.477	0.988	0.988
20	0.0529	0.000	0.000	0.000	0.000	0.0388	0.514	0.514	0.984	0.984
21	0.0451	0.000	0.000	0.000	0.000	0.0370	0.555	0.555	0.975	0.975
22	0.0492	0.000	0.000	0.000	0.000	0.0380	0.510	0.510	0.983	0.983
23	0.0503	0.000	0.000	0.000	0.000	0.0346	0.563	0.563	0.979	0.979
24	0.0497	0.000	0.000	0.000	0.000	0.0354	0.551	0.551	0.981	0.981
25	0.0511	0.000	0.000	0.000	0.000	0.0373	0.606	0.606	0.969	0.969
26	0.0553	0.000	0.000	0.000	0.000	0.0709	0.328	0.328	0.998	0.998
27	0.0537	0.000	0.000	0.000	0.000	0.0422	0.441	0.441	0.987	0.987
28	0.0592	0.000	0.000	0.000	0.000	0.0384	0.476	0.476	0.995	0.995
29	0.0511	0.000	0.000	0.000	0.000	0.0334	0.562	0.562	0.986	0.986
30	0.0528	0.000	0.000	0.000	0.000	0.0390	0.469	0.469	0.987	0.987
31	0.0472	0.000	0.000	0.000	0.000	0.0343	0.600	0.600	0.982	0.982
32	0.0563	0.000	0.000	0.000	0.000	0.0407	0.522	0.522	0.981	0.981
33	0.0511	0.000	0.000	0.000	0.000	0.0363	0.462	0.462	0.995	0.995
34	0.0395	0.000	0.000	0.000	0.000	0.0379	0.613	0.613	0.963	0.963
35	0.0430	0.000	0.000	0.000	0.000	0.0367	0.523	0.523	0.985	0.985
36	0.0534	0.000	0.000	0.000	0.000	0.0432	0.389	0.389	0.998	0.998
37	0.0490	0.000	0.000	0.000	0.000	0.0467	0.640	0.640	0.948	0.948
38	0.0541	0.000	0.000	0.000	0.000	0.0360	0.561	0.561	0.973	0.973
39	0.0494	0.000	0.000	0.000	0.000	0.0367	0.544	0.544	0.971	0.971
40	0.0437	0.000	0.000	0.000	0.000	0.0361	0.618	0.618	0.966	0.966
41	0.0397	0.000	0.000	0.000	0.000	0.0454	0.629	0.629	0.946	0.946
42	0.0484	0.000	0.000	0.000	0.000	0.0299	0.551	0.551	0.993	0.993
43	0.0484	0.000	0.000	0.000	0.000	0.0443	0.605	0.605	0.952	0.952
44	0.0457	0.000	0.000	0.000	0.000	0.0462	0.652	0.652	0.962	0.962
45	0.0500	0.000	0.000	0.000	0.000	0.0311	0.548	0.548	0.993	0.993
46	0.0451	0.000	0.000	0.000	0.000	0.0325	0.571	0.571	0.974	0.974
47	0.0432	0.000	0.000	0.000	0.000	0.0386	0.626	0.626	0.959	0.959
48	0.0410	0.000	0.000	0.000	0.000	0.0360	0.612	0.612	0.961	0.961
49	0.0464	0.000	0.000	0.000	0.000	0.0337	0.520	0.520	0.994	0.994
50	0.0491	0.000	0.000	0.000	0.000	0.0374	0.457	0.457	0.980	0.980
51	0.0474	0.000	0.000	0.000	0.000	0.0306	0.609	0.609	0.985	0.985
52	0.0416	0.000	0.000	0.000	0.000	0.0317	0.587	0.587	0.974	0.974
53	0.0425	0.000	0.000	0.000	0.000	0.0316	0.541	0.541	0.991	0.991
54	0.0467	0.000	0.000	0.000	0.000	0.0328	0.563	0.563	0.983	0.983
55	0.0475	0.000	0.000	0.000	0.000	0.0337	0.582	0.582	0.967	0.967
56	0.0393	0.000	0.000	0.000	0.000	0.0310	0.560	0.560	0.984	0.984
57	0.0456	0.000	0.000	0.000	0.000	0.0341	0.608	0.608	0.971	0.971
58	0.0410	0.000	0.000	0.000	0.000	0.0271	0.578	0.578	0.990	0.990
59	0.0366	0.000	0.000	0.000	0.000	0.0315	0.613	0.613	0.962	0.962
60	0.0448	0.000	0.000	0.000	0.000	0.0284	0.551	0.551	0.993	0.993
61	0.0388	0.000	0.000	0.000	0.000	0.0438	0.463	0.463	0.988	0.988
62	0.0453	0.000	0.000	0.000	0.000	0.0367	0.646	0.646	0.963	0.963
63	0.0419	0.000	0.000	0.000	0.000	0.0363	0.617	0.617	0.961	0.961
64	0.0416	0.000	0.000	0.000	0.000	0.0277	0.598	0.598	0.985	0.985
65	0.0470	0.000	0.000	0.000	0.000	0.0590	0.646	0.646	0.918	0.918
66	0.0397	0.000	0.000	0.000	0.000	0.0300	0.609	0.609	0.980	0.980
67	0.0479	0.000	0.000	0.000	0.000	0.0334	0.487	0.487	0.994	0.994
68	0.0445	0.000	0.000	0.000	0.000	0.0281	0.566	0.566	0.991	0.991
69	0.0395	0.000	0.000	0.000	0.000	0.0327	0.582	0.582	0.973	0.973
70	0.0365	0.000	0.000	0.000	0.000	0.0318	0.563	0.563	0.981	0.981
71	0.0392	0.000	0.000	0.000	0.000	0.0278	0.602	0.602	0.979	0.979
72	0.0418	0.000	0.000	0.000	0.000	0.0299	0.536	0.536	0.989	0.989
73	0.0360	0.000	0.000	0.000	0.000	0.0369	0.594	0.594	0.972	0.972
74	0.0385	0.000	0.000	0.000	0.000	0.0291	0.550	0.550	0.994	0.994
75	0.0482	0.000	0.000	0.000	0.000	0.0310	0.538	0.538	0.983	0.983
76	0.0430	0.000	0.000	0.000	0.000	0.0284	0.610	0.610	0.974	0.974
77	0.0392	0.000	0.000	0.000	0.000	0.0260	0.594	0.594	0.985	0.985
78	0.0358	0.000	0.000	0.000	0.000	0.0257	0.627	0.627	0.986	0.986
79	0.0431	0.000	0.000	0.000	0.000	0.0357	0.641	0.641	0.943	0.943
80	0.0451	0.000	0.000	0.000	0.000	0.0326	0.506	0.506	0.991	0.991
81	0.0395	0.000	0.000	0.000	0.000	0.0307	0.534	0.534	0.996	0.996
82	0.0403	0.000	0.000	0.000	0.000	0.0318	0.638	0.638	0.973	0.973
83	0.0411	0.000	0.000	0.000	0.000	0.0312	0.654	0.654	0.975	0.975
84	0.0312	0.000	0.000	0.000	0.000	0.0328	0.644	0.644	0.967	0.967
85	0.0371	0.000	0.000	0.000	0.000	0.0236	0.619	0.619	0.989	0.989
86	0.0382	0.000	0.000	0.000	0.000	0.0352	0.629	0.629	0.960	0.960
87	0.0355	0.000	0.000	0.000	0.000	0.0287	0.611	0.611	0.978	0.978
88	0.0370	0.000	0.000	0.000	0.000	0.0299	0.620	0.620	0.977	0.977
89	0.0394	0.000	0.000	0.000	0.000	0.0294	0.645	0.645	0.980	0.980
90	0.0397	0.000	0.000	0.000	0.000	0.0280	0.581	0.581	0.986	0.986
91	0.0384	0.000	0.000	0.000	0.000	0.0248	0.600	0.600	0.992	0.992
92	0.0340	0.000	0.000	0.000	0.000	0.0556	0.680	0.680	0.942	0.942
93	0.0390	0.000	0.000	0.000	0.000	0.0302	0.559	0.559	0.985	0.985
94	0.0406	0.000	0.000	0.000	0.000	0.0282	0.652	0.652	0.978	0.978
95	0.0367	0.000	0.000	0.000	0.000	0.0278	0.595	0.595	0.990	0.990
96	0.0409	0.000	0.000	0.000	0.000	0.0259	0.578	0.578	0.993	0.993
97	0.0355	0.000	0.000	0.000	0.000	0.0267	0.594	0.594	0.986	0.986
98	0.0467	0.000	0.000	0.000	0.000	0.0335	0.567	0.567	0.974	0.974
99	0.0464	0.000	0.000	0.000	0.000	0.0448	0.627	0.627	0.937	0.937
100	0.0447	0.000	0.000	0.000	0.000	0.0310	0.552	0.552	0.983	0.983
101	0.0411	0.000	0.000	0.000	0.000	0.0334	0.549	0.549	0.979	0.979
102	0.0490	0.000	0.000	0.000	0.000	0.0308	0.584	0.584	0.978	0.978
103	0.0381	0.000	0.000	0.000	0.000	0.0288	0.614	0.614	0.972	0.972
104	0.0485	0.000	0.000	0.000	0.000	0.0711	0.594	0.594	0.929	0.929
105	0.0419	0.000	0.000	0.000	0.000	0.0477	0.668	0.668	0.944	0.944
106	0.0411	0.000	0.000	0.000	0.000	0.0324	0.603	0.603	0.967	0.967
107	0.0416	0.000	0.000	0.000	0.000	0.0293	0.554	0.554	0.995	0.995
108	0.0508	0.000	0.000	0.000	0.000	0.0333	0.627	0.627	0.974	0.974
109	0.0393	0.000	0.000	0.000	0.000	0.0340	0.587	0.587	0.973	0.973
110	0.0452	0.000	0.000	0.000	0.000	0.0382	0.641	0.641	0.962	0.962
111	0.0432	0.000	0.000	0.000	0.000	0.0385	0.642	0.642	0.967	0.967
112	0.0478	0.000	0.000	0.000	0.000	0.0365	0.571	0.571	0.994	0.994
113	0.0450	0.000	0.000	0.000	0.000	0.0454	0.633	0.633	0.948	0.948
114	0.0442	0.000	0.000	0.000	0.000	0.0303	0.634	0.634	0.980	0.980
115	0.0455	0.000	0.000	0.000	0.000	0.0339	0.585	0.585	0.979	0.979
116	0.0462	0.000	0.000	0.000	0.000	0.0325	0.588	0.588	0.989	0.989
117	0.0380	0.000	0.000	0.000	0.000	0.0359	0.659	0.659	0.969	0.969
118	0.0433	0.000	0.000	0.000	0.000	0.0323	0.644	0.644	0.966	0.966
119	0.0475	0.000	0.000	0.000	0.000	0.0320	0.622	0.622	0.980	0.980
120	0.0476	0.000	0.000	0.000	0.000	0.0307	0.573	0.573	0.994	0.994
