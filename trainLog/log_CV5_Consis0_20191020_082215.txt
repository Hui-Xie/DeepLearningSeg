=============training from sratch============
Program ID: 31543

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy', '5', '3', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
       
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-20 08:22:15.200593
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191020_082215

Info: this is the 5th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191020_082215.
6-fold cross validation: the 5th fold is for test, the 0th fold is for validation, remaining folds are for training.

training dataset: total 24 image files.

validation dataset: total 6 image files.

test dataset: total 5 image files.
Total 24 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy
0 has 19573793 elements, with a rate of  0.7702522931519779 
1 has 5838391 elements, with a rate of  0.2297477068480222 
loss weight = tensor([1.0000, 3.3526])
Network has total 254,664,050 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		44.0119		0.25202		9.2355		0.23473		47.5546		0.12798
5	1.0000e-02		3.2828		0.57796		8.7187		0.02197		4.8686		0.19921
10	1.0000e-02		5.7949		0.50391		173.2709		0.31233		538.0168		0.15543
15	1.0000e-02		2.0572		0.65195		1.5111		0.72461		4.2467		0.56249
20	1.0000e-02		2.1845		0.65143		2.0745		0.65322		4.6718		0.51603
25	1.0000e-02		2.8247		0.65523		1.8543		0.68572		4.5748		0.54137
30	1.0000e-02		2.2440		0.66850		1.7494		0.70371		5.9615		0.53835
35	1.0000e-02		2.0210		0.66671		2.4865		0.47550		9.0998		0.31340
40	1.0000e-02		1.7323		0.68023		1.5984		0.71451		2.9571		0.60103
45	1.0000e-02		2.5884		0.59690		8.1222		0.55865		11.0244		0.42186
50	1.0000e-02		2.2189		0.65742		1.6982		0.67765		3.1818		0.56459
55	1.0000e-02		3.0862		0.59104		2.0641		0.66747		2.6941		0.57499
60	1.0000e-02		2.6447		0.60822		2.3308		0.59946		2.0445		0.60927
65	1.0000e-02		1.9179		0.70276		2.1497		0.66617		4.1897		0.41524
70	1.0000e-03		2.0612		0.68420		1.6158		0.70815		3.4503		0.44056
75	1.0000e-03		1.6558		0.70681		1.5114		0.71341		3.0903		0.50332
80	1.0000e-03		2.2865		0.74065		1.6616		0.69621		3.5022		0.45671
85	1.0000e-03		1.4381		0.75657		1.7386		0.68937		3.2084		0.48741
90	1.0000e-03		1.3950		0.73595		1.6752		0.69565		3.7800		0.48915
95	1.0000e-03		1.3781		0.73022		1.4892		0.71638		2.1130		0.61687
100	1.0000e-03		1.0691		0.74312		1.5384		0.72096		1.7641		0.60080
105	1.0000e-03		1.3668		0.73478		1.5667		0.71163		2.9567		0.59762
110	1.0000e-03		1.1788		0.74554		1.4600		0.72697		1.9225		0.65475
115	1.0000e-03		1.4611		0.73817		1.6427		0.70377		2.7629		0.60965
120	1.0000e-03		1.1367		0.78162		1.5066		0.71973		2.2131		0.64378
125	1.0000e-03		1.1185		0.77446		3.4303		0.60459		3.1233		0.54073
130	1.0000e-03		1.4457		0.73776		1.6580		0.70251		3.7087		0.58361
135	1.0000e-03		1.2408		0.75928		1.9643		0.67544		3.8029		0.58017
140	1.0000e-03		1.0237		0.78425		1.8683		0.69256		4.3498		0.59858
145	1.0000e-03		1.1080		0.78308		1.5807		0.71460		2.9910		0.62886
150	1.0000e-03		0.9629		0.79797		2.6876		0.57466		3.8592		0.60125
155	1.0000e-03		1.0117		0.80794		1.8618		0.70789		4.1669		0.53563
160	1.0000e-03		0.9907		0.78194		1.6181		0.71697		3.2041		0.58087
165	1.0000e-04		0.8798		0.80397		1.8241		0.70307		3.4782		0.56378
170	1.0000e-04		1.0618		0.79594		1.8033		0.70844		3.4588		0.53680
175	1.0000e-04		0.9309		0.80355		1.6839		0.72304		3.3133		0.49940
180	1.0000e-04		0.7194		0.81318		1.7245		0.71541		3.2333		0.50890
185	1.0000e-04		0.7131		0.80954		1.7460		0.72281		3.8214		0.50461
190	1.0000e-04		0.7643		0.81818		1.8307		0.70797		3.6223		0.50053
195	1.0000e-04		0.8150		0.82514		2.0073		0.69193		4.0512		0.50161
200	1.0000e-04		0.8150		0.81339		1.8710		0.70758		3.0001		0.52813
205	1.0000e-04		0.7839		0.81981		1.8347		0.71539		4.0601		0.51626
210	1.0000e-04		0.7150		0.82691		2.0359		0.67110		3.8733		0.52444
215	1.0000e-04		0.8221		0.81085		1.7824		0.72089		4.9267		0.48661
220	1.0000e-05		0.9446		0.81018		2.2846		0.68507		5.0068		0.50113
225	1.0000e-05		0.8104		0.82108		2.1051		0.68403		4.5321		0.48900
230	1.0000e-05		0.8334		0.81814		1.9569		0.71182		4.4710		0.47541
235	1.0000e-05		0.8022		0.81560		1.9596		0.70534		5.1631		0.48847
240	1.0000e-05		0.8887		0.81751		1.9135		0.71303		4.6359		0.47717
245	1.0000e-05		0.6588		0.80738		2.2337		0.68729		4.3642		0.47613
250	1.0000e-05		0.9212		0.80870		2.0929		0.70061		4.2776		0.51561
255	1.0000e-05		0.6632		0.82762		1.9219		0.70427		3.3772		0.49611
260	1.0000e-05		0.8491		0.81493		1.9951		0.70371		3.9484		0.48409
265	1.0000e-05		0.6237		0.82440		2.3635		0.64586		5.3633		0.48816
270	1.0000e-05		0.8145		0.80917		2.1758		0.69443		4.1442		0.52779
275	1.0000e-06		0.7759		0.80820		1.8936		0.71957		4.1159		0.48715
280	1.0000e-06		0.7338		0.83064		2.1485		0.69712		4.2212		0.54009
285	1.0000e-06		0.8532		0.82736		1.9764		0.70866		4.8453		0.47961
290	1.0000e-06		0.6899		0.82617		1.9833		0.71239		4.2968		0.47652
295	1.0000e-06		0.8017		0.81903		1.9183		0.71358		3.7081		0.49471
300	1.0000e-06		0.7701		0.82089		2.0377		0.69880		4.1337		0.54201
305	1.0000e-06		0.8170		0.82095		2.1160		0.70218		4.5050		0.48261
310	1.0000e-06		0.7269		0.82423		2.0138		0.70469		4.1340		0.49567
315	1.0000e-06		0.7939		0.82277		1.8659		0.72531		4.4839		0.48553
320	1.0000e-06		0.7668		0.80787		1.8730		0.71916		4.5852		0.47758
325	1.0000e-06		0.7612		0.81123		2.1307		0.69551		3.6451		0.49755
330	1.0000e-07		0.8234		0.81420		2.0272		0.70887		4.8022		0.47982
335	1.0000e-07		0.7321		0.82296		2.2798		0.66049		5.2608		0.49567
340	1.0000e-07		0.7909		0.82139		2.0350		0.71100		5.0093		0.48139
345	1.0000e-07		0.8122		0.80915		2.0371		0.70866		4.7048		0.49876
350	1.0000e-07		0.7419		0.81492		2.0727		0.70264		3.8828		0.51618
355	1.0000e-07		0.8840		0.82526		2.1565		0.68974		5.2069		0.48057
360	1.0000e-07		0.6575		0.82434		1.9725		0.71326		4.7422		0.52142
365	1.0000e-07		0.7646		0.82062		2.0242		0.70398		4.8847		0.49809
370	1.0000e-07		0.8679		0.80739		2.0102		0.69915		3.8577		0.57094
375	1.0000e-07		0.8238		0.81671		1.9764		0.71206		4.0941		0.48305
380	1.0000e-07		0.8932		0.82677		1.9076		0.71730		3.8997		0.48954
385	1.0000e-08		0.8765		0.82957		2.0438		0.70597		3.8423		0.48783
390	1.0000e-08		0.6213		0.82533		2.1494		0.70133		4.3625		0.47187
395	1.0000e-08		0.8496		0.81380		1.9784		0.71096		4.0846		0.47569
400	1.0000e-08		0.7878		0.82218		1.9619		0.70851		4.4846		0.58391
405	1.0000e-08		0.6500		0.82407		2.1318		0.69392		3.5603		0.49626
410	1.0000e-08		0.8252		0.81411		1.9059		0.71487		4.0930		0.48067
415	1.0000e-08		0.6593		0.82827		1.8733		0.71787		4.8574		0.48247
420	1.0000e-08		0.7720		0.81181		2.4170		0.68049		4.8152		0.52750
425	1.0000e-08		0.6494		0.83038		1.8658		0.72414		4.3976		0.48493
430	1.0000e-08		0.7450		0.83343		1.9696		0.71213		3.8491		0.50218
435	1.0000e-08		0.7280		0.83019		1.8945		0.71889		4.2419		0.48489
440	1.0000e-08		0.7136		0.82210		2.0020		0.70999		4.6296		0.47690
445	1.0000e-08		0.7624		0.82465		1.9472		0.71611		4.8650		0.47903
450	1.0000e-08		0.7268		0.81961		2.1752		0.69010		5.2004		0.49003
455	1.0000e-08		0.7796		0.82062		1.9882		0.70354		4.0085		0.47868
460	1.0000e-08		0.7460		0.83366		2.0508		0.70427		4.5248		0.48719
465	1.0000e-08		0.7025		0.82432		2.2351		0.68630		4.1864		0.55235
470	1.0000e-08		0.6488		0.82119		1.8610		0.71760		4.0102		0.49196
475	1.0000e-08		0.7918		0.83026		1.9609		0.71640		4.1237		0.49385
480	1.0000e-08		0.7926		0.82278		2.0484		0.71608		5.5184		0.49366
485	1.0000e-08		0.8181		0.81886		1.8547		0.72409		4.8676		0.48202
490	1.0000e-08		0.6839		0.82427		1.9230		0.71488		4.2960		0.48048
495	1.0000e-08		1.0778		0.80798		2.0605		0.70907		4.8111		0.52483
500	1.0000e-08		0.7470		0.83512		1.9743		0.71549		4.1849		0.48661
505	1.0000e-08		0.7198		0.82233		1.9400		0.71435		4.5499		0.48451
510	1.0000e-08		0.7767		0.81705		2.1529		0.69899		4.7054		0.54097
515	1.0000e-08		0.7367		0.81994		1.9722		0.71461		4.8330		0.47794
520	1.0000e-08		0.7872		0.81785		1.9706		0.72100		5.4454		0.47623
525	1.0000e-08		0.7092		0.82931		2.0886		0.70262		4.4860		0.46979
530	1.0000e-08		0.7363		0.81307		2.0414		0.70260		3.8505		0.52735
535	1.0000e-08		0.8386		0.82527		1.8930		0.71669		3.3386		0.50696
540	1.0000e-08		0.7938		0.82040		1.9814		0.71270		4.7985		0.48922
545	1.0000e-08		0.7613		0.82054		2.0375		0.71155		5.3496		0.48473
550	1.0000e-08		0.8430		0.80709		2.3098		0.67372		3.7675		0.56512
555	1.0000e-08		0.6457		0.81855		2.0369		0.70489		4.0343		0.51280
560	1.0000e-08		0.6043		0.83472		1.8948		0.71643		3.8761		0.49944
565	1.0000e-08		0.7965		0.83010		1.9421		0.71499		3.4204		0.50630
570	1.0000e-08		0.7818		0.82465		1.9207		0.72004		4.8091		0.47620
575	1.0000e-08		0.8275		0.82303		2.0040		0.71079		4.5375		0.47174
580	1.0000e-08		0.8017		0.82460		1.9719		0.70895		4.2428		0.47644
585	1.0000e-08		0.8807		0.81391		1.9899		0.71093		3.9021		0.48990
590	1.0000e-08		0.7520		0.83490		1.8709		0.71410		4.6079		0.48282
595	1.0000e-08		1.0026		0.82246		1.9676		0.71337		4.2041		0.48696
600	1.0000e-08		0.6598		0.81622		1.9556		0.71169		4.2994		0.54578
605	1.0000e-08		0.7801		0.81736		1.9344		0.71559		4.4194		0.47781
610	1.0000e-08		0.7774		0.81535		2.0178		0.70894		4.5409		0.54814
615	1.0000e-08		0.8336		0.82194		1.9121		0.71367		4.0291		0.48290
