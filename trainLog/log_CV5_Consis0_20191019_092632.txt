=============training from sratch============
Program ID: 12237

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy', '5', '3', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
       
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-19 09:26:32.488976
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191019_092632

Info: this is the 5th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191019_092632.
6-fold cross validation: the 5th fold is for test, the 0th fold is for validation, remaining folds are for training.

training dataset: total 24 image files.

validation dataset: total 6 image files.

test dataset: total 5 image files.
Total 24 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy
0 has 19573793 elements, with a rate of  0.7702522931519779 
1 has 5838391 elements, with a rate of  0.2297477068480222 
loss weight = tensor([1.0000, 3.3526])
Network has total 254,652,146 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		4.4256		0.15005		18.9894		0.32670		12.0341		0.30811
5	1.0000e-02		2.1388		0.63738		3.4301		0.58440		6.8915		0.48916
10	1.0000e-02		2.0343		0.65548		1.5250		0.71243		2.1121		0.61339
15	1.0000e-02		2.2753		0.62746		1.6196		0.69898		1.7903		0.62200
20	1.0000e-02		1.7568		0.69038		1.4467		0.72712		1.6700		0.65177
25	1.0000e-02		1.6059		0.71131		1.5230		0.72496		1.6193		0.65418
30	1.0000e-02		1.8350		0.64825		1.4350		0.72368		1.9811		0.62110
35	1.0000e-02		1.6365		0.69221		2.1427		0.58923		2.7333		0.45745
40	1.0000e-02		1.7904		0.74125		2.2698		0.60585		4.7386		0.38010
45	1.0000e-02		1.3613		0.76136		2.2355		0.61082		1.7761		0.64657
50	1.0000e-02		1.3013		0.76044		1.6136		0.69186		2.3390		0.52508
55	1.0000e-02		1.4414		0.75445		3.2825		0.57357		3.2296		0.48217
60	1.0000e-02		1.1795		0.75456		2.3350		0.60696		2.3486		0.59736
65	1.0000e-02		1.3419		0.74771		2.5888		0.61921		3.1157		0.56792
70	1.0000e-02		1.0229		0.77998		1.8935		0.68070		2.9146		0.54598
75	1.0000e-02		1.2200		0.77073		2.4498		0.63308		2.1010		0.56141
80	1.0000e-02		1.0838		0.78826		2.7569		0.59392		3.4021		0.54929
85	1.0000e-03		1.1149		0.76495		1.9279		0.66545		3.0488		0.43392
90	1.0000e-03		0.8567		0.82119		1.7044		0.70778		3.5150		0.53479
95	1.0000e-03		0.8687		0.81281		1.6473		0.71037		3.3362		0.56724
100	1.0000e-03		0.7975		0.81712		1.6698		0.71653		3.1800		0.55892
105	1.0000e-03		0.7551		0.80834		1.5445		0.73427		3.4881		0.57564
110	1.0000e-03		0.7152		0.82700		1.5976		0.73072		3.6656		0.58512
115	1.0000e-03		0.7555		0.82468		1.4637		0.73229		3.5440		0.57391
120	1.0000e-03		0.8872		0.79874		1.5698		0.72217		3.6199		0.54425
125	1.0000e-03		0.7087		0.82625		1.9027		0.70301		4.7554		0.55874
130	1.0000e-03		0.7375		0.82773		1.5782		0.72812		4.3931		0.54836
135	1.0000e-03		0.7712		0.81206		1.5041		0.73006		4.0125		0.55605
140	1.0000e-04		0.6186		0.82599		1.8081		0.70742		4.2976		0.53601
145	1.0000e-04		0.6787		0.81658		1.4804		0.72771		4.0827		0.55400
150	1.0000e-04		0.5907		0.83171		1.4287		0.73774		4.9536		0.54562
155	1.0000e-04		0.9762		0.82423		1.3949		0.73662		4.5243		0.55094
160	1.0000e-04		0.5834		0.84983		1.3913		0.73611		4.6425		0.55476
165	1.0000e-04		0.6312		0.83233		1.3935		0.73788		4.5701		0.55287
170	1.0000e-04		0.6183		0.84898		1.5329		0.73449		5.6257		0.54999
175	1.0000e-04		0.7256		0.83343		1.4175		0.73047		4.1097		0.55516
180	1.0000e-04		0.6475		0.84582		1.5256		0.72442		5.4153		0.53464
185	1.0000e-04		0.5424		0.85898		1.4890		0.72846		5.4350		0.55257
190	1.0000e-04		0.6450		0.83822		1.4766		0.72952		4.4626		0.55656
195	1.0000e-04		0.5227		0.84275		1.3791		0.73404		4.3306		0.56142
200	1.0000e-04		0.5699		0.84270		1.3772		0.74122		4.5371		0.55728
205	1.0000e-04		0.6215		0.84762		1.8273		0.69977		6.0378		0.52077
210	1.0000e-04		0.6195		0.84474		1.4296		0.73776		4.5240		0.55949
215	1.0000e-04		0.5778		0.85092		1.7188		0.71176		5.9050		0.50845
220	1.0000e-04		0.5702		0.84037		2.2057		0.66470		6.6183		0.46072
225	1.0000e-04		0.6207		0.83984		1.3780		0.73976		4.5495		0.55325
230	1.0000e-04		0.5333		0.85737		1.6150		0.72472		5.1077		0.55088
235	1.0000e-04		0.5587		0.84999		1.4726		0.73593		5.5300		0.53874
240	1.0000e-04		0.6680		0.83453		1.5034		0.72971		5.1642		0.52235
245	1.0000e-04		0.6136		0.84539		1.4745		0.73840		5.4065		0.55107
250	1.0000e-04		0.5926		0.83973		1.4899		0.72972		5.0597		0.54575
255	1.0000e-05		0.7253		0.83783		1.6528		0.71971		6.1877		0.52986
260	1.0000e-05		0.5295		0.83900		1.3769		0.73897		4.3130		0.56011
265	1.0000e-05		0.6087		0.83485		1.5583		0.72774		5.1918		0.53599
270	1.0000e-05		0.5178		0.85381		1.4867		0.73598		5.1857		0.55738
275	1.0000e-05		0.6590		0.83575		1.4923		0.73341		4.9914		0.54497
280	1.0000e-05		0.6518		0.84089		1.3464		0.74183		3.9796		0.56750
285	1.0000e-05		0.7283		0.83751		1.4220		0.73953		4.1979		0.55720
290	1.0000e-05		0.4776		0.84940		1.3945		0.73927		4.8876		0.55770
295	1.0000e-05		0.5191		0.85788		1.5487		0.72752		4.9863		0.54412
300	1.0000e-05		0.5360		0.84977		1.7842		0.70119		6.3393		0.50466
305	1.0000e-05		0.6374		0.84946		1.4703		0.73855		4.3798		0.55470
310	1.0000e-05		0.4731		0.85186		1.5948		0.72897		4.3955		0.55031
315	1.0000e-05		0.5726		0.84270		1.5015		0.73105		5.0212		0.54055
320	1.0000e-05		0.7302		0.85032		1.4775		0.73672		5.0067		0.54864
325	1.0000e-05		0.6384		0.83656		1.4254		0.73740		3.8807		0.56418
330	1.0000e-05		0.7976		0.84733		1.5958		0.72215		5.8309		0.52563
335	1.0000e-06		0.6800		0.83666		1.4745		0.73701		4.6695		0.54750
340	1.0000e-06		0.6851		0.84816		1.4289		0.73826		5.4978		0.54679
345	1.0000e-06		0.6667		0.83996		1.4811		0.73738		4.7501		0.54633
350	1.0000e-06		0.5776		0.84410		1.3824		0.74143		4.3691		0.55926
355	1.0000e-06		0.5153		0.86051		1.5543		0.72758		5.6062		0.53881
360	1.0000e-06		0.5172		0.85932		1.5216		0.73200		5.1444		0.54421
365	1.0000e-06		0.6959		0.84610		1.4856		0.73208		5.0959		0.54028
370	1.0000e-06		0.6493		0.85530		1.4509		0.73718		4.7666		0.54980
375	1.0000e-06		0.5211		0.84339		1.3964		0.74243		4.3764		0.55545
380	1.0000e-06		0.5970		0.84808		1.5741		0.72817		5.1588		0.54493
385	1.0000e-06		0.6032		0.86153		1.7025		0.71046		6.1992		0.51479
390	1.0000e-07		0.5356		0.86029		1.5366		0.73101		5.5698		0.53455
395	1.0000e-07		0.4698		0.85394		1.4871		0.73487		5.1456		0.55040
400	1.0000e-07		0.7119		0.84304		1.5341		0.72908		5.2146		0.54050
405	1.0000e-07		0.5376		0.84764		1.4030		0.73887		5.0218		0.55197
410	1.0000e-07		0.5192		0.86064		1.4016		0.73725		4.5192		0.56065
415	1.0000e-07		0.5413		0.84737		1.5129		0.73177		5.4216		0.54161
420	1.0000e-07		0.5829		0.84588		1.4309		0.73919		4.3895		0.55620
425	1.0000e-07		0.5984		0.84242		1.4255		0.73975		4.5173		0.55653
430	1.0000e-07		0.6240		0.84362		1.3979		0.73970		4.4773		0.56281
435	1.0000e-07		0.5356		0.85076		1.4224		0.73901		4.5696		0.56110
440	1.0000e-07		0.5456		0.84677		1.4322		0.74240		4.6002		0.56406
445	1.0000e-08		0.4903		0.84544		1.4196		0.74085		4.7099		0.55917
450	1.0000e-08		0.6317		0.83674		1.4554		0.73398		3.8918		0.56308
455	1.0000e-08		0.5554		0.86020		1.4352		0.73813		5.4795		0.54697
460	1.0000e-08		0.5848		0.85831		1.3984		0.73378		4.5327		0.55667
465	1.0000e-08		0.6042		0.85607		1.5243		0.72864		5.2797		0.53314
470	1.0000e-08		0.6763		0.84218		1.4640		0.74041		4.5608		0.55611
475	1.0000e-08		0.6992		0.83743		1.5184		0.73110		5.2948		0.54000
480	1.0000e-08		0.5250		0.84816		1.4419		0.73783		4.7203		0.54991
485	1.0000e-08		0.6022		0.83136		1.6185		0.72458		5.6071		0.53282
490	1.0000e-08		0.5488		0.84796		1.3895		0.73669		4.2409		0.56528
495	1.0000e-08		0.6663		0.83520		1.4274		0.73385		4.2166		0.56039
500	1.0000e-08		0.5154		0.85873		1.6534		0.71842		5.8976		0.53253
505	1.0000e-08		0.5916		0.85167		1.3904		0.73940		4.5629		0.56004
510	1.0000e-08		0.5905		0.84905		1.5245		0.73188		4.8107		0.55330
515	1.0000e-08		0.7439		0.84084		1.5865		0.72696		5.0264		0.53110
520	1.0000e-08		0.5261		0.86177		1.4538		0.74111		4.9396		0.55283
525	1.0000e-08		0.6673		0.84254		1.4137		0.74024		4.5314		0.55317
530	1.0000e-08		0.6898		0.83401		1.4463		0.73784		4.3758		0.55087
535	1.0000e-08		0.6144		0.83872		1.5684		0.72783		5.3687		0.53591
540	1.0000e-08		0.5902		0.85307		1.5471		0.72654		5.6447		0.53401
545	1.0000e-08		0.5980		0.83953		1.4554		0.73402		4.5479		0.55957
550	1.0000e-08		0.5958		0.85425		1.4297		0.73836		5.1031		0.55242
