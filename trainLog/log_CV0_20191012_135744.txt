=============training from sratch============
Program ID: 4827

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy', '0', '3,2,1']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relauch training;
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-12 13:57:44.840530
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191012_135744

Info: this is the 0th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 6

Info: useLabelConsistencyLoss = False and searchWindowSize= 3

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191012_135744.
program re-initializes all input files list, which will lead previous all K_fold cross validation invalid.
6-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.

training dataset: total 19 image files.

validation dataset: total 5 image files.

test dataset: total 5 image files.
Total 19 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy
0 has 18433812 elements, with a rate of  0.9162854777808447 
1 has 1684167 elements, with a rate of  0.08371452221915532 
loss weight = tensor([ 1.0000, 10.9454])
Network has total 113,191,074 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		17.4495		0.11799		8.6305		0.00049		15.4643		0.00019
5	1.0000e-02		10.5279		0.05575		6.5487		0.07230		9.7257		0.21995
10	1.0000e-02		5.9384		0.41693		18.2261		0.11274		10.2806		0.30037
15	1.0000e-02		6.2274		0.38831		16.6653		0.10360		6.4827		0.35787
20	1.0000e-02		4.9726		0.39787		14.7702		0.13723		5.8162		0.33945
25	1.0000e-02		5.8969		0.40755		7.9779		0.13012		4.1784		0.43481
30	1.0000e-02		3.6785		0.44943		5.3820		0.20925		5.3879		0.38106
35	1.0000e-02		3.6619		0.47723		4.5741		0.21823		5.2384		0.37076
40	1.0000e-02		4.7977		0.44933		8.0308		0.16134		7.9672		0.34104
45	1.0000e-02		4.4083		0.44127		3.4812		0.21403		5.7921		0.35798
50	1.0000e-02		3.6459		0.42990		5.0097		0.21027		5.1106		0.39480
55	1.0000e-02		3.6041		0.43562		2.3750		0.29799		3.6226		0.44397
60	1.0000e-02		5.8311		0.46587		6.0478		0.22811		6.4011		0.36815
65	1.0000e-02		3.9710		0.45443		17.0927		0.12465		12.4352		0.27815
70	1.0000e-02		3.9807		0.40995		6.7270		0.14858		5.3783		0.43461
75	1.0000e-02		3.4775		0.48925		4.9867		0.15045		4.2361		0.45178
80	1.0000e-02		3.5582		0.50041		9.3895		0.18540		5.1745		0.40393
85	1.0000e-02		4.7305		0.42628		5.4073		0.19875		5.5167		0.38832
90	1.0000e-02		4.5025		0.41753		4.5915		0.20143		5.6052		0.36549
95	1.0000e-02		3.6397		0.45813		3.3159		0.33720		3.1874		0.50118
100	1.0000e-02		3.2362		0.52042		3.1107		0.20577		4.0962		0.44492
105	1.0000e-02		3.3194		0.48504		2.0291		0.32209		4.5279		0.50418
110	1.0000e-02		3.6174		0.51462		6.3493		0.09506		8.4254		0.46180
115	1.0000e-02		2.6119		0.56172		5.2415		0.19590		4.1076		0.46173
120	1.0000e-02		2.8973		0.53399		4.0371		0.18742		3.6996		0.44747
125	1.0000e-02		2.3160		0.53629		3.4720		0.16424		3.0891		0.50795
130	1.0000e-02		2.5596		0.51644		5.5675		0.14489		5.6315		0.46611
135	1.0000e-02		3.1536		0.54275		4.9351		0.25585		3.7618		0.41149
140	1.0000e-02		2.7449		0.49110		7.1200		0.14355		6.3241		0.40871
145	1.0000e-02		2.5954		0.49469		5.0086		0.21168		4.0046		0.43993
150	1.0000e-02		3.8908		0.43283		3.9947		0.20824		5.3227		0.44917
155	1.0000e-02		3.8876		0.51523		3.5132		0.25705		3.8865		0.46345
160	1.0000e-03		3.4910		0.55483		6.4023		0.11385		5.3129		0.39483
165	1.0000e-03		3.0086		0.51916		3.6126		0.23307		3.6534		0.45969
170	1.0000e-03		3.3594		0.52117		3.0185		0.25293		3.7286		0.50384
175	1.0000e-03		3.3253		0.48874		3.3486		0.22718		3.2064		0.51333
180	1.0000e-03		2.7838		0.54688		2.9931		0.25077		3.4190		0.51577
185	1.0000e-03		2.8569		0.53782		3.1308		0.25441		3.7901		0.51300
190	1.0000e-03		3.3462		0.52604		3.5638		0.23940		3.9203		0.52235
195	1.0000e-03		2.7123		0.52099		3.2718		0.23636		3.2667		0.50088
200	1.0000e-03		2.4251		0.53408		3.3700		0.22248		3.2252		0.49662
205	1.0000e-03		3.6476		0.54140		3.1920		0.23787		3.4422		0.49294
210	1.0000e-03		2.5481		0.50619		3.5615		0.22226		3.3892		0.48123
215	1.0000e-04		2.5163		0.53892		2.7619		0.26802		3.3475		0.50639
220	1.0000e-04		3.1955		0.53460		2.9769		0.25803		3.3621		0.50281
225	1.0000e-04		2.9891		0.53299		2.9231		0.25618		3.0921		0.50650
230	1.0000e-04		3.3038		0.52834		2.8568		0.26466		3.2288		0.50567
235	1.0000e-04		5.5226		0.53649		3.2194		0.24747		3.4864		0.50400
240	1.0000e-04		2.8462		0.54078		3.0915		0.25778		3.4160		0.49517
245	1.0000e-04		2.2824		0.53898		2.9787		0.26975		3.6918		0.50352
250	1.0000e-04		2.0832		0.54964		3.0694		0.25664		3.5252		0.50731
255	1.0000e-04		2.5551		0.51649		3.4354		0.22028		3.3254		0.48881
260	1.0000e-04		3.1181		0.52332		3.2907		0.22877		3.4824		0.46481
265	1.0000e-04		2.8507		0.53204		3.2911		0.24186		3.1045		0.48545
270	1.0000e-05		2.2441		0.53445		3.3096		0.23176		3.3987		0.49447
275	1.0000e-05		2.5676		0.54423		3.3482		0.22151		3.4118		0.46930
280	1.0000e-05		2.3008		0.54733		3.2499		0.24655		3.1902		0.48936
285	1.0000e-05		2.8500		0.52190		2.9687		0.25443		3.2395		0.46272
290	1.0000e-05		2.2028		0.53251		2.9915		0.24829		3.3077		0.47168
295	1.0000e-05		2.0262		0.56517		3.1921		0.24521		2.9836		0.50407
300	1.0000e-05		2.2812		0.54785		2.8983		0.26193		3.2074		0.49824
305	1.0000e-05		2.3290		0.54765		3.1605		0.24781		3.3821		0.50576
310	1.0000e-05		2.8695		0.53370		3.0267		0.26293		3.2901		0.51187
315	1.0000e-05		2.5389		0.54887		3.0922		0.25242		3.2695		0.49788
320	1.0000e-05		2.7753		0.53348		3.3926		0.22196		3.0543		0.50688
325	1.0000e-06		3.8163		0.53226		2.9889		0.25798		3.2690		0.50207
330	1.0000e-06		2.2916		0.55345		3.3033		0.22966		3.4049		0.47745
335	1.0000e-06		2.0795		0.54844		3.1397		0.24081		3.2599		0.46976
340	1.0000e-06		2.1979		0.57779		3.0676		0.23924		3.2297		0.46022
345	1.0000e-06		2.7739		0.56893		2.9703		0.25568		3.2470		0.48122
350	1.0000e-06		3.0395		0.50783		3.2316		0.23433		3.4960		0.46482
355	1.0000e-06		2.5308		0.54015		3.1096		0.24687		3.2271		0.47681
360	1.0000e-06		2.3062		0.59102		3.0126		0.25141		3.0441		0.49363
365	1.0000e-06		2.6069		0.57454		3.4007		0.22589		3.0294		0.46863
370	1.0000e-06		2.3270		0.54402		3.3101		0.24173		3.0304		0.50357
375	1.0000e-06		2.0802		0.53094		2.9193		0.25770		3.1688		0.48486
380	1.0000e-07		2.3844		0.55183		3.1596		0.24576		3.1246		0.49985
385	1.0000e-07		2.2741		0.55464		2.9761		0.25257		3.2122		0.49251
390	1.0000e-07		2.2736		0.52661		3.0712		0.25244		3.1003		0.47883
395	1.0000e-07		2.4661		0.53368		2.9463		0.24626		3.1012		0.46751
400	1.0000e-07		2.6240		0.55491		3.1783		0.23953		3.1537		0.49042
405	1.0000e-07		1.8266		0.54816		3.2395		0.22901		3.3420		0.46868
410	1.0000e-07		2.1639		0.57187		3.6510		0.21249		3.2624		0.48237
415	1.0000e-07		2.2499		0.54185		2.9179		0.26102		3.2580		0.49183
420	1.0000e-07		2.6989		0.55990		2.9757		0.25994		3.4089		0.48625
425	1.0000e-07		2.1801		0.53828		3.3923		0.22758		3.2442		0.48982
430	1.0000e-07		2.5727		0.54491		3.0882		0.24968		3.0821		0.49668
435	1.0000e-08		3.2253		0.53959		3.1840		0.24190		3.2569		0.49109
440	1.0000e-08		2.3732		0.52042		3.0103		0.26082		3.3722		0.51195
445	1.0000e-08		2.5367		0.54538		3.0980		0.25054		3.1511		0.50437
450	1.0000e-08		2.2671		0.53551		3.2202		0.23785		3.1402		0.49611
455	1.0000e-08		2.1952		0.53574		3.5750		0.21248		3.6140		0.47362
460	1.0000e-08		2.3905		0.55326		3.0213		0.25725		3.2421		0.50382
465	1.0000e-08		2.0341		0.56899		3.5759		0.22033		3.2963		0.48657
470	1.0000e-08		2.8649		0.52902		3.0598		0.25267		2.9908		0.49580
475	1.0000e-08		1.9587		0.57298		2.9442		0.26945		3.2082		0.50625
480	1.0000e-08		3.0788		0.54927		3.1285		0.25093		3.2353		0.50028
485	1.0000e-08		3.0007		0.50794		3.2990		0.22869		3.4905		0.44994
490	1.0000e-08		2.5636		0.54508		3.3429		0.23954		2.9573		0.48988
495	1.0000e-08		2.5098		0.53200		3.2918		0.23939		3.2182		0.49584
500	1.0000e-08		2.5559		0.54033		3.2901		0.24089		3.2735		0.50161
505	1.0000e-08		3.0404		0.53453		3.2262		0.23933		3.3097		0.47932
510	1.0000e-08		2.2044		0.54193		3.2921		0.23683		3.4425		0.49735
515	1.0000e-08		3.0333		0.51141		3.5565		0.22197		3.4488		0.49417
520	1.0000e-08		2.4875		0.54346		2.9133		0.26528		3.3146		0.49198
525	1.0000e-08		2.5320		0.53701		3.1176		0.25104		3.1043		0.51446
530	1.0000e-08		2.1830		0.55716		3.1382		0.23076		3.2558		0.46722
535	1.0000e-08		2.4557		0.52127		3.3580		0.22272		3.5037		0.47061
540	1.0000e-08		2.0374		0.57368		3.3202		0.23242		3.0545		0.49299
545	1.0000e-08		2.1655		0.54139		3.8313		0.19746		3.6464		0.47930
550	1.0000e-08		2.3613		0.54315		3.3195		0.23271		3.2957		0.48731
555	1.0000e-08		2.1555		0.55370		3.6169		0.21571		3.7008		0.49704
560	1.0000e-08		2.8690		0.53541		2.9789		0.25699		3.3313		0.47324
565	1.0000e-08		2.3077		0.54504		3.1899		0.24574		3.3474		0.49691
570	1.0000e-08		2.6030		0.54664		3.6203		0.20689		3.2878		0.50156
575	1.0000e-08		3.0549		0.54938		3.6675		0.21226		3.3331		0.50433
580	1.0000e-08		2.1798		0.55485		3.4287		0.23115		3.1045		0.48338
585	1.0000e-08		2.8669		0.54276		3.3647		0.23363		3.0905		0.49948
590	1.0000e-08		2.3672		0.56324		3.1919		0.24221		2.9816		0.49678
595	1.0000e-08		2.5703		0.55345		3.3797		0.23519		3.0434		0.49796
600	1.0000e-08		2.0604		0.55802		3.2618		0.23102		3.2920		0.44718
605	1.0000e-08		2.7082		0.54544		3.0442		0.24721		3.1424		0.50734
610	1.0000e-08		2.9597		0.53593		3.1904		0.24300		3.2925		0.49617
615	1.0000e-08		2.4146		0.54687		2.9419		0.25075		3.2617		0.47374
620	1.0000e-08		2.7080		0.56517		3.0536		0.25244		3.1705		0.49165
625	1.0000e-08		2.3800		0.55649		2.9003		0.25608		3.1771		0.48019
630	1.0000e-08		3.1205		0.54385		2.9655		0.24952		3.2944		0.47255
635	1.0000e-08		2.6592		0.53719		2.9164		0.25917		3.1607		0.49084
640	1.0000e-08		2.3484		0.55173		3.3962		0.23156		2.9734		0.49171
645	1.0000e-08		2.6982		0.53752		3.3738		0.23113		3.3237		0.49757
650	1.0000e-08		2.0610		0.54652		3.2054		0.24245		3.4505		0.49809
655	1.0000e-08		2.8690		0.53602		2.8268		0.26648		3.2250		0.48266
660	1.0000e-08		2.6145		0.54879		2.9298		0.24867		3.4756		0.45882
665	1.0000e-08		3.2079		0.54054		3.2006		0.24345		3.3258		0.49888
670	1.0000e-08		2.6674		0.52372		3.0812		0.24918		3.1112		0.48328
675	1.0000e-08		2.1095		0.53504		3.1006		0.25290		3.0849		0.50025
680	1.0000e-08		2.3037		0.54976		2.9751		0.26353		3.3570		0.49670
685	1.0000e-08		2.3883		0.53961		2.9757		0.26695		3.5881		0.50641
690	1.0000e-08		2.4563		0.56468		3.0795		0.25244		3.2315		0.49542
695	1.0000e-08		2.4374		0.54611		2.7832		0.26718		3.0589		0.48986
700	1.0000e-08		2.5075		0.53896		3.3304		0.24305		3.0138		0.50115
705	1.0000e-08		1.9721		0.54929		3.5806		0.21851		3.3081		0.50228
710	1.0000e-08		2.6584		0.55600		3.3368		0.23342		3.3705		0.46974
715	1.0000e-08		1.9063		0.52954		3.1963		0.23424		3.3600		0.46615
720	1.0000e-08		2.1054		0.57347		2.8934		0.26599		3.4368		0.50913
725	1.0000e-08		2.4037		0.55809		2.9343		0.25077		3.0822		0.49725
730	1.0000e-08		2.5893		0.53421		3.3838		0.22948		3.1645		0.51160
735	1.0000e-08		2.3874		0.55290		3.1028		0.24867		3.2290		0.49026
740	1.0000e-08		2.7736		0.51559		3.2562		0.23329		3.1300		0.49654
745	1.0000e-08		2.4601		0.54420		3.3061		0.23984		3.1505		0.49174
750	1.0000e-08		2.6438		0.54004		3.1030		0.23507		3.4627		0.44637
755	1.0000e-08		2.4343		0.52802		3.3692		0.21637		3.5015		0.43448
760	1.0000e-08		2.7334		0.52735		3.2520		0.23263		3.3009		0.48173
765	1.0000e-08		2.5596		0.48785		3.4526		0.21977		3.3776		0.46267
770	1.0000e-08		2.4026		0.54953		2.9195		0.26300		2.9370		0.48749
775	1.0000e-08		2.3715		0.55294		3.3392		0.23679		3.0070		0.50417
780	1.0000e-08		2.7672		0.52788		3.0219		0.25654		3.3156		0.49431
785	1.0000e-08		4.7376		0.52308		3.3094		0.23937		3.0746		0.49881
790	1.0000e-08		2.7156		0.57133		3.2122		0.24740		3.1019		0.48804
