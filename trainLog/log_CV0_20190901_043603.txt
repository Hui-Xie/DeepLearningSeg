=============training from sratch============
Program ID: 23009

Program command: 
 ['TrainResNeXtVNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/ResNeXtVNet', '1', '/home/hxie1/data/OvarianCancerCT/pixelSize223withLabel/numpy', '/home/hxie1/data/OvarianCancerCT/pixelSize223withLabel/numpyLabel', '0', '3,2,1']

Major program changes: 
     1  a V model with ResNeXt block: use z convolution, and then xy convolution, to implement 3D convolution.
     2  at ground truth, only check the segmented slices, about 3 slices per patient;
     3  the input is whole 3D volume, instead of ROI around a segmented slice;
     4  support input data augmentation: affine in xy plane, and translation in z direction;
     5  input Size: 231*251*251 with label, instead of previous SkyWatch Model of 29*140*140;
     6  treat all 1,2,3 labels as 1, in other words, do not differentiate primary, metastase, and nymph node;
     7  initializaiton LR is 0.1; considering zero padding and the bigger risk cost of missing cancer, adjust loss positive weight;  
    

Discarded changes:                  

Experiment setting:
Input CT data: maximum size 231*251*251 (zyx) of 3D numpy array with spacing size(3*2*2)

Loss Function:  BCELogitLoss

Data:   total 143 patients with weak annotaton label, 5-fold cross validation, test 29, validation 29, and training 85.  

Training strategy: 

          

Program starting Time: 2019-09-01 04:36:03.708808
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/ResNeXtVNet/20190901_043603

Info: this is the 0th fold leave for test in the 5-fold cross-validation.

Info: batchSize = 12

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/ResNeXtVNet/20190901_043603.
5-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.

training dataset: total 85 image files.

validation dataset: total 29 image files.

test dataset: total 29 image files.
Network has total 26,372,379 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-01		12655003.6172	0.03304		4461440.2083	0.02300		5638809.5833	0.03527
5	1.0000e-01		2130962.8018	0.07130		5068430.8333	0.00353		7963002.0417	0.00508
10	1.0000e-01		1902770.6836	0.08510		2707821.8333	0.06844		3552955.3125	0.08108
15	1.0000e-01		1893468.5469	0.09960		2949840.0833	0.07727		3865164.2083	0.08834
20	1.0000e-01		2145701.2344	0.09310		4253942.6667	0.04947		4522614.1667	0.06470
25	1.0000e-01		1765016.9131	0.08015		2329059.8750	0.09121		2747634.9375	0.11312
30	1.0000e-01		1711269.2549	0.09415		6995422.1667	0.01893		8591696.2917	0.03227
35	1.0000e-01		1623416.5781	0.10710		7447336.8333	0.07257		10591558.4167	0.06797
40	1.0000e-01		1496310.7891	0.09675		3653490.3333	0.06135		4017015.9583	0.08230
45	1.0000e-01		1525436.3091	0.11123		2763706.5417	0.12274		3598838.9583	0.14116
50	1.0000e-02		1531042.2773	0.11887		1547506.0833	0.11674		1914261.1875	0.13429
55	1.0000e-02		1340392.1797	0.12948		1478602.2917	0.13310		1903655.9375	0.13034
60	1.0000e-02		1221878.0410	0.12790		1449500.2917	0.13961		1884512.8125	0.13585
65	1.0000e-02		1383640.2134	0.13125		1440246.1875	0.13187		1907138.2500	0.13607
70	1.0000e-02		1217042.2900	0.12912		1582514.3333	0.11074		1953613.7917	0.13643
75	1.0000e-02		1211719.7207	0.13451		1458209.0000	0.13172		1923543.8333	0.13641
80	1.0000e-02		1260954.2559	0.13615		1423616.4792	0.12741		1810220.9792	0.13512
85	1.0000e-02		1227578.7324	0.13290		1458722.9167	0.13060		1846190.0833	0.13070
90	1.0000e-02		1239639.1475	0.12888		1490062.2917	0.12478		1904495.7708	0.14020
95	1.0000e-02		1186616.3457	0.14190		1486023.7083	0.12816		1845596.0833	0.13980
100	1.0000e-02		1249083.0938	0.12159		1661178.5208	0.11015		2020926.3333	0.12691
105	1.0000e-02		1231484.5645	0.12308		1469624.8125	0.12302		1896396.1875	0.12854
110	1.0000e-02		1212353.4414	0.12751		1458587.8125	0.13260		1863009.3021	0.14164
115	1.0000e-02		1234589.5586	0.12896		1537384.2917	0.11393		1779890.2917	0.13911
120	1.0000e-02		1310886.9141	0.13041		1392530.0208	0.14270		1836678.3750	0.14041
125	1.0000e-02		1249885.1250	0.13061		1570433.8750	0.12305		1997431.3542	0.13613
130	1.0000e-02		1165222.6211	0.12837		1475462.2708	0.13002		1864913.3125	0.13846
135	1.0000e-02		1199729.9902	0.13801		1502362.6667	0.12302		1850634.6667	0.13856
140	1.0000e-02		1023001.3179	0.14030		1438326.8958	0.13366		1890064.7917	0.13644
145	1.0000e-02		1207957.0879	0.15104		1466573.5833	0.13017		1921572.1042	0.13828
150	1.0000e-03		1331466.4043	0.13212		1427210.3750	0.13558		1852519.7812	0.13931
155	1.0000e-03		1223652.9805	0.13275		1421418.8125	0.13711		1874540.6458	0.13903
160	1.0000e-03		1073453.7725	0.13833		1434915.9583	0.13751		1894720.1667	0.14203
165	1.0000e-03		1006817.6543	0.15582		1416744.0417	0.13384		1833880.4167	0.13772
170	1.0000e-03		1122283.7656	0.13805		1407844.4375	0.13683		1829759.9375	0.13865
175	1.0000e-03		1164645.0469	0.15150		1427664.5000	0.13363		1843549.7812	0.14267
180	1.0000e-03		1057383.0078	0.15779		1432909.5417	0.13384		1887559.1042	0.14034
185	1.0000e-03		1013364.9043	0.14963		1438529.1042	0.13491		1879028.2500	0.14291
190	1.0000e-03		1219720.6006	0.13994		1420330.1250	0.13645		1827401.7917	0.14039
195	1.0000e-03		1073086.4619	0.13995		1480059.1667	0.13474		1922063.9375	0.14416
200	1.0000e-03		1195794.2285	0.14139		1430293.8750	0.13562		1857973.9062	0.14163
205	1.0000e-03		1205488.6290	0.15656		1406832.3542	0.14068		1836560.6354	0.13964
210	1.0000e-03		1121552.7324	0.15129		1449459.6875	0.13249		1834922.3021	0.14668
215	1.0000e-03		1169206.5820	0.15415		1429286.2500	0.13338		1754307.5938	0.14348
