=============training from sratch============
Program ID: 18847

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy', '2', '1', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-13 08:31:07.263536
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191013_083107

Info: this is the 2th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 2

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191013_083107.
6-fold cross validation: the 2th fold is for test, the 3th fold is for validation, remaining folds are for training.

training dataset: total 19 image files.

validation dataset: total 5 image files.

test dataset: total 5 image files.
Total 19 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy
0 has 18801751 elements, with a rate of  0.9345745415083692 
1 has 1316228 elements, with a rate of  0.0654254584916308 
loss weight = tensor([ 1.0000, 14.2846])
Network has total 113,191,074 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		8.9352		0.11325		1307.7681		0.00000		3281.9974		0.00000
5	1.0000e-02		3.5305		0.26015		2.3273		0.40707		53.6467		0.21924
10	1.0000e-02		2.0463		0.35017		4.7689		0.17286		3.4778		0.36468
15	1.0000e-02		2.0520		0.34791		2.8950		0.29531		2.4244		0.43112
20	1.0000e-02		2.1432		0.33614		5.5288		0.17793		5.5207		0.29064
25	1.0000e-02		1.5708		0.41482		5.4153		0.16220		2.7486		0.41898
30	1.0000e-02		1.3731		0.37887		3.8972		0.23830		2.1489		0.42021
35	1.0000e-02		1.4889		0.36131		4.2745		0.22044		2.1239		0.42592
40	1.0000e-02		1.5407		0.37733		5.2044		0.19450		1.9870		0.51367
45	1.0000e-02		1.2275		0.42024		4.6053		0.23549		1.9287		0.43586
50	1.0000e-02		3.4436		0.33377		4.8952		0.20659		3.2197		0.40225
55	1.0000e-02		2.5743		0.33025		2.9341		0.29426		2.2946		0.44942
60	1.0000e-03		1.4164		0.37926		7.6908		0.13659		3.8711		0.35652
65	1.0000e-03		1.2121		0.44808		3.0364		0.26914		2.3399		0.39551
70	1.0000e-03		1.2013		0.43951		3.7331		0.24304		2.1516		0.41184
75	1.0000e-03		1.0124		0.45339		4.0370		0.22313		2.0494		0.43101
80	1.0000e-03		1.2114		0.43320		2.8884		0.30085		2.2527		0.41778
85	1.0000e-03		1.2339		0.46396		3.1040		0.28639		2.3748		0.41628
90	1.0000e-03		0.9844		0.47532		3.1992		0.28088		2.0739		0.42922
95	1.0000e-03		1.1736		0.45189		3.8132		0.25203		2.1772		0.45817
100	1.0000e-03		1.2200		0.46058		4.3592		0.23673		2.0344		0.48114
105	1.0000e-03		0.9267		0.45799		3.5585		0.27107		2.1431		0.44659
110	1.0000e-03		0.8873		0.46846		3.4346		0.28358		2.2379		0.45000
115	1.0000e-04		0.9524		0.45443		3.3716		0.29183		2.0579		0.42896
120	1.0000e-04		1.0340		0.45015		3.5831		0.28466		2.0283		0.44109
125	1.0000e-04		1.4495		0.45125		3.7925		0.28013		2.0224		0.46039
130	1.0000e-04		1.1701		0.43673		3.4056		0.28832		2.0570		0.44741
135	1.0000e-04		0.8901		0.51393		3.4667		0.27647		2.2378		0.42616
140	1.0000e-04		1.0491		0.46604		3.5461		0.27778		2.1789		0.44892
145	1.0000e-04		1.1590		0.45379		3.3448		0.28167		2.1640		0.44025
150	1.0000e-04		1.3336		0.47402		3.6579		0.27267		2.1519		0.45085
155	1.0000e-04		1.0526		0.45430		3.8833		0.25038		2.1597		0.44144
160	1.0000e-04		0.9629		0.46413		3.5070		0.26983		2.1254		0.44009
165	1.0000e-04		0.9549		0.42852		3.4561		0.27930		2.0227		0.45485
170	1.0000e-05		1.0153		0.45196		3.6440		0.26300		2.1306		0.43898
175	1.0000e-05		0.9668		0.44378		3.5233		0.26947		2.1292		0.43715
180	1.0000e-05		1.1160		0.46423		3.6245		0.26833		2.1497		0.44864
185	1.0000e-05		0.9598		0.45702		3.2984		0.28258		2.1691		0.43367
190	1.0000e-05		1.0706		0.44629		3.3878		0.28206		2.2625		0.42872
195	1.0000e-05		1.1115		0.46088		3.6052		0.27340		2.2124		0.44710
200	1.0000e-05		0.9086		0.47512		3.4766		0.26999		2.1503		0.43446
205	1.0000e-05		0.9663		0.44852		3.8589		0.25390		2.1988		0.44295
210	1.0000e-05		0.9802		0.47485		3.1938		0.28893		2.2663		0.42856
215	1.0000e-05		1.1069		0.48076		3.7911		0.26472		2.3119		0.44280
220	1.0000e-05		1.0554		0.44155		3.7053		0.25937		2.1395		0.43891
225	1.0000e-06		1.0402		0.44128		3.1192		0.29601		2.2218		0.42754
230	1.0000e-06		0.9953		0.44151		3.3978		0.27430		2.1027		0.43979
235	1.0000e-06		1.0615		0.46686		3.3192		0.28807		2.1745		0.44070
240	1.0000e-06		0.9296		0.47235		3.2750		0.27199		2.2180		0.41926
245	1.0000e-06		0.9231		0.49233		3.4631		0.27524		2.1072		0.44101
250	1.0000e-06		0.9963		0.47896		3.9386		0.25541		2.2101		0.45021
255	1.0000e-06		0.9275		0.47703		3.4455		0.27579		2.2572		0.43955
