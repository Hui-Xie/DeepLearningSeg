=============training from sratch============
Program ID: 3271

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy', '0', '3,2,1']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together.  
         

Discarded changes:                  

Experiment setting:
Input CT data: 51*171*171 ROI around primary cancer

Loss Function:  SoftMax

Data:   total 36 patients with 50-80% label, 6-fold cross validation, test 6, validation 6, and training 24.  
    script: python3.7 statisticsLabelFiles.py 
    Total 36 in /home/hxie1/data/OvarianCancerCT/primaryROI/labels_npy
    0 has 48159408 elements, with a rate of  0.8970491562903105 
    1 has 5527068 elements, with a rate of  0.10295084370968957

Training strategy: 

          

Program starting Time: 2019-10-07 11:03:41.369971
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191007_110341

Info: this is the 0th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 6

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191007_110341.
6-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.

training dataset: total 27 image files.

validation dataset: total 7 image files.

test dataset: total 7 image files.
Total 27 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy
0 has 23525912 elements, with a rate of  0.822909269733675 
1 has 5062795 elements, with a rate of  0.1770907302663251 
loss weight = tensor([1.0000, 4.6468])
Network has total 113,192,738 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		71.9245		0.10754		19006.9473		0.00000		4888.2823		0.00000
5	1.0000e-02		11.4876		0.31335		7.8430		0.44304		4.6957		0.21609
10	1.0000e-02		8.9463		0.33394		5.1767		0.47129		3.8811		0.29261
15	1.0000e-02		8.4095		0.36485		6.7892		0.49346		3.5167		0.34335
20	1.0000e-02		7.5154		0.47453		7.8807		0.50862		3.4982		0.40929
25	1.0000e-02		7.6164		0.46485		15.6317		0.49796		3.2393		0.41151
30	1.0000e-02		6.8022		0.47352		4.6002		0.58273		2.5455		0.45717
35	1.0000e-02		8.9586		0.44371		4.0457		0.51168		3.2508		0.34774
40	1.0000e-02		7.1065		0.47715		4.2452		0.50921		3.3344		0.37475
45	1.0000e-02		5.7579		0.46864		6.5520		0.58033		2.6580		0.41911
50	1.0000e-02		6.2537		0.48840		4.6726		0.55185		2.4614		0.40223
55	1.0000e-02		5.8888		0.46444		3.5375		0.62204		2.0390		0.47405
60	1.0000e-02		5.8312		0.48887		3.7203		0.54187		2.3890		0.40860
65	1.0000e-02		5.1498		0.51883		3.1564		0.60893		1.8733		0.47555
70	1.0000e-02		5.2705		0.49146		3.7176		0.59357		2.1515		0.43289
75	1.0000e-02		5.4165		0.51919		5.5105		0.51277		2.9987		0.37052
80	1.0000e-02		5.2748		0.51848		3.1445		0.59335		1.7551		0.52061
85	1.0000e-02		4.8451		0.53760		3.0403		0.57855		1.8472		0.49233
90	1.0000e-02		4.1980		0.54175		3.2494		0.56262		1.7301		0.53803
95	1.0000e-02		4.9710		0.52510		3.3508		0.62517		2.4728		0.45473
100	1.0000e-02		5.6253		0.52980		3.2932		0.52543		2.0755		0.49159
105	1.0000e-02		4.7229		0.53585		6.2039		0.63642		3.3721		0.36004
110	1.0000e-02		4.2080		0.55394		2.8080		0.60589		1.8023		0.52180
115	1.0000e-02		4.9823		0.49921		3.6849		0.53239		3.0515		0.34605
120	1.0000e-02		5.0466		0.53103		3.2801		0.56541		2.1911		0.45334
125	1.0000e-02		4.9111		0.54666		3.8746		0.51331		2.4100		0.41898
130	1.0000e-02		3.8699		0.54227		4.0804		0.50527		3.2043		0.34159
135	1.0000e-02		4.2893		0.53185		3.1316		0.63066		2.2838		0.42519
140	1.0000e-02		3.8999		0.57004		3.1439		0.57592		2.1059		0.45657
145	1.0000e-02		4.1459		0.54491		2.3251		0.65230		2.0663		0.54711
150	1.0000e-02		3.5043		0.55477		2.4362		0.62461		1.9692		0.49808
155	1.0000e-02		4.1919		0.57692		3.2095		0.66165		2.8594		0.44447
160	1.0000e-02		4.4054		0.57302		2.7872		0.60431		1.5981		0.55193
165	1.0000e-02		3.6392		0.57519		3.2930		0.64959		2.6746		0.42389
170	1.0000e-02		3.9288		0.54129		4.4641		0.51283		2.5741		0.44201
175	1.0000e-02		3.7925		0.57315		3.5190		0.58791		2.0186		0.46616
180	1.0000e-02		3.8962		0.53301		5.0429		0.49726		3.2160		0.36933
185	1.0000e-02		3.1023		0.56971		3.2266		0.59660		2.4953		0.48274
190	1.0000e-02		3.7276		0.60060		2.8191		0.62910		1.8347		0.51083
195	1.0000e-02		3.2642		0.55056		2.2614		0.66449		2.1410		0.50274
200	1.0000e-02		2.9214		0.62957		2.9405		0.69118		2.2243		0.49326
205	1.0000e-02		3.5485		0.56555		2.3684		0.66204		3.4112		0.49417
210	1.0000e-02		4.0455		0.61929		2.3550		0.62891		2.5988		0.42702
215	1.0000e-02		3.1926		0.57360		2.7484		0.66365		2.3058		0.43428
220	1.0000e-02		2.5786		0.63160		2.6720		0.68410		2.4605		0.51865
225	1.0000e-02		3.0648		0.61661		2.8749		0.63500		2.0637		0.50996
230	1.0000e-02		2.7249		0.61137		2.7584		0.71644		2.4345		0.42723
235	1.0000e-02		3.0095		0.59903		2.9386		0.61190		1.8280		0.50632
240	1.0000e-02		4.0783		0.58478		3.5226		0.54884		3.3130		0.37456
245	1.0000e-02		3.8772		0.53466		4.7799		0.69784		3.0799		0.25523
250	1.0000e-03		2.3907		0.60643		2.8428		0.71999		2.5983		0.51661
255	1.0000e-03		2.7541		0.62227		2.4900		0.64221		2.4585		0.53017
260	1.0000e-03		2.4568		0.62286		2.3020		0.65681		2.1538		0.53723
265	1.0000e-03		2.0703		0.63944		2.2033		0.67504		2.6582		0.51901
270	1.0000e-03		2.4959		0.64826		2.2313		0.67218		2.3029		0.55233
275	1.0000e-03		2.4796		0.62369		2.2078		0.66093		2.6362		0.52802
