=============training from sratch============
Program ID: 482

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy', '3', '2', '1']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-14 09:22:41.195202
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191014_092241

Info: this is the 3th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = True and searchWindowSize= 7

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191014_092241.
6-fold cross validation: the 3th fold is for test, the 4th fold is for validation, remaining folds are for training.

training dataset: total 19 image files.

validation dataset: total 5 image files.

test dataset: total 5 image files.
Total 19 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy
0 has 18805544 elements, with a rate of  0.9347630793331676 
1 has 1312435 elements, with a rate of  0.06523692066683238 
loss weight = tensor([ 1.0000, 14.3287])
Network has total 113,191,074 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		7.5124		0.12065		1982.1899		0.00009		1075.1727		0.00000
5	1.0000e-02		2.8224		0.30879		4.6157		0.36765		2.3145		0.28765
10	1.0000e-02		2.3819		0.28093		2.7588		0.50301		2.7282		0.30131
15	1.0000e-02		2.0261		0.37293		3.3303		0.50373		1.5049		0.40426
20	1.0000e-02		1.7184		0.34382		3.2030		0.53134		3.4641		0.36889
25	1.0000e-02		2.1946		0.37804		4.1261		0.48476		2.0471		0.40540
30	1.0000e-02		2.2498		0.37353		5.4065		0.41516		6.6666		0.33155
35	1.0000e-02		2.2305		0.33608		3.2026		0.44957		3.2432		0.30306
40	1.0000e-02		2.1042		0.37696		3.1844		0.44961		3.6912		0.21680
45	1.0000e-02		1.9715		0.35480		3.3678		0.41816		3.4989		0.24919
50	1.0000e-02		1.8014		0.37789		3.4188		0.40766		2.6800		0.26468
55	1.0000e-02		1.7281		0.40667		1.9219		0.46697		4.3346		0.28331
60	1.0000e-02		1.1392		0.43546		2.0368		0.53114		4.0460		0.31372
65	1.0000e-02		1.0837		0.46013		1.4573		0.54941		8.2404		0.19453
70	1.0000e-02		0.8704		0.47678		1.9882		0.54276		7.8219		0.27271
75	1.0000e-02		1.2716		0.42040		1.4944		0.55537		16.6180		0.15919
80	1.0000e-02		1.1013		0.44266		3.0189		0.49417		2.0194		0.34676
85	1.0000e-02		1.2703		0.44665		1.7589		0.56149		6.5537		0.26216
90	1.0000e-02		0.8718		0.50788		2.2190		0.50912		7.4214		0.23947
95	1.0000e-02		1.0252		0.47901		2.1583		0.51911		10.4099		0.18907
100	1.0000e-02		0.9989		0.47538		1.9423		0.50968		6.2680		0.20002
105	1.0000e-02		1.9890		0.36153		1.5406		0.58245		8.7505		0.21549
110	1.0000e-02		1.8736		0.38306		2.0259		0.47332		2.3109		0.30826
115	1.0000e-02		0.8008		0.50144		2.3723		0.53285		7.4357		0.32424
120	1.0000e-03		0.9488		0.48084		2.6173		0.44735		3.2356		0.36922
125	1.0000e-03		0.8215		0.52339		1.9871		0.49398		7.7454		0.26743
130	1.0000e-03		0.6414		0.56497		2.0573		0.50347		6.9300		0.27801
135	1.0000e-03		0.6554		0.59153		2.0851		0.51450		10.6647		0.19485
140	1.0000e-03		0.5697		0.58691		2.0378		0.53540		10.9892		0.19992
145	1.0000e-03		0.6470		0.52961		1.9116		0.53183		9.7975		0.22495
150	1.0000e-03		0.8272		0.57302		1.9183		0.52356		9.2043		0.22563
155	1.0000e-03		0.6022		0.57878		2.0814		0.51182		8.9124		0.24584
160	1.0000e-03		0.6429		0.59633		2.0248		0.54055		12.5780		0.17496
165	1.0000e-03		0.6287		0.59548		1.7652		0.59952		16.2479		0.16644
170	1.0000e-03		0.5285		0.61009		1.8277		0.55058		15.4177		0.18191
175	1.0000e-04		0.5908		0.57280		1.9516		0.55805		13.9794		0.19285
180	1.0000e-04		0.6348		0.61004		2.0159		0.55526		12.2643		0.19577
185	1.0000e-04		0.4377		0.65055		2.0387		0.56289		16.3805		0.17729
190	1.0000e-04		0.5057		0.60892		2.4177		0.50918		11.9638		0.19890
195	1.0000e-04		0.4799		0.59392		2.0760		0.51751		12.4905		0.19590
200	1.0000e-04		0.4519		0.61346		2.0280		0.54728		13.3351		0.17718
205	1.0000e-04		0.4372		0.61794		2.1243		0.54538		13.7571		0.15909
210	1.0000e-04		0.4925		0.60671		1.8553		0.56085		16.3361		0.17363
215	1.0000e-04		0.5772		0.64433		2.0573		0.54219		17.1525		0.16957
220	1.0000e-04		0.4808		0.61991		2.2116		0.54435		14.6846		0.17384
225	1.0000e-04		0.5528		0.59914		1.8783		0.54050		12.5674		0.19386
230	1.0000e-05		0.5089		0.59225		1.8643		0.54234		11.8422		0.19499
235	1.0000e-05		0.4917		0.64597		2.0560		0.54325		16.4963		0.15738
240	1.0000e-05		0.4644		0.61305		2.0091		0.54923		19.1484		0.16572
245	1.0000e-05		0.6040		0.59510		1.8487		0.54014		14.0893		0.19299
250	1.0000e-05		0.3896		0.63148		1.9860		0.52444		11.4000		0.20810
255	1.0000e-05		0.5301		0.61330		2.0980		0.51023		13.4773		0.20167
260	1.0000e-05		0.5966		0.61733		2.2418		0.50948		16.6871		0.18020
265	1.0000e-05		0.4025		0.61013		2.0055		0.52732		11.9610		0.19885
270	1.0000e-05		0.5687		0.61003		2.1180		0.53299		11.9283		0.18428
275	1.0000e-05		0.6444		0.63352		1.9442		0.55770		16.9037		0.14654
280	1.0000e-05		0.4636		0.65594		2.0299		0.50990		9.7700		0.21550
285	1.0000e-06		0.5877		0.60588		2.1894		0.50494		14.2040		0.18409
290	1.0000e-06		0.5049		0.62081		1.8820		0.54132		17.0950		0.17127
295	1.0000e-06		0.5844		0.62028		2.1200		0.51077		14.7086		0.19034
300	1.0000e-06		0.7070		0.60329		1.8812		0.54082		14.0738		0.19114
305	1.0000e-06		0.4813		0.61380		1.9554		0.53465		13.7606		0.19096
310	1.0000e-06		0.4120		0.64879		2.1000		0.51559		9.0010		0.21942
315	1.0000e-06		0.4383		0.65628		2.0740		0.54323		14.6372		0.13209
320	1.0000e-06		0.5206		0.65792		2.1614		0.53391		12.9865		0.15058
325	1.0000e-06		0.4176		0.58626		1.8630		0.54297		17.9703		0.17259
330	1.0000e-06		0.5397		0.61517		2.1484		0.52003		13.9170		0.17568
335	1.0000e-06		0.6742		0.64111		2.0766		0.52533		15.2816		0.18436
340	1.0000e-07		0.5583		0.59063		2.0342		0.52019		14.4705		0.18609
345	1.0000e-07		0.5798		0.57129		1.8739		0.55352		13.6487		0.18858
350	1.0000e-07		0.5080		0.61560		1.9319		0.55373		19.7481		0.16114
355	1.0000e-07		0.5087		0.63016		1.8097		0.54160		11.9022		0.19221
360	1.0000e-07		0.5470		0.64214		2.0764		0.51818		12.0064		0.15895
365	1.0000e-07		0.5000		0.63573		2.0212		0.51009		12.9249		0.19716
370	1.0000e-07		0.6732		0.59137		2.1943		0.54464		16.3408		0.16738
375	1.0000e-07		0.5566		0.59345		2.1694		0.53437		15.0673		0.16894
380	1.0000e-07		0.4626		0.64844		2.3885		0.50022		8.6157		0.18446
385	1.0000e-07		0.5174		0.61454		2.0881		0.51044		14.8396		0.18340
390	1.0000e-07		0.4344		0.64824		2.2171		0.53769		16.3680		0.15171
395	1.0000e-08		0.4912		0.59454		1.9332		0.53280		14.6533		0.18491
400	1.0000e-08		0.6668		0.66360		2.3876		0.51095		12.7618		0.19134
405	1.0000e-08		0.4743		0.60061		1.9771		0.52328		10.0245		0.21971
410	1.0000e-08		0.5306		0.62236		1.9769		0.52203		14.1432		0.19568
415	1.0000e-08		0.4722		0.63605		2.2806		0.52562		17.3114		0.17129
420	1.0000e-08		0.6501		0.62692		2.0714		0.55169		18.1058		0.16345
425	1.0000e-08		0.5016		0.61822		1.9353		0.54179		12.7478		0.18830
430	1.0000e-08		0.4211		0.62724		1.9212		0.52226		13.4729		0.19454
435	1.0000e-08		0.5758		0.61971		1.9112		0.52686		12.7303		0.20270
440	1.0000e-08		0.5346		0.64648		1.9618		0.54044		15.8927		0.17282
445	1.0000e-08		0.6521		0.64476		2.1488		0.51202		14.3651		0.18392
450	1.0000e-08		0.4480		0.62733		2.0941		0.51825		14.7423		0.17972
455	1.0000e-08		0.6330		0.63232		2.2001		0.50517		10.4772		0.19810
460	1.0000e-08		0.5057		0.58576		1.8149		0.52712		14.6012		0.18860
465	1.0000e-08		0.5068		0.60646		2.2409		0.49148		9.6699		0.23113
470	1.0000e-08		0.4384		0.65147		2.2246		0.53590		14.9980		0.16061
475	1.0000e-08		0.4673		0.62039		2.0759		0.51709		12.6494		0.19825
480	1.0000e-08		0.4387		0.63903		2.1763		0.50434		12.5530		0.18828
485	1.0000e-08		0.5169		0.62702		1.9966		0.52680		13.0413		0.19076
490	1.0000e-08		0.4607		0.58343		1.8175		0.54940		13.1718		0.19401
495	1.0000e-08		0.6402		0.61035		1.9572		0.53609		13.8789		0.18109
500	1.0000e-08		0.4674		0.64390		1.9661		0.52297		12.7830		0.16880
505	1.0000e-08		0.4319		0.65127		1.9531		0.52861		13.0085		0.17707
510	1.0000e-08		0.4519		0.61043		1.8432		0.56960		18.2633		0.16519
515	1.0000e-08		0.6079		0.61860		1.9262		0.51137		13.6035		0.19445
520	1.0000e-08		0.4217		0.62690		2.0132		0.54132		15.2760		0.17283
525	1.0000e-08		0.7057		0.59904		2.2456		0.49443		11.8690		0.20745
530	1.0000e-08		0.4837		0.61348		1.9338		0.54021		15.9691		0.18512
535	1.0000e-08		0.4378		0.62678		2.0511		0.52848		13.7034		0.15029
540	1.0000e-08		0.5859		0.60227		1.7916		0.56822		16.6333		0.17634
545	1.0000e-08		0.4389		0.61422		1.9870		0.53807		13.5063		0.17644
550	1.0000e-08		0.5737		0.63171		1.9948		0.56328		16.0879		0.15907
555	1.0000e-08		0.5553		0.60191		2.0960		0.51362		13.7173		0.19224
560	1.0000e-08		0.4846		0.62533		1.8261		0.56369		17.8017		0.16515
565	1.0000e-08		0.4052		0.62970		2.0325		0.53726		12.9914		0.17272
