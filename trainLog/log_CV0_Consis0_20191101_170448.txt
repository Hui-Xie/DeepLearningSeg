=============training from sratch============
Program ID: 31339

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy', '0', '3', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
      Oct 23th, 2019
      1   change to MaxPool with 2*2*2 with stride 2;
      2   Loss use GeneralizedDiceLoss function;
      
      Oct 25th, 2019
      1   add trainAllData switch
      
      Oct 29th, 2019
      1   revise reduce learnging rate at training loss;
      2   reduce min_lr = 1e-9
      
      Nov 1th, 2019
      1   erase batchNorm, for training.
        
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-11-01 17:04:48.693509
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191101_170448

Info: this is the 0th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191101_170448.
All files are in one partition.

all dataset: total 35 image files.
Network has total 254,616,050 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		0.8491		0.00032
5	1.0000e-02		0.8423		0.00000
10	1.0000e-02		0.8557		0.00000
15	1.0000e-02		0.8487		0.00000
20	1.0000e-02		0.8478		0.00000
25	1.0000e-02		0.8567		0.00000
30	1.0000e-02		0.8629		0.00000
35	1.0000e-02		0.8571		0.00000
40	1.0000e-02		0.8520		0.00000
45	1.0000e-02		0.8552		0.00000
50	1.0000e-02		0.8379		0.00000
55	1.0000e-02		0.8576		0.00000
60	1.0000e-02		0.8482		0.00000
65	1.0000e-02		0.8691		0.00000
70	1.0000e-02		0.8509		0.00000
75	1.0000e-02		0.8506		0.00000
80	1.0000e-02		0.8445		0.00000
85	1.0000e-02		0.8495		0.00000
90	1.0000e-02		0.8666		0.00000
95	1.0000e-02		0.8453		0.00000
100	1.0000e-02		0.8612		0.00000
105	1.0000e-03		0.8537		0.00000
110	1.0000e-03		0.8664		0.00000
115	1.0000e-03		0.8418		0.00000
120	1.0000e-03		0.8645		0.00000
125	1.0000e-03		0.8508		0.00000
130	1.0000e-03		0.8365		0.00000
135	1.0000e-03		0.8351		0.00000
140	1.0000e-03		0.8517		0.00000
145	1.0000e-03		0.8480		0.00000
150	1.0000e-03		0.8521		0.00000
155	1.0000e-03		0.8530		0.00000
160	1.0000e-03		0.8479		0.00000
165	1.0000e-03		0.8555		0.00000
170	1.0000e-03		0.8560		0.00000
175	1.0000e-03		0.8484		0.00000
180	1.0000e-03		0.8527		0.00000
185	1.0000e-03		0.8575		0.00000
190	1.0000e-03		0.8313		0.00000
195	1.0000e-03		0.8570		0.00000
200	1.0000e-03		0.8424		0.00000
205	1.0000e-03		0.8473		0.00000
210	1.0000e-03		0.8528		0.00000
215	1.0000e-03		0.8375		0.00000
220	1.0000e-03		0.8616		0.00000
225	1.0000e-03		0.8490		0.00000
230	1.0000e-03		0.8553		0.00000
235	1.0000e-03		0.8566		0.00000
240	1.0000e-03		0.8446		0.00000
245	1.0000e-04		0.8669		0.00000
250	1.0000e-04		0.8514		0.00000
255	1.0000e-04		0.8611		0.00000
260	1.0000e-04		0.8469		0.00000
265	1.0000e-04		0.8432		0.00000
270	1.0000e-04		0.8602		0.00000
275	1.0000e-04		0.8599		0.00000
280	1.0000e-04		0.8446		0.00000
285	1.0000e-04		0.8403		0.00000
290	1.0000e-04		0.8479		0.00000
295	1.0000e-04		0.8601		0.00000
300	1.0000e-05		0.8491		0.00000
305	1.0000e-05		0.8543		0.00000
310	1.0000e-05		0.8507		0.00000
315	1.0000e-05		0.8465		0.00000
320	1.0000e-05		0.8473		0.00000
325	1.0000e-05		0.8528		0.00000
330	1.0000e-05		0.8479		0.00000
335	1.0000e-05		0.8564		0.00000
340	1.0000e-05		0.8508		0.00000
345	1.0000e-05		0.8473		0.00000
350	1.0000e-05		0.8593		0.00000
355	1.0000e-06		0.8441		0.00000
360	1.0000e-06		0.8507		0.00000
365	1.0000e-06		0.8417		0.00000
370	1.0000e-06		0.8520		0.00000
375	1.0000e-06		0.8405		0.00000
380	1.0000e-06		0.8683		0.00000
385	1.0000e-06		0.8455		0.00000
390	1.0000e-06		0.8644		0.00000
395	1.0000e-06		0.8410		0.00000
400	1.0000e-06		0.8489		0.00000
405	1.0000e-06		0.8617		0.00000
410	1.0000e-07		0.8410		0.00000
415	1.0000e-07		0.8457		0.00000
420	1.0000e-07		0.8634		0.00000
425	1.0000e-07		0.8551		0.00000
430	1.0000e-07		0.8592		0.00000
435	1.0000e-07		0.8546		0.00000
440	1.0000e-07		0.8545		0.00000
445	1.0000e-07		0.8554		0.00000
450	1.0000e-07		0.8581		0.00000
455	1.0000e-07		0.8501		0.00000
460	1.0000e-07		0.8552		0.00000
465	1.0000e-08		0.8658		0.00000
470	1.0000e-08		0.8479		0.00000
475	1.0000e-08		0.8511		0.00000
480	1.0000e-08		0.8527		0.00000
485	1.0000e-08		0.8449		0.00000
490	1.0000e-08		0.8482		0.00000
495	1.0000e-08		0.8547		0.00000
500	1.0000e-08		0.8556		0.00000
505	1.0000e-08		0.8559		0.00000
510	1.0000e-08		0.8512		0.00000
515	1.0000e-08		0.8466		0.00000
520	1.0000e-08		0.8498		0.00000
525	1.0000e-08		0.8565		0.00000
530	1.0000e-08		0.8564		0.00000
535	1.0000e-08		0.8409		0.00000
540	1.0000e-08		0.8529		0.00000
545	1.0000e-08		0.8578		0.00000
550	1.0000e-08		0.8494		0.00000
555	1.0000e-08		0.8507		0.00000
560	1.0000e-08		0.8413		0.00000
565	1.0000e-08		0.8523		0.00000
570	1.0000e-08		0.8579		0.00000
575	1.0000e-08		0.8583		0.00000
580	1.0000e-08		0.8550		0.00000
585	1.0000e-08		0.8453		0.00000
590	1.0000e-08		0.8485		0.00000
595	1.0000e-08		0.8505		0.00000
600	1.0000e-08		0.8589		0.00000
605	1.0000e-08		0.8445		0.00000
610	1.0000e-08		0.8453		0.00000
615	1.0000e-08		0.8472		0.00000
620	1.0000e-08		0.8573		0.00000
625	1.0000e-08		0.8529		0.00000
630	1.0000e-08		0.8490		0.00000
635	1.0000e-08		0.8459		0.00000
640	1.0000e-08		0.8445		0.00000
645	1.0000e-08		0.8508		0.00000
650	1.0000e-08		0.8576		0.00000
655	1.0000e-08		0.8479		0.00000
660	1.0000e-08		0.8511		0.00000
665	1.0000e-08		0.8490		0.00000
670	1.0000e-08		0.8626		0.00000
675	1.0000e-08		0.8585		0.00000
680	1.0000e-08		0.8462		0.00000
685	1.0000e-08		0.8515		0.00000
690	1.0000e-08		0.8631		0.00000
695	1.0000e-08		0.8514		0.00000
700	1.0000e-08		0.8567		0.00000
705	1.0000e-08		0.8577		0.00000
710	1.0000e-08		0.8547		0.00000
715	1.0000e-08		0.8679		0.00000
720	1.0000e-08		0.8489		0.00000
725	1.0000e-08		0.8632		0.00000
730	1.0000e-08		0.8420		0.00000
735	1.0000e-08		0.8404		0.00000
740	1.0000e-08		0.8560		0.00000
745	1.0000e-08		0.8660		0.00000
750	1.0000e-08		0.8557		0.00000
755	1.0000e-08		0.8438		0.00000
760	1.0000e-08		0.8460		0.00000
765	1.0000e-08		0.8485		0.00000
770	1.0000e-08		0.8648		0.00000
775	1.0000e-08		0.8531		0.00000
780	1.0000e-08		0.8541		0.00000
785	1.0000e-08		0.8448		0.00000
790	1.0000e-08		0.8566		0.00000
795	1.0000e-08		0.8445		0.00000
800	1.0000e-08		0.8494		0.00000
805	1.0000e-08		0.8597		0.00000
810	1.0000e-08		0.8560		0.00000
815	1.0000e-08		0.8515		0.00000
820	1.0000e-08		0.8499		0.00000
825	1.0000e-08		0.8460		0.00000
830	1.0000e-08		0.8618		0.00000
835	1.0000e-08		0.8777		0.00000
840	1.0000e-08		0.8495		0.00000
845	1.0000e-08		0.8476		0.00000
850	1.0000e-08		0.8515		0.00000
855	1.0000e-08		0.8404		0.00000
860	1.0000e-08		0.8519		0.00000
865	1.0000e-08		0.8491		0.00000
870	1.0000e-08		0.8547		0.00000
875	1.0000e-08		0.8486		0.00000
880	1.0000e-08		0.8586		0.00000
885	1.0000e-08		0.8550		0.00000
890	1.0000e-08		0.8545		0.00000
895	1.0000e-08		0.8526		0.00000
900	1.0000e-08		0.8435		0.00000
905	1.0000e-08		0.8376		0.00000
910	1.0000e-08		0.8492		0.00000
915	1.0000e-08		0.8581		0.00000
920	1.0000e-08		0.8496		0.00000
925	1.0000e-08		0.8466		0.00000
930	1.0000e-08		0.8430		0.00000
935	1.0000e-08		0.8511		0.00000
940	1.0000e-08		0.8593		0.00000
945	1.0000e-08		0.8444		0.00000
950	1.0000e-08		0.8622		0.00000
955	1.0000e-08		0.8629		0.00000
960	1.0000e-08		0.8601		0.00000
965	1.0000e-08		0.8502		0.00000
970	1.0000e-08		0.8416		0.00000
975	1.0000e-08		0.8495		0.00000
980	1.0000e-08		0.8401		0.00000
985	1.0000e-08		0.8445		0.00000
990	1.0000e-08		0.8560		0.00000
995	1.0000e-08		0.8427		0.00000
