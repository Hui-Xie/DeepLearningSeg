=============training from sratch============
Program ID: 428

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy', '1', '3', '1']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-14 09:22:04.457360
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191014_092204

Info: this is the 1th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = True and searchWindowSize= 7

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191014_092204.
6-fold cross validation: the 1th fold is for test, the 2th fold is for validation, remaining folds are for training.

training dataset: total 19 image files.

validation dataset: total 5 image files.

test dataset: total 5 image files.
Total 19 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy
0 has 18657366 elements, with a rate of  0.9273976277637033 
1 has 1460613 elements, with a rate of  0.0726023722362967 
loss weight = tensor([ 1.0000, 12.7737])
Network has total 113,191,074 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		5.8595		0.06467		15.9329		0.18958		3.3016		0.06863
5	1.0000e-02		3.1671		0.32319		7.5009		0.36539		6.8599		0.12905
10	1.0000e-02		3.2783		0.31952		3.8701		0.40374		2.2333		0.16761
15	1.0000e-02		2.6186		0.41148		5.1961		0.41371		6.7134		0.12312
20	1.0000e-02		2.2703		0.37022		9.9696		0.43918		15.0832		0.10952
25	1.0000e-02		3.7208		0.32851		3.8656		0.41990		1.8431		0.15520
30	1.0000e-02		1.9438		0.38619		2.6769		0.43674		1.9230		0.19090
35	1.0000e-02		2.2238		0.36457		7.8151		0.23449		8.7594		0.07254
40	1.0000e-02		1.7520		0.45008		4.7539		0.36946		4.1950		0.17536
45	1.0000e-02		2.1890		0.40953		4.6326		0.39717		5.0317		0.12571
50	1.0000e-02		1.5415		0.45596		3.9943		0.34530		2.9504		0.13610
55	1.0000e-02		3.4281		0.35205		4.3753		0.30625		3.4298		0.10891
60	1.0000e-02		1.9045		0.44754		3.8679		0.36709		3.5850		0.12005
65	1.0000e-02		1.5298		0.45005		3.6224		0.40095		1.6410		0.21791
70	1.0000e-02		1.5610		0.45407		3.5657		0.40646		3.2941		0.13945
75	1.0000e-02		1.4948		0.47436		4.9705		0.31289		3.1864		0.17358
80	1.0000e-02		1.8751		0.42444		4.2596		0.36095		3.4347		0.19067
85	1.0000e-03		3.5691		0.31440		7.0645		0.32878		9.9892		0.06069
90	1.0000e-03		2.1177		0.36331		3.1316		0.38505		2.8688		0.14494
95	1.0000e-03		1.8966		0.46276		3.1139		0.42174		2.2723		0.17644
100	1.0000e-03		1.7698		0.43826		4.2540		0.39537		2.7753		0.16240
105	1.0000e-03		1.6189		0.46740		3.9823		0.38450		4.4528		0.14184
110	1.0000e-03		2.1855		0.44147		4.0502		0.43458		4.2516		0.18741
115	1.0000e-03		1.3767		0.46272		4.6633		0.41803		4.1578		0.15049
120	1.0000e-03		1.5493		0.47593		4.4016		0.37764		2.7702		0.17490
125	1.0000e-03		1.1838		0.48121		4.2581		0.36286		4.8985		0.13654
130	1.0000e-03		1.6288		0.46063		5.0580		0.39458		7.8404		0.15318
135	1.0000e-03		1.1134		0.50017		5.1496		0.34887		5.8697		0.15239
140	1.0000e-04		1.0214		0.49016		4.2767		0.36698		5.0477		0.16336
145	1.0000e-04		1.1815		0.48370		4.3652		0.36623		5.6740		0.16284
150	1.0000e-04		1.0943		0.50061		4.3620		0.36793		5.0575		0.15509
155	1.0000e-04		1.2370		0.48825		4.2714		0.36240		4.7518		0.15949
160	1.0000e-04		1.1567		0.49570		4.4444		0.35081		5.0207		0.16329
165	1.0000e-04		1.1233		0.51202		4.4197		0.35042		3.6874		0.16408
170	1.0000e-04		1.3291		0.49741		5.8781		0.34568		7.9909		0.14357
175	1.0000e-04		1.1508		0.48469		6.0796		0.34962		7.1457		0.11334
180	1.0000e-04		1.0810		0.50167		6.9359		0.32559		8.6475		0.15383
185	1.0000e-04		1.0897		0.48409		6.4624		0.33651		8.4346		0.14600
190	1.0000e-04		1.0400		0.51235		5.9362		0.31887		4.4594		0.15775
195	1.0000e-05		0.9114		0.49507		4.8970		0.35810		5.9442		0.15977
200	1.0000e-05		1.0712		0.51897		6.1118		0.33419		5.8546		0.13622
205	1.0000e-05		1.4781		0.49067		5.6827		0.33315		6.5040		0.16341
210	1.0000e-05		1.0211		0.51034		5.9814		0.32851		6.4710		0.16802
215	1.0000e-05		0.9588		0.51454		5.8880		0.33252		6.6226		0.15323
220	1.0000e-05		1.1683		0.51479		7.8151		0.32009		7.8779		0.12476
225	1.0000e-05		1.2506		0.50550		6.4252		0.30351		3.7304		0.16908
230	1.0000e-05		0.9976		0.50846		6.1739		0.34764		6.9191		0.13183
235	1.0000e-05		1.3134		0.50623		5.5808		0.33881		6.9356		0.15811
240	1.0000e-05		1.1070		0.50560		5.5998		0.34149		5.4078		0.15006
245	1.0000e-05		1.1994		0.50376		6.8695		0.32227		7.1164		0.15760
250	1.0000e-06		1.0488		0.50985		5.5666		0.32346		3.5365		0.16780
255	1.0000e-06		1.0611		0.52158		5.6785		0.32835		5.1707		0.15922
260	1.0000e-06		1.0055		0.50528		6.2382		0.32845		6.5038		0.15190
265	1.0000e-06		1.2552		0.49936		5.8703		0.33253		6.1443		0.15439
270	1.0000e-06		1.2616		0.49490		5.4567		0.37230		6.4486		0.15133
275	1.0000e-06		1.0998		0.49716		7.2023		0.32846		8.4358		0.14529
280	1.0000e-06		0.9860		0.51994		5.7952		0.32462		5.0572		0.16192
285	1.0000e-06		1.0129		0.52510		5.0416		0.34897		5.6754		0.16491
290	1.0000e-06		1.1747		0.51600		6.3454		0.32173		5.0138		0.15550
295	1.0000e-06		1.2810		0.50024		7.0450		0.33333		7.5141		0.12843
300	1.0000e-06		1.0736		0.50069		7.4597		0.29016		4.3645		0.14033
305	1.0000e-07		1.1730		0.49755		5.9583		0.32951		6.1552		0.15811
310	1.0000e-07		1.2036		0.50373		6.3751		0.34181		6.3645		0.13498
315	1.0000e-07		1.1123		0.50866		5.5852		0.33410		5.2383		0.15876
320	1.0000e-07		1.1573		0.51691		5.3659		0.33204		3.4925		0.15400
325	1.0000e-07		1.0218		0.48792		5.6677		0.33199		6.6757		0.16459
330	1.0000e-07		0.9489		0.50523		4.8206		0.35437		5.3311		0.16130
335	1.0000e-07		0.9771		0.50132		5.7165		0.33318		6.3865		0.15706
340	1.0000e-07		1.0966		0.51317		5.0983		0.33265		4.5859		0.17148
345	1.0000e-07		1.1293		0.50453		5.5572		0.33167		5.1266		0.16087
350	1.0000e-07		0.9907		0.50499		5.8591		0.33573		6.3017		0.15255
355	1.0000e-07		1.4166		0.51016		6.4697		0.32955		6.9972		0.14920
360	1.0000e-08		1.0110		0.48980		5.2259		0.34384		5.9488		0.16273
365	1.0000e-08		1.1841		0.51160		6.8587		0.32095		5.9314		0.15312
370	1.0000e-08		1.1122		0.50474		5.4923		0.32874		4.8957		0.16436
375	1.0000e-08		1.0723		0.49795		5.2464		0.34107		5.3148		0.15709
380	1.0000e-08		1.0136		0.51488		4.5188		0.36098		4.2048		0.16117
385	1.0000e-08		0.9982		0.50288		5.7095		0.34424		7.2249		0.15564
390	1.0000e-08		1.4900		0.48623		5.1210		0.33927		5.7270		0.17458
395	1.0000e-08		1.0462		0.50735		7.0433		0.33102		8.2164		0.15170
400	1.0000e-08		1.1284		0.49458		6.7294		0.32156		6.8711		0.15617
405	1.0000e-08		1.2004		0.49638		5.2259		0.35346		5.4377		0.14529
410	1.0000e-08		1.0337		0.50634		6.3216		0.35678		7.3554		0.13640
415	1.0000e-08		1.0782		0.51523		7.0041		0.30464		4.3769		0.15557
420	1.0000e-08		1.2189		0.51576		5.7701		0.32104		4.2443		0.16542
425	1.0000e-08		1.0405		0.49199		5.7931		0.33977		7.1374		0.15929
430	1.0000e-08		1.1073		0.52606		6.4974		0.34123		8.2046		0.15530
435	1.0000e-08		1.0506		0.50102		5.7052		0.35651		7.3276		0.15156
440	1.0000e-08		1.0763		0.49445		6.6949		0.33206		8.0687		0.14921
445	1.0000e-08		1.0602		0.50153		5.7196		0.34372		7.2030		0.15854
450	1.0000e-08		1.0995		0.48173		5.9532		0.34025		6.0665		0.13672
455	1.0000e-08		0.9602		0.49872		6.9042		0.33904		8.7558		0.15262
460	1.0000e-08		0.8959		0.50565		6.9340		0.33678		9.5039		0.14651
465	1.0000e-08		1.1040		0.52133		5.0218		0.34416		4.4398		0.15819
470	1.0000e-08		1.1241		0.50140		5.5478		0.33808		5.4654		0.15828
475	1.0000e-08		1.0183		0.50163		6.3770		0.32548		6.3255		0.15356
480	1.0000e-08		1.2453		0.51380		6.0023		0.32594		6.0347		0.16136
485	1.0000e-08		1.2298		0.52088		5.6583		0.35627		6.3184		0.13365
490	1.0000e-08		1.2010		0.53079		5.8094		0.34215		6.2208		0.14280
495	1.0000e-08		1.1375		0.50535		5.7648		0.33733		5.9853		0.15282
500	1.0000e-08		1.1072		0.49942		8.1951		0.32615		10.8669		0.14026
505	1.0000e-08		0.9767		0.50365		6.4917		0.33630		8.3561		0.15596
510	1.0000e-08		1.1078		0.51040		6.4491		0.32617		5.4221		0.13136
515	1.0000e-08		1.2505		0.51216		6.2054		0.34522		7.1159		0.13725
520	1.0000e-08		1.0479		0.50521		6.9891		0.34876		8.1401		0.13117
525	1.0000e-08		1.1962		0.49115		6.8713		0.32811		8.5426		0.15323
530	1.0000e-08		1.1157		0.50700		5.7417		0.34009		5.7366		0.15440
535	1.0000e-08		0.9708		0.51923		5.7782		0.32327		4.8464		0.16295
540	1.0000e-08		1.1666		0.49731		5.9014		0.33760		7.4476		0.15466
545	1.0000e-08		1.3765		0.49615		5.7215		0.34468		6.4138		0.15275
550	1.0000e-08		1.0409		0.50914		6.1724		0.33648		5.0179		0.12634
555	1.0000e-08		1.1055		0.51667		5.4528		0.35348		5.7193		0.14650
560	1.0000e-08		1.1165		0.50356		5.7298		0.34013		6.9291		0.15828
565	1.0000e-08		1.1612		0.49509		5.9041		0.33263		5.9890		0.14566
