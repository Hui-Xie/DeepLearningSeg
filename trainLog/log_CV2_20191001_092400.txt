=============training from sratch============
Program ID: 3555

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy', '2', '3,2,1']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      
         

Discarded changes:                  

Experiment setting:
Input CT data: 51*171*171 ROI around primary cancer

Loss Function:  SoftMax

Data:   total 36 patients with 50-80% label, 6-fold cross validation, test 6, validation 6, and training 24.  
    script: python3.7 statisticsLabelFiles.py 
    Total 36 in /home/hxie1/data/OvarianCancerCT/primaryROI/labels_npy
    0 has 48159408 elements, with a rate of  0.8970491562903105 
    1 has 5527068 elements, with a rate of  0.10295084370968957

Training strategy: 

          

Program starting Time: 2019-10-01 09:24:00.335447
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191001_092400

Info: this is the 2th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 6

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191001_092400.
6-fold cross validation: the 2th fold is for test, the 3th fold is for validation, remaining folds are for training.

training dataset: total 23 image files.

validation dataset: total 6 image files.

test dataset: total 6 image files.
Total 23 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy
0 has 20366782 elements, with a rate of  0.7820812354058996 
1 has 5674991 elements, with a rate of  0.21791876459410042 
loss weight = tensor([1.0000, 3.5889])
Network has total 73,047,746 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		33.1324		0.08230		23322.9688		0.00000		24328.5117		0.00000
5	1.0000e-02		8.2340		0.53900		54.4068		0.68027		79.0696		0.52682
10	1.0000e-02		5.7261		0.68125		9.3199		0.66833		7.4346		0.60505
15	1.0000e-02		5.5748		0.67083		5.2613		0.76722		4.3953		0.70373
20	1.0000e-02		5.5969		0.67929		5.1028		0.75052		4.9033		0.68809
25	1.0000e-02		5.0112		0.67167		8.0756		0.76650		4.7266		0.72836
30	1.0000e-02		5.4396		0.68035		6.6465		0.71630		5.7706		0.66441
35	1.0000e-02		4.6700		0.70604		5.8884		0.76822		4.1902		0.73066
40	1.0000e-02		5.0691		0.70335		5.8985		0.76818		4.1183		0.73637
45	1.0000e-02		4.2992		0.71347		6.3995		0.74898		5.9543		0.69333
50	1.0000e-02		4.9390		0.71043		4.7291		0.77461		4.7842		0.70590
55	1.0000e-02		4.9240		0.70984		5.7379		0.74089		5.7470		0.67551
60	1.0000e-02		5.2785		0.71453		6.4852		0.74145		7.4800		0.66061
65	1.0000e-02		5.4770		0.68176		5.8168		0.76520		4.4216		0.72327
70	1.0000e-02		4.7238		0.69557		4.3725		0.77886		4.5913		0.72023
75	1.0000e-02		4.0934		0.72992		5.7493		0.76623		5.2802		0.69716
80	1.0000e-02		4.3491		0.70991		8.6538		0.68972		7.9235		0.63779
85	1.0000e-02		4.5075		0.72917		4.0293		0.78093		3.9805		0.74027
90	1.0000e-02		4.5985		0.72745		5.4907		0.79521		4.7779		0.72153
95	1.0000e-02		4.5676		0.67700		4.3097		0.79831		5.2194		0.67430
100	1.0000e-02		4.0617		0.72639		4.5517		0.79023		3.7126		0.74878
105	1.0000e-02		3.5842		0.74907		6.1657		0.80648		3.9638		0.76672
110	1.0000e-02		3.8666		0.72882		4.5025		0.75650		4.1712		0.72991
115	1.0000e-02		4.7200		0.74345		5.4440		0.80642		4.1068		0.74907
120	1.0000e-02		3.7863		0.73941		4.1063		0.79383		4.0641		0.73105
125	1.0000e-02		3.7220		0.74613		4.6343		0.77538		3.6809		0.76529
130	1.0000e-02		3.6772		0.77403		4.9494		0.79225		3.2630		0.79147
135	1.0000e-02		4.0135		0.73291		8.2195		0.69238		4.1443		0.77030
140	1.0000e-03		3.0154		0.76953		7.8351		0.73254		3.7142		0.77146
145	1.0000e-03		3.9679		0.75347		4.4274		0.78356		3.1652		0.76186
150	1.0000e-03		3.4950		0.76457		5.0478		0.77987		3.0698		0.77733
155	1.0000e-03		3.0615		0.75879		4.7178		0.79242		3.1928		0.76856
160	1.0000e-03		3.1813		0.77094		4.5004		0.79472		3.0253		0.77304
165	1.0000e-03		3.5373		0.76273		5.0208		0.78314		2.9476		0.78088
170	1.0000e-03		2.5580		0.79575		4.8568		0.79841		3.4477		0.76746
175	1.0000e-03		2.5609		0.77148		4.5049		0.78450		3.0914		0.77033
180	1.0000e-03		3.0210		0.77495		4.6690		0.79306		3.0830		0.77543
185	1.0000e-03		2.9965		0.78026		5.8026		0.78786		2.8121		0.79233
190	1.0000e-03		2.9939		0.78376		5.1054		0.78710		2.9040		0.78173
195	1.0000e-04		3.3117		0.78068		7.6775		0.74932		3.1022		0.79315
200	1.0000e-04		3.1998		0.77605		5.7755		0.78519		2.9236		0.78710
205	1.0000e-04		3.0158		0.78303		5.2341		0.79580		2.9415		0.78193
210	1.0000e-04		2.4553		0.79192		6.5383		0.77047		3.0373		0.79002
215	1.0000e-04		2.9769		0.79225		5.7754		0.77767		2.9218		0.78395
220	1.0000e-04		3.1621		0.78115		9.1061		0.72484		3.2202		0.78771
225	1.0000e-04		2.6219		0.78996		8.2261		0.74495		2.8077		0.79815
230	1.0000e-04		3.0000		0.77319		4.5300		0.79637		3.0365		0.77161
235	1.0000e-04		2.7313		0.77788		5.5910		0.77462		2.9366		0.78341
240	1.0000e-04		3.7654		0.77650		4.9171		0.79023		2.9111		0.78382
245	1.0000e-04		2.6690		0.78052		6.1942		0.76944		2.8141		0.79311
250	1.0000e-05		2.9665		0.77890		5.5719		0.78175		2.9561		0.78660
255	1.0000e-05		2.3493		0.78779		9.1548		0.72751		3.2080		0.79301
260	1.0000e-05		2.9703		0.79768		6.9647		0.76457		3.0690		0.79093
265	1.0000e-05		2.7576		0.79437		5.7416		0.77875		2.7595		0.78985
270	1.0000e-05		2.6172		0.79969		4.7199		0.79901		2.8906		0.78297
275	1.0000e-05		2.8569		0.77752		6.7711		0.76123		2.7705		0.79305
280	1.0000e-05		2.4917		0.78516		5.1019		0.79557		2.9174		0.78906
285	1.0000e-05		2.6847		0.77937		6.1947		0.76993		2.9643		0.79001
290	1.0000e-05		2.5656		0.78588		6.5170		0.76592		2.6862		0.79338
295	1.0000e-05		2.6517		0.79609		6.1237		0.78008		2.7652		0.79760
300	1.0000e-05		2.9542		0.78327		4.8287		0.79711		2.9189		0.77713
305	1.0000e-06		2.4917		0.76820		4.5710		0.79831		3.0154		0.77253
310	1.0000e-06		2.6986		0.79318		5.3519		0.79067		2.8505		0.78979
315	1.0000e-06		3.0900		0.76818		5.2037		0.78648		2.9398		0.78299
320	1.0000e-06		2.7506		0.79631		4.9360		0.79298		2.8922		0.78329
325	1.0000e-06		2.6387		0.79033		5.6984		0.78464		2.7556		0.79227
330	1.0000e-06		2.9062		0.77894		5.3749		0.78478		3.0496		0.78736
335	1.0000e-06		2.7817		0.78843		6.9405		0.75676		2.9295		0.79153
340	1.0000e-06		2.3760		0.79607		4.6521		0.79617		2.9185		0.77793
345	1.0000e-06		2.5698		0.79722		4.5555		0.80408		2.9577		0.77812
350	1.0000e-06		2.9757		0.78076		5.1688		0.79467		2.9198		0.79106
355	1.0000e-06		2.2400		0.78268		4.9466		0.79512		2.8349		0.78745
360	1.0000e-07		3.0144		0.78467		6.0281		0.77423		2.9049		0.79112
365	1.0000e-07		2.8765		0.78706		5.6020		0.78241		2.9267		0.78819
370	1.0000e-07		2.8596		0.79877		6.2841		0.77097		2.7440		0.78866
375	1.0000e-07		2.5538		0.78254		7.1570		0.75907		2.9567		0.79021
380	1.0000e-07		2.8435		0.77650		5.5547		0.77952		2.9502		0.77549
385	1.0000e-07		2.9678		0.77326		6.2285		0.77514		2.8945		0.78415
390	1.0000e-07		2.5226		0.79755		5.1097		0.78992		3.0480		0.77908
395	1.0000e-07		3.3393		0.76886		4.6555		0.80727		2.9335		0.78343
400	1.0000e-07		2.8655		0.79958		4.8061		0.79852		2.9558		0.77754
405	1.0000e-07		2.3077		0.80421		4.8627		0.79170		2.8436		0.78298
410	1.0000e-07		2.7405		0.79510		5.5271		0.79042		2.7864		0.78921
415	1.0000e-08		2.5700		0.78286		4.7133		0.79389		2.9020		0.77583
420	1.0000e-08		2.7458		0.77941		5.5421		0.78777		2.7373		0.79091
425	1.0000e-08		2.7660		0.79847		5.4709		0.78017		2.7793		0.78747
430	1.0000e-08		2.8044		0.79923		4.8386		0.79216		2.8827		0.78002
435	1.0000e-08		3.1585		0.78052		5.2565		0.78582		2.8585		0.78308
440	1.0000e-08		2.7555		0.77158		4.8105		0.79735		2.9510		0.78295
445	1.0000e-08		2.6062		0.78963		6.4704		0.77148		2.9279		0.79428
450	1.0000e-08		2.9423		0.78730		4.8949		0.79695		2.8232		0.78288
455	1.0000e-08		3.3015		0.78764		4.9884		0.79903		2.8689		0.78436
460	1.0000e-08		2.7522		0.77703		7.1293		0.75481		2.6871		0.79611
465	1.0000e-08		3.7135		0.78157		5.1276		0.79578		3.0201		0.78713
470	1.0000e-08		3.1786		0.78846		5.2854		0.78958		2.8091		0.78606
475	1.0000e-08		2.7766		0.78423		4.9230		0.79275		2.8595		0.77949
480	1.0000e-08		3.0191		0.79494		8.1519		0.73825		2.8673		0.79875
485	1.0000e-08		2.9813		0.78405		6.2715		0.76998		2.7117		0.79275
