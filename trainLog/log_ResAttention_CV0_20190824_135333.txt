=============training from sratch============
Program ID: 22141

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/SurgicalResNeXt', '1', '/home/hxie1/data/OvarianCancerCT/pixelSize223/numpy', '/home/hxie1/data/OvarianCancerCT/patientSurgicalResults.json', '0', '2,3']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
            8    at July 30th 03:00, 2019:
                    add learning rate print;
                    use convStride =2;
                    add filter number by 2 times along deeper layers.
            9    at July 30th, 10:13, 2019:
                    add MaxPool2d in stage1;
                    add final filters to 2048.
            10   at July 30th, 15:23, 2019
                    final conv layer filter number: 1024
            11   at Aug 10th, 2019:
                    A. Add new patient data; and exclude non-standard patient data;
                    B. test the k-th fold,  validation on the (k+1)th fold;
                    C. new inputsize: 231*251*251 with pixels size 3*2*2 mm
                    D. window level shresthold [0,300]
                    E. put data padding in to converting from nrrd to numpy;
                    F. Add STN network as first laye of network
                    G. change input data into gaussian normalization in slice by slice; (x-mean)/std;
            12   Aug 12th, 2019
                    A. fix the bug that SITk reads int imape, and normalization into interger error;
                    B  test without SPN.           
            13   Aug 13th, 2019
                    A change input data into Gausssian distribution with non-zero mean,
                      it will make the padding zero do not occupy meaning of gaussian distribution.        
                    B add filters in the ResNeXt network to 128 at first stage, and reduce batchSize to 3. 
                    C Add STN at the beginning of the ResNeXt network
            14   Aug 14th, 2019
                    A. Fix the Bug the STN convert all image into 0 problem;
                    B. change DataTransform: change shear into [-30, 30], and add scale[0.6,1.2]
                    C  put STN at teh begginning of the network;
                    D  change optimizer from SGD to Adam; and change learning rate decay with gamma=0.5 per 20 steps.                                             
            15   Aug 16th, 2019
                    A The affine matrix in the STN is divided by its spectral norm;
                    B All conv layer add spectralNorm, while all FC layer do not add spectral Norm;
                    C reduce initial LR at 0.0001, and decay step to 30.
            16   Aug 17th, 2019
                    A  Cancel STN;
                    B  Add LeakyReLU;
                    C  before final FC layer, add ReLU and LocalResponseNorm   
            17   Aug 17th, 2019:10:44am
                    A enable STN;
                    B STN support LeakyReLU and localResponseNorm  
            18   Aug 17th, 2019 14:38 pm
                    A  del STN at beginining;
                    B  put STN at final 2 layer before FC. 
            19  Aug 17th 18:09 2019
                    A add surgical result file support;                    
            20  Aug 18th 08:00 am 2019
                    A  change all bias in Conv2d, and Linear into True.   
            21  Aug 19th 10:33am 2019
                    A initalize the bias in the final Linear layer as 0.3; (1-2*0.65 = -0.3)
                    16:17 pm:
                    B  add inductive bias 0.3 in the network forward function. 
                    C  initial LR =0.1, with decay steps =30                          
            22  Aug 20th 10:24am 2019
                    A change inductive bias = 0.2;
                    16:29pm:
                    B add modulation factor in the STN
            23  Aug 21th, 10:16, 2019
                    A delete the inductive bias in the final FC.  
                    15:47 pm
                    B change LRscheduler into MultiStepLR;
            24  Aug 22nd, 11:14, 2019
                    A replace ResNeXtBlock with DeformConvBlock in the stage3,4,5.
            25  Agu 23th 10:31, 2019
                    A in stage 3,4,4, reduce DeformConnBlock into 1 block;
                    B reduce the final FC layer width into 512.
                    15:10pm
                    C before regression with bias = 0, nomalize x. 
            26   Aug 24th, 2019, 13:42
                     A change + into - in all ResNeXtBlock, DeformConv2D, and STN;
                       Rational: deduct irrelevant features.                 
                    
                          
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   total 220 patients, 5-fold cross validation, test 45, validation 45, and training 130.  

Training strategy: 

          

Program starting Time: 2019-08-24 13:53:33.322090
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SurgicalResNeXt/20190824_135333

Info: this is the 0th fold leave for test in the 5-fold cross-validation.

Infor: In all data of 220 files, label 0 has 50 files,
	  and label 1 has 170 files, where positive response rate = 0.7727272727272727 in full data
5-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.
training dataset:	 total 132 files, where 1 has 102.0 with rate of 0.7727272727272727
validation dataset:	 total 44 files, where 1 has 34.0 with rate of 0.7727272727272727
test dataset:	 total 44 files, where 1 has 34.0 with rate of 0.7727272727272727
Info: batchSize = 8

=== Network trains from scratch ====
Network has total 45,564,597 parameters.
This predict surgical results.
Info: program will use GPU [2, 3] from all 4 GPUs.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	LearningRate		TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	1.0000e-01		0.3790	0.6136	0.6863	0.3667		0.3341	0.2273	0.0000	1.0000		0.3341	0.2273	0.0000	1.0000
5	1.0000e-01		0.3791	0.6288	0.6961	0.4000		0.5032	0.7727	1.0000	0.0000		0.4926	0.7727	1.0000	0.0000
10	1.0000e-01		0.3356	0.5227	0.5784	0.3333		0.3278	0.2500	0.0588	0.9000		0.3214	0.2727	0.1176	0.8000
15	1.0000e-01		0.3442	0.4924	0.5098	0.4333		0.3089	0.7727	1.0000	0.0000		0.3042	0.7727	1.0000	0.0000
20	1.0000e-01		0.3264	0.5985	0.6765	0.3333		0.3262	0.2727	0.0882	0.9000		0.3152	0.3182	0.1176	1.0000
25	1.0000e-01		0.3304	0.4848	0.4412	0.6333		0.3089	0.7727	1.0000	0.0000		0.3087	0.7727	1.0000	0.0000
30	1.0000e-01		0.3657	0.3182	0.2059	0.7000		0.3644	0.3409	0.2647	0.6000		0.4010	0.4545	0.3235	0.9000
35	1.0000e-01		0.4295	0.4394	0.3725	0.6667		0.3109	0.2500	0.0294	1.0000		0.3117	0.2045	0.0000	0.9000
40	1.0000e-01		0.3769	0.5455	0.5686	0.4667		0.4121	0.2273	0.0000	1.0000		0.3944	0.2273	0.0000	1.0000
45	1.0000e-01		0.3584	0.3939	0.3431	0.5667		0.3078	0.7273	0.8824	0.2000		0.3104	0.7045	0.8529	0.2000
50	1.0000e-02		0.3248	0.6212	0.7549	0.1667		0.3053	0.7727	0.9412	0.2000		0.2966	0.7727	0.9412	0.2000
55	1.0000e-02		0.3157	0.4924	0.5000	0.4667		0.3050	0.7727	0.9118	0.3000		0.3019	0.7500	0.8529	0.4000
60	1.0000e-02		0.3304	0.4470	0.4216	0.5333		0.3266	0.7727	1.0000	0.0000		0.3072	0.7727	1.0000	0.0000
65	1.0000e-02		0.3315	0.4773	0.4902	0.4333		0.3060	0.5227	0.4412	0.8000		0.3069	0.4773	0.3529	0.9000
70	1.0000e-02		0.3210	0.5000	0.5000	0.5000		0.3042	0.6364	0.7059	0.4000		0.3045	0.6136	0.6471	0.5000
75	1.0000e-02		0.3140	0.4848	0.4412	0.6333		0.3069	0.7727	1.0000	0.0000		0.3052	0.7727	0.9412	0.2000
80	1.0000e-02		0.3136	0.5152	0.4804	0.6333		0.3075	0.5455	0.5294	0.6000		0.3065	0.5455	0.5588	0.5000
85	1.0000e-02		0.3243	0.3636	0.2745	0.6667		0.3048	0.2500	0.0294	1.0000		0.3068	0.2955	0.0882	1.0000
90	1.0000e-02		0.3212	0.4697	0.4804	0.4333		0.3049	0.5909	0.5588	0.7000		0.3075	0.4545	0.4412	0.5000
95	1.0000e-02		0.3273	0.4848	0.4608	0.5667		0.3035	0.7045	0.8235	0.3000		0.3051	0.7045	0.7647	0.5000
100	1.0000e-02		0.3135	0.5530	0.6275	0.3000		0.3017	0.7727	0.9412	0.2000		0.3054	0.7273	0.9118	0.1000
105	1.0000e-02		0.3114	0.5530	0.5588	0.5333		0.3040	0.7727	0.9706	0.1000		0.3059	0.7727	1.0000	0.0000
110	1.0000e-02		0.3150	0.3864	0.2843	0.7333		0.3043	0.2273	0.0000	1.0000		0.3132	0.2273	0.0294	0.9000
115	1.0000e-02		0.3153	0.5530	0.5686	0.5000		0.2997	0.4091	0.2647	0.9000		0.3121	0.4091	0.3529	0.6000
120	1.0000e-02		0.3233	0.4545	0.4020	0.6333		0.2938	0.7727	0.9706	0.1000		0.3043	0.7273	0.9118	0.1000
125	1.0000e-02		0.3120	0.6212	0.6961	0.3667		0.3016	0.7500	0.9118	0.2000		0.3050	0.7273	0.8824	0.2000
130	1.0000e-02		0.3161	0.4924	0.4804	0.5333		0.2953	0.7955	0.9412	0.3000		0.3006	0.7500	0.9118	0.2000
135	1.0000e-02		0.3141	0.5152	0.5490	0.4000		0.3036	0.4318	0.3235	0.8000		0.3089	0.3864	0.2941	0.7000
140	1.0000e-02		0.3052	0.5530	0.5196	0.6667		0.3080	0.2045	0.0000	0.9000		0.3096	0.2273	0.0000	1.0000
145	1.0000e-02		0.3037	0.5303	0.4804	0.7000		0.3039	0.3636	0.2353	0.8000		0.3092	0.3636	0.2647	0.7000
150	1.0000e-03		0.3065	0.4924	0.4608	0.6000		0.3194	0.2273	0.0000	1.0000		0.3281	0.2273	0.0000	1.0000
155	1.0000e-03		0.3090	0.6061	0.6176	0.5667		0.3069	0.2500	0.0588	0.9000		0.3080	0.2727	0.0882	0.9000
160	1.0000e-03		0.3080	0.6439	0.6765	0.5333		0.3097	0.2727	0.0882	0.9000		0.3132	0.2500	0.0588	0.9000
165	1.0000e-03		0.3243	0.5758	0.6275	0.4000		0.2941	0.6591	0.6765	0.6000		0.3031	0.7045	0.7941	0.4000
170	1.0000e-03		0.2854	0.7424	0.7353	0.7667		0.3053	0.2727	0.0882	0.9000		0.3085	0.2500	0.0882	0.8000
175	1.0000e-03		0.2998	0.6515	0.6667	0.6000		0.3059	0.2727	0.0882	0.9000		0.3110	0.2955	0.1471	0.8000
180	1.0000e-03		0.3098	0.6212	0.6765	0.4333		0.2989	0.6591	0.6765	0.6000		0.3004	0.7273	0.7941	0.5000
185	1.0000e-03		0.3070	0.6212	0.6373	0.5667		0.3051	0.3409	0.2059	0.8000		0.3052	0.2955	0.1471	0.8000
190	1.0000e-03		0.2892	0.6061	0.6176	0.5667		0.3106	0.2273	0.0294	0.9000		0.3141	0.2045	0.0000	0.9000
195	1.0000e-03		0.3030	0.5833	0.5686	0.6333		0.2921	0.6818	0.6471	0.8000		0.3029	0.6364	0.6765	0.5000
200	1.0000e-03		0.2944	0.6591	0.7059	0.5000		0.3011	0.6818	0.7353	0.5000		0.2883	0.8182	0.9412	0.4000
205	1.0000e-03		0.3001	0.6061	0.5686	0.7333		0.2855	0.7727	0.7647	0.8000		0.3043	0.6591	0.7647	0.3000
210	1.0000e-03		0.3002	0.6742	0.6863	0.6333		0.2855	0.7500	0.7353	0.8000		0.2979	0.7273	0.7941	0.5000
215	1.0000e-03		0.2944	0.6667	0.6667	0.6667		0.3075	0.3864	0.2647	0.8000		0.3000	0.3636	0.2353	0.8000
220	1.0000e-03		0.2949	0.6742	0.6373	0.8000		0.2930	0.7273	0.8824	0.2000		0.3006	0.7727	0.9706	0.1000
225	1.0000e-03		0.3147	0.6212	0.6373	0.5667		0.2832	0.7273	0.7647	0.6000		0.2930	0.7500	0.8824	0.3000
230	1.0000e-03		0.3043	0.6515	0.6667	0.6000		0.2891	0.7045	0.7353	0.6000		0.2855	0.7955	0.8824	0.5000
235	1.0000e-03		0.3130	0.6136	0.6275	0.5667		0.2870	0.7273	0.7647	0.6000		0.2958	0.7727	0.9118	0.3000
240	1.0000e-03		0.2943	0.6364	0.6176	0.7000		0.2958	0.5000	0.4118	0.8000		0.2981	0.5000	0.4118	0.8000
245	1.0000e-03		0.3048	0.5909	0.5882	0.6000		0.2982	0.6136	0.6471	0.5000		0.2869	0.7273	0.7941	0.5000
250	1.0000e-03		0.2889	0.6061	0.6176	0.5667		0.2871	0.6818	0.6765	0.7000		0.2887	0.7045	0.7647	0.5000
255	1.0000e-03		0.3183	0.5909	0.6078	0.5333		0.2796	0.7273	0.7059	0.8000		0.2898	0.7500	0.8235	0.5000
260	1.0000e-03		0.3016	0.6515	0.6961	0.5000		0.2798	0.7273	0.7647	0.6000		0.3037	0.7273	0.9118	0.1000
265	1.0000e-03		0.3005	0.6136	0.6373	0.5333		0.2962	0.7045	0.7647	0.5000		0.2878	0.7955	0.9118	0.4000
270	1.0000e-03		0.2785	0.6970	0.6863	0.7333		0.2703	0.7955	0.7941	0.8000		0.2992	0.7955	0.8824	0.5000
275	1.0000e-03		0.2826	0.7045	0.6961	0.7333		0.2922	0.6364	0.6471	0.6000		0.2979	0.7045	0.7941	0.4000
280	1.0000e-03		0.2843	0.6742	0.7157	0.5333		0.2947	0.6364	0.6471	0.6000		0.2886	0.7727	0.8529	0.5000
285	1.0000e-03		0.2811	0.6439	0.6863	0.5000		0.3014	0.6591	0.7059	0.5000		0.2899	0.7500	0.7941	0.6000
290	1.0000e-03		0.2846	0.6818	0.6667	0.7333		0.2795	0.6818	0.6471	0.8000		0.2869	0.7500	0.8235	0.5000
295	1.0000e-03		0.2884	0.6288	0.6176	0.6667		0.2815	0.7727	0.8824	0.4000		0.3137	0.7955	1.0000	0.1000
300	1.0000e-04		0.2715	0.6894	0.6961	0.6667		0.2788	0.7500	0.7647	0.7000		0.3035	0.7500	0.8824	0.3000
305	1.0000e-04		0.3039	0.6364	0.6765	0.5000		0.2893	0.7045	0.7059	0.7000		0.3011	0.7045	0.8235	0.3000
310	1.0000e-04		0.2985	0.6742	0.6961	0.6000		0.2767	0.7045	0.7059	0.7000		0.2962	0.7727	0.8824	0.4000
315	1.0000e-04		0.2951	0.6894	0.7255	0.5667		0.2850	0.6591	0.6471	0.7000		0.2873	0.7500	0.8529	0.4000
320	1.0000e-04		0.2989	0.6818	0.7157	0.5667		0.2802	0.6591	0.7353	0.4000		0.3075	0.7727	0.9412	0.2000
325	1.0000e-04		0.2810	0.6212	0.6176	0.6333		0.2858	0.6818	0.6765	0.7000		0.3018	0.6818	0.7647	0.4000
330	1.0000e-04		0.2902	0.7348	0.7843	0.5667		0.2833	0.6591	0.6471	0.7000		0.2993	0.7045	0.7941	0.4000
335	1.0000e-04		0.2935	0.6364	0.6471	0.6000		0.2780	0.7045	0.7647	0.5000		0.3070	0.7955	0.9706	0.2000
340	1.0000e-04		0.2818	0.6894	0.7255	0.5667		0.2829	0.6818	0.6765	0.7000		0.3046	0.7045	0.7941	0.4000
345	1.0000e-04		0.2853	0.6970	0.7255	0.6000		0.2821	0.6591	0.6471	0.7000		0.2930	0.6818	0.7941	0.3000
350	1.0000e-04		0.2888	0.6288	0.6373	0.6000		0.2933	0.6364	0.6176	0.7000		0.3042	0.6364	0.7059	0.4000
355	1.0000e-04		0.2772	0.7348	0.7353	0.7333		0.2751	0.7045	0.7353	0.6000		0.3007	0.7500	0.8824	0.3000
360	1.0000e-04		0.2672	0.6970	0.6961	0.7000		0.2868	0.6591	0.6765	0.6000		0.2963	0.6818	0.7941	0.3000
365	1.0000e-04		0.2765	0.6894	0.7059	0.6333		0.2767	0.6591	0.6765	0.6000		0.3061	0.7727	0.9118	0.3000
370	1.0000e-04		0.2909	0.6515	0.6863	0.5333		0.2785	0.6818	0.7059	0.6000		0.2973	0.7955	0.9412	0.3000
375	1.0000e-04		0.2982	0.6894	0.7353	0.5333		0.2744	0.7045	0.7059	0.7000		0.2978	0.7273	0.8529	0.3000
380	1.0000e-04		0.2807	0.6439	0.6275	0.7000		0.2818	0.6591	0.6765	0.6000		0.3016	0.7500	0.8824	0.3000
385	1.0000e-04		0.2826	0.6515	0.6471	0.6667		0.2700	0.7500	0.7647	0.7000		0.2973	0.7727	0.8824	0.4000
390	1.0000e-04		0.2700	0.6818	0.6765	0.7000		0.2927	0.7273	0.7059	0.8000		0.3172	0.6818	0.7647	0.4000
395	1.0000e-04		0.2895	0.6894	0.6961	0.6667		0.2730	0.7045	0.7353	0.6000		0.3092	0.6818	0.8235	0.2000
400	1.0000e-04		0.3083	0.6364	0.6667	0.5333		0.2850	0.6591	0.6471	0.7000		0.2979	0.6818	0.7647	0.4000
405	1.0000e-04		0.2849	0.6515	0.6569	0.6333		0.2749	0.7045	0.6765	0.8000		0.3073	0.7045	0.8235	0.3000
410	1.0000e-04		0.2863	0.7121	0.7255	0.6667		0.2670	0.7273	0.7353	0.7000		0.3002	0.7727	0.9118	0.3000
415	1.0000e-04		0.2819	0.6439	0.6471	0.6333		0.2749	0.6818	0.6765	0.7000		0.3004	0.6591	0.7647	0.3000
420	1.0000e-04		0.2720	0.6818	0.6961	0.6333		0.2831	0.7500	0.7353	0.8000		0.3135	0.6818	0.7647	0.4000
425	1.0000e-04		0.2721	0.7045	0.6961	0.7333		0.2889	0.6591	0.6471	0.7000		0.3077	0.6364	0.7353	0.3000
430	1.0000e-04		0.2688	0.6970	0.7157	0.6333		0.2811	0.7045	0.7059	0.7000		0.3109	0.6591	0.7941	0.2000
435	1.0000e-04		0.2834	0.6591	0.6765	0.6000		0.2834	0.6591	0.7059	0.5000		0.3034	0.7727	0.9118	0.3000
440	1.0000e-04		0.2669	0.7348	0.7353	0.7333		0.2745	0.7045	0.7059	0.7000		0.2996	0.7500	0.8529	0.4000
445	1.0000e-04		0.2781	0.6894	0.6863	0.7000		0.2662	0.7045	0.7941	0.4000		0.3155	0.7955	0.9706	0.2000
450	1.0000e-04		0.2707	0.6894	0.7059	0.6333		0.2758	0.7045	0.6765	0.8000		0.3059	0.7273	0.8235	0.4000
455	1.0000e-04		0.2627	0.7500	0.7353	0.8000		0.2866	0.6364	0.6765	0.5000		0.2998	0.7273	0.8529	0.3000
460	1.0000e-04		0.2827	0.6818	0.6863	0.6667		0.2721	0.7273	0.7353	0.7000		0.3056	0.7727	0.9118	0.3000
465	1.0000e-04		0.2883	0.6515	0.6667	0.6000		0.2742	0.6818	0.7647	0.4000		0.3132	0.7955	0.9706	0.2000
470	1.0000e-04		0.2966	0.6288	0.6275	0.6333		0.2695	0.7500	0.7647	0.7000		0.2995	0.7500	0.8824	0.3000
475	1.0000e-04		0.2873	0.6515	0.6176	0.7667		0.2646	0.7727	0.7647	0.8000		0.3084	0.7955	0.9412	0.3000
480	1.0000e-04		0.3089	0.6667	0.6863	0.6000		0.2791	0.6818	0.6471	0.8000		0.3055	0.7045	0.7941	0.4000
485	1.0000e-04		0.2630	0.7045	0.7059	0.7000		0.2727	0.7273	0.7353	0.7000		0.3136	0.6818	0.8235	0.2000
490	1.0000e-04		0.2841	0.6742	0.6667	0.7000		0.2826	0.6364	0.6176	0.7000		0.3011	0.6818	0.7647	0.4000
495	1.0000e-04		0.2785	0.6667	0.6667	0.6667		0.2665	0.7045	0.7059	0.7000		0.3119	0.7273	0.8529	0.3000
500	1.0000e-04		0.2755	0.7045	0.6765	0.8000		0.2758	0.7500	0.7647	0.7000		0.3187	0.6818	0.8235	0.2000
505	1.0000e-04		0.2824	0.6667	0.6667	0.6667		0.2661	0.7955	0.7647	0.9000		0.3128	0.7273	0.8529	0.3000
510	1.0000e-04		0.2957	0.6515	0.6765	0.5667		0.2808	0.7727	0.7941	0.7000		0.3112	0.7273	0.9118	0.1000
515	1.0000e-04		0.2769	0.7121	0.7353	0.6333		0.2774	0.6591	0.6765	0.6000		0.3038	0.6591	0.7647	0.3000
520	1.0000e-04		0.3006	0.6288	0.6569	0.5333		0.2843	0.6591	0.6765	0.6000		0.2955	0.7500	0.8824	0.3000
525	1.0000e-04		0.2880	0.6515	0.6667	0.6000		0.2702	0.7045	0.7059	0.7000		0.2991	0.7045	0.8235	0.3000
530	1.0000e-04		0.2849	0.6439	0.6569	0.6000		0.2774	0.7045	0.7059	0.7000		0.3070	0.7273	0.8824	0.2000
535	1.0000e-04		0.2751	0.6894	0.6863	0.7000		0.2819	0.7045	0.7059	0.7000		0.2930	0.7045	0.8235	0.3000
540	1.0000e-04		0.2772	0.7045	0.7255	0.6333		0.2767	0.6591	0.6471	0.7000		0.3083	0.6818	0.7941	0.3000
545	1.0000e-04		0.2742	0.7273	0.7647	0.6000		0.2805	0.7045	0.7059	0.7000		0.3066	0.7273	0.8529	0.3000
550	1.0000e-04		0.2747	0.7121	0.7157	0.7000		0.2766	0.7045	0.7353	0.6000		0.3053	0.7727	0.9118	0.3000
555	1.0000e-04		0.2734	0.6667	0.6765	0.6333		0.2646	0.7045	0.7059	0.7000		0.2973	0.7045	0.8235	0.3000
560	1.0000e-04		0.2566	0.6970	0.6765	0.7667		0.2623	0.7500	0.7941	0.6000		0.3116	0.7273	0.8529	0.3000
565	1.0000e-04		0.2709	0.7500	0.7745	0.6667		0.2859	0.6818	0.6765	0.7000		0.3048	0.7045	0.8235	0.3000
570	1.0000e-04		0.2744	0.6970	0.6863	0.7333		0.2797	0.6591	0.6471	0.7000		0.3111	0.6818	0.7941	0.3000
575	1.0000e-04		0.3036	0.6439	0.6569	0.6000		0.2774	0.6591	0.6176	0.8000		0.3130	0.6818	0.7647	0.4000
580	1.0000e-04		0.2905	0.6894	0.6961	0.6667		0.2916	0.6591	0.6176	0.8000		0.3144	0.6591	0.7647	0.3000
585	1.0000e-04		0.2635	0.6970	0.6863	0.7333		0.2887	0.7045	0.7059	0.7000		0.3076	0.7500	0.8824	0.3000
590	1.0000e-04		0.2865	0.6364	0.6373	0.6333		0.2922	0.6591	0.7059	0.5000		0.3052	0.7045	0.8529	0.2000
595	1.0000e-04		0.2927	0.6742	0.6863	0.6333		0.2868	0.5909	0.5588	0.7000		0.3127	0.6591	0.7353	0.4000
600	1.0000e-04		0.2811	0.6742	0.6863	0.6333		0.3006	0.6591	0.6765	0.6000		0.2922	0.7500	0.8235	0.5000
605	1.0000e-04		0.2392	0.7803	0.7745	0.8000		0.2866	0.7045	0.7059	0.7000		0.3185	0.7273	0.8529	0.3000
610	1.0000e-04		0.2680	0.7045	0.7255	0.6333		0.2904	0.6818	0.6765	0.7000		0.3094	0.7273	0.8529	0.3000
615	1.0000e-04		0.2793	0.6742	0.6765	0.6667		0.2809	0.6591	0.6765	0.6000		0.2924	0.6818	0.7647	0.4000
620	1.0000e-04		0.2771	0.7121	0.7059	0.7333		0.2946	0.6364	0.6176	0.7000		0.3079	0.6364	0.7353	0.3000
625	1.0000e-04		0.2771	0.6515	0.6765	0.5667		0.2732	0.7045	0.7353	0.6000		0.2987	0.7500	0.8824	0.3000
630	1.0000e-04		0.2585	0.7727	0.7941	0.7000		0.2822	0.7500	0.7647	0.7000		0.3153	0.7273	0.8529	0.3000
635	1.0000e-04		0.2919	0.6742	0.7157	0.5333		0.2754	0.7045	0.7059	0.7000		0.3071	0.7045	0.7941	0.4000
640	1.0000e-04		0.2610	0.7348	0.7549	0.6667		0.2725	0.6818	0.6765	0.7000		0.2981	0.7273	0.8235	0.4000
645	1.0000e-04		0.2511	0.7273	0.7157	0.7667		0.2951	0.6818	0.6765	0.7000		0.3004	0.7045	0.8235	0.3000
650	1.0000e-04		0.2861	0.6818	0.6765	0.7000		0.2721	0.7045	0.7059	0.7000		0.2969	0.7727	0.8824	0.4000
655	1.0000e-04		0.2775	0.6667	0.6863	0.6000		0.2746	0.6591	0.6471	0.7000		0.2948	0.7500	0.8235	0.5000
660	1.0000e-04		0.3042	0.6742	0.7059	0.5667		0.2775	0.7045	0.7059	0.7000		0.3022	0.7727	0.8824	0.4000
665	1.0000e-04		0.2782	0.7273	0.7353	0.7000		0.2647	0.7500	0.7647	0.7000		0.3165	0.7045	0.8529	0.2000
670	1.0000e-04		0.2586	0.6818	0.6765	0.7000		0.2928	0.6591	0.6176	0.8000		0.3052	0.7045	0.8235	0.3000
675	1.0000e-04		0.2642	0.7197	0.7157	0.7333		0.2692	0.7500	0.7647	0.7000		0.3048	0.7727	0.9412	0.2000
680	1.0000e-04		0.2532	0.7500	0.7451	0.7667		0.2806	0.6591	0.6765	0.6000		0.2961	0.7045	0.8235	0.3000
685	1.0000e-04		0.2577	0.7197	0.7157	0.7333		0.2912	0.6818	0.6471	0.8000		0.3061	0.6591	0.7647	0.3000
690	1.0000e-04		0.2524	0.7576	0.7353	0.8333		0.2758	0.7045	0.7353	0.6000		0.3138	0.6818	0.8235	0.2000
695	1.0000e-04		0.2529	0.7197	0.7353	0.6667		0.2856	0.6818	0.7353	0.5000		0.3164	0.7273	0.8824	0.2000
