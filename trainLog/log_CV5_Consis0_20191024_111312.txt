=============training from sratch============
Program ID: 23899

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy', '5', '3', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
      Oct 23th, 2019
      1   change to MaxPool with 2*2*2 with stride 2;
      2   Loss use GeneralizedDiceLoss function;
        
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-24 11:13:12.825825
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191024_111312

Info: this is the 5th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191024_111312.
6-fold cross validation: the 5th fold is for test, the 0th fold is for validation, remaining folds are for training.

training dataset: total 24 image files.

validation dataset: total 6 image files.

test dataset: total 5 image files.
Total 24 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy
0 has 22825464 elements, with a rate of  0.898209457321732 
1 has 2586720 elements, with a rate of  0.10179054267826804 
loss weight = tensor([1.0000, 8.8241])
Network has total 254,652,146 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		0.8040		0.20586		0.7329		0.00000		0.9096		0.00000
5	1.0000e-02		0.5825		0.36083		0.4118		0.51118		0.6536		0.33343
10	1.0000e-02		0.5379		0.40131		0.3957		0.57036		0.6730		0.31865
15	1.0000e-02		0.5081		0.44334		0.5249		0.30440		0.6647		0.29436
20	1.0000e-02		0.5470		0.39311		0.7323		0.00248		0.8793		0.03006
25	1.0000e-02		0.4995		0.44163		0.4395		0.53624		0.7142		0.27710
30	1.0000e-02		0.4874		0.47132		0.4652		0.39509		0.5400		0.44377
35	1.0000e-02		0.4776		0.46976		0.6020		0.20233		0.6435		0.32268
40	1.0000e-02		0.4317		0.52511		0.4448		0.43044		0.5811		0.39521
45	1.0000e-02		0.4664		0.48543		0.4132		0.47747		0.4972		0.49145
50	1.0000e-02		0.4346		0.53354		0.2985		0.66228		0.5719		0.41678
55	1.0000e-02		0.3753		0.58799		0.5427		0.26642		0.7388		0.20137
60	1.0000e-02		0.3803		0.57728		0.2857		0.66564		0.5293		0.45893
65	1.0000e-02		0.3898		0.58571		0.3624		0.56529		0.5174		0.47121
70	1.0000e-02		0.3820		0.57375		0.3614		0.55441		0.5300		0.45790
75	1.0000e-02		0.3796		0.58958		0.3957		0.49611		0.5460		0.44177
80	1.0000e-02		0.3747		0.59688		0.2850		0.67353		0.5270		0.46280
85	1.0000e-02		0.3189		0.64384		0.2901		0.66766		0.5643		0.42509
90	1.0000e-02		0.3967		0.55460		0.3202		0.63833		0.6102		0.37965
95	1.0000e-02		0.3164		0.65588		0.3256		0.61011		0.5338		0.45563
100	1.0000e-02		0.2854		0.67358		0.3765		0.52957		0.5874		0.38867
105	1.0000e-02		0.3002		0.67375		0.3075		0.64100		0.5421		0.44417
110	1.0000e-02		0.2727		0.71082		0.3068		0.63867		0.5360		0.45026
115	1.0000e-02		0.2929		0.68362		0.3173		0.61855		0.5106		0.48204
120	1.0000e-02		0.2785		0.69525		0.2922		0.65996		0.5295		0.45982
125	1.0000e-02		0.2703		0.70507		0.3269		0.61121		0.5146		0.47374
130	1.0000e-02		0.2522		0.72947		0.4746		0.39466		0.6004		0.37750
135	1.0000e-03		0.2688		0.71050		0.3935		0.47877		0.5966		0.38035
140	1.0000e-03		0.2430		0.74092		0.3136		0.62139		0.5270		0.45958
145	1.0000e-03		0.2331		0.74771		0.2942		0.65302		0.5195		0.47143
150	1.0000e-03		0.2163		0.77234		0.3290		0.60684		0.5296		0.45985
155	1.0000e-03		0.1953		0.79499		0.3305		0.60551		0.5399		0.45027
160	1.0000e-03		0.2105		0.77771		0.3756		0.53650		0.5187		0.47128
165	1.0000e-03		0.1848		0.80444		0.3721		0.54000		0.5150		0.47559
170	1.0000e-03		0.1721		0.81806		0.3613		0.55920		0.5224		0.46757
175	1.0000e-03		0.1642		0.82571		0.3562		0.57163		0.5425		0.44810
180	1.0000e-03		0.1758		0.81373		0.3610		0.56361		0.5400		0.45063
185	1.0000e-03		0.1674		0.82080		0.4066		0.49178		0.5303		0.45910
190	1.0000e-04		0.1549		0.83711		0.4141		0.48968		0.5316		0.45859
195	1.0000e-04		0.1988		0.78957		0.4020		0.51131		0.5443		0.44605
200	1.0000e-04		0.1658		0.82338		0.4307		0.45755		0.5381		0.44916
205	1.0000e-04		0.1512		0.83941		0.4101		0.49130		0.5332		0.45567
210	1.0000e-04		0.1639		0.82790		0.3977		0.50982		0.5316		0.45829
215	1.0000e-04		0.1674		0.81770		0.4155		0.48553		0.5389		0.45125
220	1.0000e-04		0.1634		0.82484		0.3992		0.50838		0.5417		0.44837
225	1.0000e-04		0.1555		0.83528		0.3930		0.51980		0.5471		0.44304
230	1.0000e-04		0.1549		0.83259		0.3901		0.52084		0.5318		0.45808
235	1.0000e-04		0.1440		0.84729		0.3973		0.51015		0.5367		0.45359
240	1.0000e-04		0.1575		0.83545		0.4711		0.39616		0.5307		0.45785
245	1.0000e-05		0.1495		0.84306		0.4051		0.49653		0.5224		0.46722
250	1.0000e-05		0.1574		0.83305		0.4218		0.47353		0.5330		0.45714
255	1.0000e-05		0.1531		0.83539		0.3645		0.55777		0.5261		0.46452
260	1.0000e-05		0.1631		0.82747		0.4477		0.43630		0.5456		0.44469
265	1.0000e-05		0.1758		0.81389		0.4309		0.46405		0.5496		0.44161
270	1.0000e-05		0.1545		0.83658		0.4250		0.47148		0.5443		0.44626
275	1.0000e-05		0.1606		0.83120		0.4393		0.44917		0.5408		0.44951
280	1.0000e-05		0.1498		0.84207		0.3955		0.51200		0.5370		0.45370
285	1.0000e-05		0.1547		0.83720		0.4075		0.49701		0.5340		0.45658
290	1.0000e-05		0.1448		0.84729		0.4639		0.40032		0.5287		0.46002
295	1.0000e-05		0.1615		0.83058		0.4530		0.42708		0.5466		0.44378
300	1.0000e-06		0.1499		0.84105		0.4438		0.44339		0.5474		0.44351
305	1.0000e-06		0.1432		0.84715		0.4181		0.47760		0.5281		0.46176
310	1.0000e-06		0.1724		0.82097		0.4326		0.46012		0.5476		0.44300
315	1.0000e-06		0.1579		0.83395		0.4236		0.47149		0.5375		0.45259
320	1.0000e-06		0.1526		0.83774		0.3869		0.52480		0.5343		0.45625
325	1.0000e-06		0.1635		0.82736		0.3958		0.51495		0.5429		0.44772
330	1.0000e-06		0.1715		0.81654		0.4281		0.46098		0.5261		0.46375
335	1.0000e-06		0.1762		0.81291		0.4021		0.50531		0.5461		0.44453
340	1.0000e-06		0.1524		0.83919		0.4403		0.44703		0.5350		0.45545
345	1.0000e-06		0.1559		0.83656		0.4052		0.50007		0.5415		0.44945
350	1.0000e-06		0.1479		0.84276		0.4418		0.43985		0.5405		0.44923
355	1.0000e-07		0.1842		0.80718		0.4238		0.47306		0.5425		0.44793
360	1.0000e-07		0.1706		0.82099		0.4156		0.48693		0.5479		0.44273
365	1.0000e-07		0.1679		0.82394		0.3941		0.51809		0.5442		0.44683
370	1.0000e-07		0.1632		0.82685		0.4273		0.46542		0.5332		0.45729
375	1.0000e-07		0.1587		0.83334		0.4278		0.46363		0.5387		0.45177
380	1.0000e-07		0.1516		0.83975		0.4235		0.46887		0.5319		0.45841
385	1.0000e-07		0.1451		0.84654		0.4398		0.44261		0.5311		0.45921
390	1.0000e-07		0.1650		0.82551		0.4423		0.43826		0.5295		0.46042
395	1.0000e-07		0.1535		0.83842		0.4002		0.50754		0.5452		0.44580
400	1.0000e-07		0.1604		0.82980		0.3884		0.52474		0.5296		0.46088
