=============training from sratch============
Program ID: 12195

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy', '4', '2', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
       
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-19 09:25:45.437864
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191019_092545

Info: this is the 4th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191019_092545.
6-fold cross validation: the 4th fold is for test, the 5th fold is for validation, remaining folds are for training.

training dataset: total 24 image files.

validation dataset: total 5 image files.

test dataset: total 6 image files.
Total 24 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy
0 has 19588550 elements, with a rate of  0.7708329988481116 
1 has 5823634 elements, with a rate of  0.2291670011518884 
loss weight = tensor([1.0000, 3.3636])
Network has total 254,652,146 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		4.5311		0.26071		3.3383		0.36287		4.1241		0.44852
5	1.0000e-02		2.3930		0.56888		3.7780		0.53765		2.9557		0.57257
10	1.0000e-02		2.3210		0.63804		3.5086		0.54506		2.7690		0.56399
15	1.0000e-02		1.8249		0.71832		5.7747		0.52505		4.7729		0.57108
20	1.0000e-02		2.0523		0.65218		2.6725		0.57491		1.8475		0.65421
25	1.0000e-02		1.8573		0.69175		5.8427		0.55528		4.7219		0.54240
30	1.0000e-02		1.4862		0.71587		1.8755		0.63235		1.6817		0.68441
35	1.0000e-02		1.9590		0.67380		2.3814		0.59253		2.0096		0.63131
40	1.0000e-02		1.7906		0.68730		2.7126		0.45067		3.1010		0.40021
45	1.0000e-02		1.5906		0.66979		3.2851		0.45935		4.6174		0.39816
50	1.0000e-02		1.6667		0.71992		3.4583		0.45080		1.8918		0.67022
55	1.0000e-02		1.5776		0.75522		6.5359		0.49751		7.1783		0.57091
60	1.0000e-02		1.4685		0.73308		9.7435		0.38488		4.0857		0.59242
65	1.0000e-02		1.4174		0.74042		4.1610		0.50334		2.5574		0.62480
70	1.0000e-02		1.3712		0.74317		3.3421		0.54619		2.3402		0.52921
75	1.0000e-02		1.0116		0.76442		4.1046		0.44266		2.3060		0.64860
80	1.0000e-02		1.1063		0.79227		2.5587		0.60289		1.5333		0.67871
85	1.0000e-03		1.4869		0.75506		4.2209		0.47550		2.1689		0.64687
90	1.0000e-03		0.9952		0.79042		3.7817		0.51994		2.1417		0.65340
95	1.0000e-03		0.8860		0.80041		3.5338		0.47964		2.2397		0.64739
100	1.0000e-03		0.9767		0.80424		3.5783		0.50276		2.1502		0.65066
105	1.0000e-03		0.8316		0.81061		3.0657		0.50533		2.3464		0.63883
110	1.0000e-03		0.9426		0.80928		3.4322		0.49842		1.9959		0.64298
115	1.0000e-03		0.8046		0.81765		3.0227		0.54253		1.8516		0.65543
120	1.0000e-03		0.7078		0.83157		4.0171		0.49567		2.7998		0.62833
125	1.0000e-03		0.8260		0.81895		3.4516		0.53889		2.2885		0.64514
130	1.0000e-03		0.7373		0.81597		4.4435		0.48252		2.6640		0.63694
135	1.0000e-03		0.8666		0.81680		3.5131		0.53954		1.9106		0.67907
140	1.0000e-04		0.9007		0.82694		5.1140		0.44743		3.0316		0.56613
145	1.0000e-04		0.6980		0.82607		4.8159		0.45300		2.1480		0.64878
150	1.0000e-04		0.6656		0.83258		3.8885		0.56980		4.2529		0.58308
155	1.0000e-04		0.6343		0.83628		3.4817		0.56039		3.1386		0.57343
160	1.0000e-04		0.9173		0.82840		3.7861		0.52978		2.3699		0.58867
165	1.0000e-04		0.6441		0.82667		3.9827		0.50765		2.0448		0.63221
170	1.0000e-04		0.7195		0.83384		3.9631		0.52167		3.0411		0.57805
175	1.0000e-04		0.6166		0.83867		3.8640		0.59349		4.0051		0.58282
180	1.0000e-04		0.7010		0.84156		4.2119		0.55722		3.9015		0.56657
185	1.0000e-04		0.6908		0.83692		3.9752		0.50295		1.9927		0.66059
190	1.0000e-04		0.6747		0.83429		3.7155		0.55415		3.5412		0.57646
195	1.0000e-05		0.7960		0.83729		3.7391		0.54414		3.1705		0.56937
200	1.0000e-05		0.6966		0.83185		4.3884		0.50889		3.4359		0.56850
205	1.0000e-05		0.6627		0.83309		3.9289		0.57166		4.2766		0.58028
210	1.0000e-05		0.6594		0.83567		3.9012		0.51473		1.9873		0.65019
215	1.0000e-05		0.7491		0.82172		4.6395		0.46485		2.1195		0.67974
220	1.0000e-05		0.7661		0.83123		4.2431		0.49440		2.3000		0.63784
225	1.0000e-05		0.6607		0.83797		3.8216		0.60146		4.1020		0.57259
230	1.0000e-05		0.6848		0.83483		3.8882		0.52394		2.3141		0.59042
235	1.0000e-05		0.6186		0.83866		4.2206		0.54757		2.8758		0.57648
240	1.0000e-05		0.6656		0.82784		3.8497		0.51660		2.1250		0.61984
245	1.0000e-05		0.7280		0.82981		4.1290		0.59675		4.9525		0.57896
250	1.0000e-06		0.7384		0.82286		5.7360		0.42483		2.7525		0.65472
255	1.0000e-06		0.6248		0.83932		4.2955		0.48428		1.9570		0.67957
260	1.0000e-06		0.6766		0.83668		4.1291		0.49999		2.0533		0.65233
265	1.0000e-06		0.6042		0.83998		3.5987		0.59429		3.7029		0.57971
270	1.0000e-06		0.6293		0.82880		3.8505		0.52840		2.0918		0.62386
275	1.0000e-06		0.6499		0.83607		3.7916		0.57058		2.5770		0.57023
