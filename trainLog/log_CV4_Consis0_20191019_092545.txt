=============training from sratch============
Program ID: 12195

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy', '4', '2', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
       
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-19 09:25:45.437864
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191019_092545

Info: this is the 4th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191019_092545.
6-fold cross validation: the 4th fold is for test, the 5th fold is for validation, remaining folds are for training.

training dataset: total 24 image files.

validation dataset: total 5 image files.

test dataset: total 6 image files.
Total 24 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy
0 has 19588550 elements, with a rate of  0.7708329988481116 
1 has 5823634 elements, with a rate of  0.2291670011518884 
loss weight = tensor([1.0000, 3.3636])
Network has total 254,652,146 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		4.5311		0.26071		3.3383		0.36287		4.1241		0.44852
5	1.0000e-02		2.3930		0.56888		3.7780		0.53765		2.9557		0.57257
10	1.0000e-02		2.3210		0.63804		3.5086		0.54506		2.7690		0.56399
15	1.0000e-02		1.8249		0.71832		5.7747		0.52505		4.7729		0.57108
20	1.0000e-02		2.0523		0.65218		2.6725		0.57491		1.8475		0.65421
25	1.0000e-02		1.8573		0.69175		5.8427		0.55528		4.7219		0.54240
30	1.0000e-02		1.4862		0.71587		1.8755		0.63235		1.6817		0.68441
35	1.0000e-02		1.9590		0.67380		2.3814		0.59253		2.0096		0.63131
40	1.0000e-02		1.7906		0.68730		2.7126		0.45067		3.1010		0.40021
45	1.0000e-02		1.5906		0.66979		3.2851		0.45935		4.6174		0.39816
50	1.0000e-02		1.6667		0.71992		3.4583		0.45080		1.8918		0.67022
55	1.0000e-02		1.5776		0.75522		6.5359		0.49751		7.1783		0.57091
60	1.0000e-02		1.4685		0.73308		9.7435		0.38488		4.0857		0.59242
65	1.0000e-02		1.4174		0.74042		4.1610		0.50334		2.5574		0.62480
70	1.0000e-02		1.3712		0.74317		3.3421		0.54619		2.3402		0.52921
75	1.0000e-02		1.0116		0.76442		4.1046		0.44266		2.3060		0.64860
80	1.0000e-02		1.1063		0.79227		2.5587		0.60289		1.5333		0.67871
85	1.0000e-03		1.4869		0.75506		4.2209		0.47550		2.1689		0.64687
90	1.0000e-03		0.9952		0.79042		3.7817		0.51994		2.1417		0.65340
95	1.0000e-03		0.8860		0.80041		3.5338		0.47964		2.2397		0.64739
100	1.0000e-03		0.9767		0.80424		3.5783		0.50276		2.1502		0.65066
105	1.0000e-03		0.8316		0.81061		3.0657		0.50533		2.3464		0.63883
110	1.0000e-03		0.9426		0.80928		3.4322		0.49842		1.9959		0.64298
115	1.0000e-03		0.8046		0.81765		3.0227		0.54253		1.8516		0.65543
120	1.0000e-03		0.7078		0.83157		4.0171		0.49567		2.7998		0.62833
125	1.0000e-03		0.8260		0.81895		3.4516		0.53889		2.2885		0.64514
130	1.0000e-03		0.7373		0.81597		4.4435		0.48252		2.6640		0.63694
135	1.0000e-03		0.8666		0.81680		3.5131		0.53954		1.9106		0.67907
140	1.0000e-04		0.9007		0.82694		5.1140		0.44743		3.0316		0.56613
145	1.0000e-04		0.6980		0.82607		4.8159		0.45300		2.1480		0.64878
150	1.0000e-04		0.6656		0.83258		3.8885		0.56980		4.2529		0.58308
155	1.0000e-04		0.6343		0.83628		3.4817		0.56039		3.1386		0.57343
160	1.0000e-04		0.9173		0.82840		3.7861		0.52978		2.3699		0.58867
165	1.0000e-04		0.6441		0.82667		3.9827		0.50765		2.0448		0.63221
170	1.0000e-04		0.7195		0.83384		3.9631		0.52167		3.0411		0.57805
175	1.0000e-04		0.6166		0.83867		3.8640		0.59349		4.0051		0.58282
180	1.0000e-04		0.7010		0.84156		4.2119		0.55722		3.9015		0.56657
185	1.0000e-04		0.6908		0.83692		3.9752		0.50295		1.9927		0.66059
190	1.0000e-04		0.6747		0.83429		3.7155		0.55415		3.5412		0.57646
195	1.0000e-05		0.7960		0.83729		3.7391		0.54414		3.1705		0.56937
200	1.0000e-05		0.6966		0.83185		4.3884		0.50889		3.4359		0.56850
205	1.0000e-05		0.6627		0.83309		3.9289		0.57166		4.2766		0.58028
210	1.0000e-05		0.6594		0.83567		3.9012		0.51473		1.9873		0.65019
215	1.0000e-05		0.7491		0.82172		4.6395		0.46485		2.1195		0.67974
220	1.0000e-05		0.7661		0.83123		4.2431		0.49440		2.3000		0.63784
225	1.0000e-05		0.6607		0.83797		3.8216		0.60146		4.1020		0.57259
230	1.0000e-05		0.6848		0.83483		3.8882		0.52394		2.3141		0.59042
235	1.0000e-05		0.6186		0.83866		4.2206		0.54757		2.8758		0.57648
240	1.0000e-05		0.6656		0.82784		3.8497		0.51660		2.1250		0.61984
245	1.0000e-05		0.7280		0.82981		4.1290		0.59675		4.9525		0.57896
250	1.0000e-06		0.7384		0.82286		5.7360		0.42483		2.7525		0.65472
255	1.0000e-06		0.6248		0.83932		4.2955		0.48428		1.9570		0.67957
260	1.0000e-06		0.6766		0.83668		4.1291		0.49999		2.0533		0.65233
265	1.0000e-06		0.6042		0.83998		3.5987		0.59429		3.7029		0.57971
270	1.0000e-06		0.6293		0.82880		3.8505		0.52840		2.0918		0.62386
275	1.0000e-06		0.6499		0.83607		3.7916		0.57058		2.5770		0.57023
280	1.0000e-06		0.6551		0.84755		3.8449		0.59658		3.7320		0.57513
285	1.0000e-06		0.5918		0.84462		3.7444		0.53621		2.2460		0.59801
290	1.0000e-06		0.6604		0.84618		3.6930		0.55123		2.3766		0.59642
295	1.0000e-06		0.5628		0.83941		5.0008		0.45641		2.3826		0.66885
300	1.0000e-06		0.6071		0.83088		4.0095		0.53770		2.7113		0.58995
305	1.0000e-07		0.7288		0.84176		4.0904		0.54308		3.0024		0.57000
310	1.0000e-07		0.6127		0.83543		3.9766		0.50752		2.0150		0.64520
315	1.0000e-07		0.6257		0.84270		4.5307		0.48985		2.2024		0.65873
320	1.0000e-07		0.5937		0.83834		4.4098		0.59373		4.7845		0.58267
325	1.0000e-07		0.7025		0.84313		3.6380		0.57812		3.1639		0.58042
330	1.0000e-07		0.5814		0.83109		6.9214		0.38647		3.0493		0.63687
335	1.0000e-07		0.5567		0.83774		3.8968		0.51414		1.8943		0.66576
340	1.0000e-07		0.6696		0.83940		3.4972		0.58025		3.6263		0.58278
345	1.0000e-07		0.6888		0.81915		4.4777		0.48362		1.9814		0.67070
350	1.0000e-07		0.6880		0.83440		4.1628		0.50345		2.0773		0.65429
355	1.0000e-07		1.0534		0.82140		4.9796		0.45710		2.4706		0.65888
360	1.0000e-08		0.7026		0.82798		4.0057		0.54459		2.7555		0.57369
365	1.0000e-08		0.7972		0.83568		4.2347		0.53786		2.5167		0.58585
370	1.0000e-08		0.6248		0.83557		4.0484		0.60184		4.1966		0.57707
375	1.0000e-08		0.6178		0.84193		3.8959		0.51447		1.9559		0.65620
380	1.0000e-08		0.6357		0.83625		4.0456		0.53857		2.8641		0.57405
385	1.0000e-08		0.6314		0.83191		4.1494		0.49641		1.8883		0.68065
390	1.0000e-08		0.8140		0.84677		4.4627		0.59197		4.8017		0.57615
395	1.0000e-08		0.7757		0.83934		4.4071		0.58742		4.3334		0.57208
400	1.0000e-08		0.6905		0.84193		4.2527		0.58966		4.6814		0.57968
405	1.0000e-08		0.6658		0.83219		3.9388		0.53511		2.2296		0.62399
410	1.0000e-08		0.7043		0.84096		4.7771		0.46353		2.3284		0.66921
415	1.0000e-08		0.7128		0.81839		3.7238		0.53807		2.4536		0.59999
420	1.0000e-08		0.6306		0.84345		4.1178		0.56425		3.3543		0.57438
425	1.0000e-08		0.6502		0.84373		3.9568		0.58436		2.7630		0.57438
430	1.0000e-08		0.6868		0.83797		3.7667		0.53920		2.5078		0.59254
435	1.0000e-08		0.8729		0.83093		4.3127		0.50825		2.8445		0.58732
440	1.0000e-08		0.6938		0.84061		3.3904		0.58625		2.5957		0.57476
445	1.0000e-08		0.8165		0.83887		4.2559		0.58576		4.3253		0.57143
450	1.0000e-08		0.7052		0.83138		4.1681		0.51369		1.9562		0.66528
455	1.0000e-08		0.7749		0.83680		3.6064		0.58039		3.2173		0.57555
460	1.0000e-08		0.5951		0.83800		4.7205		0.46122		2.2112		0.67508
465	1.0000e-08		0.5864		0.83845		4.1880		0.51809		2.1562		0.63609
470	1.0000e-08		0.5969		0.83464		4.3446		0.51951		2.2878		0.61687
475	1.0000e-08		0.6948		0.83111		3.8994		0.53193		2.4282		0.59699
480	1.0000e-08		0.8759		0.83840		4.4662		0.57330		3.9379		0.56533
485	1.0000e-08		0.6778		0.83651		4.3839		0.52192		2.5286		0.59083
490	1.0000e-08		0.6699		0.83764		3.9180		0.55696		2.4883		0.58807
495	1.0000e-08		0.6875		0.83541		3.7537		0.53161		2.0919		0.62501
500	1.0000e-08		0.6321		0.83884		3.8548		0.56204		3.2567		0.57863
505	1.0000e-08		0.7585		0.83936		4.1168		0.57288		3.7006		0.57437
510	1.0000e-08		0.6985		0.83494		3.7604		0.55649		2.4293		0.58930
515	1.0000e-08		0.7287		0.83848		4.0460		0.54853		2.9846		0.55396
520	1.0000e-08		0.7157		0.83833		3.9094		0.54629		2.5898		0.58066
525	1.0000e-08		0.6868		0.84322		4.6456		0.59428		5.3191		0.58501
530	1.0000e-08		0.5940		0.84050		3.9803		0.58383		4.1339		0.57227
535	1.0000e-08		0.7037		0.83302		5.0622		0.45213		2.5446		0.65568
540	1.0000e-08		0.7298		0.83291		4.1779		0.56084		3.7225		0.56276
545	1.0000e-08		0.7134		0.83646		3.4030		0.58749		2.7676		0.57307
550	1.0000e-08		0.7202		0.84046		3.9362		0.53623		2.1100		0.62273
