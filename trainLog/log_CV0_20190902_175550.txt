=============training from sratch============
Program ID: 18309

Program command: 
 ['TrainResNeXtVNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/ResNeXtVNet', '1', '/home/hxie1/data/OvarianCancerCT/pixelSize223withLabel/numpy', '/home/hxie1/data/OvarianCancerCT/pixelSize223withLabel/numpyLabel', '0', '3,2,1']

Major program changes: 
     1  a V model with ResNeXt block: use z convolution, and then xy convolution, to implement 3D convolution.
     2  at ground truth, only check the segmented slices, about 3 slices per patient;
     3  the input is whole 3D volume, instead of ROI around a segmented slice;
     4  support input data augmentation: affine in xy plane, and translation in z direction;
     5  input Size: 231*251*251 with label, instead of previous SkyWatch Model of 29*140*140;
     6  treat all 1,2,3 labels as 1, in other words, do not differentiate primary, metastase, and nymph node;
     7  initializaiton LR is 0.1; considering zero padding and the bigger risk cost of missing cancer, adjust loss positive weight;
     4  add to 3 ResReXt Blocks in each layer in V model. 
    

Discarded changes:                  

Experiment setting:
Input CT data: maximum size 231*251*251 (zyx) of 3D numpy array with spacing size(3*2*2)

Loss Function:  BCELogitLoss

Data:   total 143 patients with weak annotaton label, 5-fold cross validation, test 29, validation 29, and training 85.  

Training strategy: 

          

Program starting Time: 2019-09-02 17:55:50.867314
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/ResNeXtVNet/20190902_175550

Info: this is the 0th fold leave for test in the 5-fold cross-validation.

Info: batchSize = 12

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/ResNeXtVNet/20190902_175550.
5-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.

training dataset: total 85 image files.

validation dataset: total 29 image files.

test dataset: total 29 image files.
Network has total 29,804,699 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-01		16586770.2656	0.02754		5930936.5833	0.00506		8107799.5417	0.01213
5	1.0000e-01		2221610.0845	0.06918		5382525.9167	0.02612		7652952.0000	0.02873
10	1.0000e-01		2105055.1953	0.06794		2566420.0000	0.05398		3321175.5208	0.06861
15	1.0000e-01		2044806.8594	0.07984		3803835.4583	0.03675		5859813.4167	0.04744
20	1.0000e-01		2008789.2422	0.07137		3185541.7917	0.05605		4086251.7500	0.06161
25	1.0000e-01		2060618.2285	0.07482		2951505.9375	0.05893		3905412.9792	0.07209
30	1.0000e-01		2319494.9521	0.07056		2815902.9375	0.05850		3825846.7917	0.06678
35	1.0000e-01		1969390.5283	0.08340		5033989.6667	0.03397		6608127.8333	0.04849
40	1.0000e-01		2141021.1626	0.07968		2798279.7917	0.08489		3564167.8125	0.10174
45	1.0000e-01		2004339.9590	0.09536		5126199.2917	0.04353		5516330.7917	0.05868
50	1.0000e-02		2307840.5410	0.05656		3048824.8333	0.07130		3756279.0833	0.07999
55	1.0000e-02		1791220.7891	0.09344		2341751.2083	0.07819		2897194.9375	0.10032
60	1.0000e-02		1897939.5117	0.09815		2237160.6250	0.08566		2830353.1667	0.10188
65	1.0000e-02		1635779.8516	0.10528		2204698.3958	0.08725		2611883.3125	0.11318
70	1.0000e-02		1703069.4980	0.10166		2060762.2500	0.09482		2489268.0417	0.12209
75	1.0000e-02		1712806.3340	0.11305		1972185.2083	0.09657		2422142.5208	0.12161
80	1.0000e-02		1756911.2129	0.11312		2038833.1458	0.10123		2489121.4167	0.12737
85	1.0000e-02		1713509.9102	0.11126		1890708.6458	0.10417		2404267.1042	0.12676
90	1.0000e-02		1675826.5117	0.10628		1886806.9375	0.10880		2457589.6667	0.12665
95	1.0000e-02		1559615.3770	0.10475		1910282.2917	0.09839		2429323.3125	0.12088
100	1.0000e-02		1631365.3281	0.10930		2084942.7500	0.09700		2517834.3333	0.11790
105	1.0000e-02		1528761.3711	0.11834		2116311.8750	0.10061		2549633.7292	0.12266
110	1.0000e-02		1606365.9590	0.11321		2323629.0000	0.09642		2645558.7500	0.12534
115	1.0000e-02		1634595.0918	0.11239		2063853.4167	0.09951		2432690.6458	0.12077
120	1.0000e-02		1429479.7549	0.11675		1893708.2708	0.11237		2431266.2083	0.13457
125	1.0000e-02		1469748.7793	0.13299		2197611.1458	0.11215		2567022.8958	0.13501
130	1.0000e-02		1403537.4087	0.12318		2125616.9167	0.10819		2512911.0417	0.13070
135	1.0000e-02		1498553.5605	0.11867		2079758.5417	0.12425		2571879.9583	0.14586
140	1.0000e-02		1375308.7314	0.12696		2213611.8125	0.11546		2565788.5208	0.13522
145	1.0000e-02		1313761.4873	0.13165		2335677.8750	0.10648		2586280.9375	0.12616
150	1.0000e-03		1427483.8025	0.13331		2228419.3125	0.10576		2443113.2292	0.12941
155	1.0000e-03		1312564.2715	0.13632		2276755.6667	0.11345		2527565.9583	0.13692
160	1.0000e-03		1276790.2063	0.13093		2388244.9167	0.10948		2569795.6667	0.13222
165	1.0000e-03		1343912.8516	0.12699		2299412.1458	0.11686		2578175.9375	0.13899
170	1.0000e-03		1331467.1416	0.12726		2219819.2917	0.11334		2452491.3125	0.13585
175	1.0000e-03		1376498.0845	0.12820		2234663.4167	0.11606		2505181.7708	0.13933
180	1.0000e-03		1309731.4832	0.13278		1978764.2917	0.11683		2370826.4583	0.13781
185	1.0000e-03		1443308.5039	0.12923		2142195.1250	0.11466		2368993.7083	0.13787
190	1.0000e-03		1335114.4395	0.12832		2311092.7083	0.11833		2552749.2708	0.13990
195	1.0000e-03		1362738.3301	0.13066		2354558.3333	0.11238		2507212.4583	0.13500
200	1.0000e-03		1254013.4629	0.12304		2177034.5833	0.11423		2443390.3125	0.13714
