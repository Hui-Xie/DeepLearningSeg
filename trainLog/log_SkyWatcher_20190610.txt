Program ID of SkyWatcher Network training:29443

Program command: 
 ['TrainSkyWatcher.py', '/home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/Images_ROI_29_140_140', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/Labels_ROI_23_127_127', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json']

Major program changes: 
                      merge train and test dataMgr into one.
                      

Experiment setting for Image3d ROI to response:
Input CT data: 29*140*140  3D CT raw image ROI with spacing size(5*2*2)
segmentation label: 23*127*127 with spacing size(5*2*2) segmentation label with value (0,1,2) which erases lymph node label

This is a multi-task learning. 

Predictive Model: 1,  first 3-layer dense conv block with channel size 128.
                  2,  and 3 dense conv DownBB blocks,  each of which includes a stride 2 conv and 3-layers dense conv block; 
                  3,  and 3 fully connected layers  changes the tensor into size 2*1;
                  4,  final a softmax for binary classification;
                  Total network learning parameters are 8 million.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/SkyWatcherModel.py

response Loss Function:   focus loss  with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.
segmentation loss function: focus loss  with weight [1.0416883685076772, 39.37007874015748, 68.39945280437757] for label (0, 1, 2)

Data:   training data has 113 patients, and valdiation data has 27 patients with training/test rate 80/20.
        We randomize all data, and then assign same distrubtion of treat reponse 0,1 into to training and test data set.
        

Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.  

                    Learning Scheduler:  Reduce learning rate on  plateau, and learning rate patience is 30 epochs.                                

            

Program starting Time: 2019-06-10 15:24:51.550722
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher

Now program get 140 input files.
Infor: In all data of 140 files, respone 0 has 41 files,
	  and response 1 has 99 files 
==== Regenerate training set and validation set by random with same distribution of 0 and 1 ==== 
Infor: Validation Set has 27 files,and Training Set has 113 files
TrainTestData Input:  batchSize=4, depth=29, height=140, width=140

Info: the size of bottle neck in the net = 128* (1, 7, 7)

Info: program uses Mixup with alpha=0.4, and mixupProb = 0.5.
Network trains from scratch.
Network has total 8,134,012 parameters.
Infor: Cross Entropy Weight: [3.3333333333333335, 1.4285714285714286] for label[0, 1]
Infor: Segmentation Cross Entropy Weight: [1.0416883685076772, 39.37007874015748, 68.39945280437757] for label (0, 1, 2)
Info: program will use 4 GPUs.
Loss Functions List: FocalCELoss with weight of 1; 	FocalCELoss with weight of 1; 
Hints: Test Dice_0 is the dice coeff for all non-zero labels
Hints: Test Dice_1 is for primary cancer(green), 	
 test Dice_2 is for metastasis(yellow), 	
 and test Dice_3 is for invaded lymph node(brown).
Hints: Test TPR_0 is the TPR for all non-zero labels
Hints: Test TPR_1 is for primary cancer(green), 	
 TPR_2 is for metastasis(yellow), 	
 and TPR_3 is for invaded lymph node(brown).

Dice is based on all 2D segmented slices in the volume from weak annotation, not real 3D dice.

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	TrLoss	Dice0	Dice1	Dice2	TPR_0	TPR_1	TPR_2	Accura	TPR_r	TsLoss	Dice0	Dice1	Dice2	TPR_0	TPR_1	TPR_2	Accura	TPR_r
0	0.8171	0.116	0.137	0.036	0.680	0.488	0.324	0.6719	0.9149	0.8292	0.112	0.124	0.058	0.673	0.424	0.365	0.6667	1.0000
1	0.5510	0.000	0.000	0.000	0.000	0.000	0.000	0.7206	1.0000	0.5789	0.129	0.178	0.082	0.938	0.620	0.717	0.7083	1.0000
2	0.5438	0.000	0.000	0.000	0.000	0.000	0.000	0.7727	1.0000	0.5205	0.179	0.210	0.130	0.799	0.467	0.628	0.7083	1.0000
3	0.5182	0.000	0.000	0.000	0.000	0.000	0.000	0.6667	1.0000	0.6566	0.160	0.224	0.045	0.929	0.919	0.230	0.6667	1.0000
4	0.5200	0.000	0.000	0.000	0.000	0.000	0.000	0.7500	1.0000	0.5779	0.128	0.197	0.044	0.966	0.726	0.440	0.7083	1.0000
5	0.5077	0.227	0.365	0.104	0.954	0.658	0.557	0.6250	1.0000	0.5310	0.189	0.298	0.061	0.902	0.864	0.353	0.7500	1.0000
6	0.4678	0.000	0.000	0.000	0.000	0.000	0.000	0.7308	1.0000	0.5702	0.227	0.293	0.162	0.890	0.879	0.478	0.7083	1.0000
7	0.4601	0.000	0.000	0.000	0.000	0.000	0.000	0.7500	1.0000	0.4312	0.193	0.327	0.144	0.810	0.622	0.790	0.7083	1.0000
8	0.4601	0.000	0.000	0.000	0.000	0.000	0.000	0.6964	1.0000	0.4269	0.184	0.236	0.142	0.866	0.869	0.682	0.7083	1.0000
9	0.4082	0.000	0.000	0.000	0.000	0.000	0.000	0.6607	1.0000	0.4269	0.229	0.345	0.158	0.861	0.927	0.660	0.6667	1.0000
10	0.4168	0.253	0.411	0.183	0.951	0.756	0.790	0.7333	1.0000	0.5141	0.201	0.204	0.165	0.828	0.840	0.575	0.7083	1.0000
11	0.4316	0.000	0.000	0.000	0.000	0.000	0.000	0.6875	1.0000	0.6034	0.177	0.305	0.077	0.807	0.909	0.224	0.6667	1.0000
12	0.4633	0.000	0.000	0.000	0.000	0.000	0.000	0.6071	1.0000	0.6834	0.180	0.169	0.126	0.891	0.903	0.512	0.7917	1.0000
13	0.4324	0.000	0.000	0.000	0.000	0.000	0.000	0.6923	1.0000	0.5334	0.218	0.268	0.131	0.843	0.927	0.391	0.7917	1.0000
14	0.4218	0.000	0.000	0.000	0.000	0.000	0.000	0.7237	1.0000	0.4586	0.207	0.197	0.126	0.886	0.749	0.545	0.7083	1.0000
15	0.3846	0.269	0.399	0.213	0.958	0.821	0.770	0.6974	1.0000	0.6259	0.221	0.255	0.162	0.863	0.936	0.383	0.7083	1.0000
16	0.4018	0.000	0.000	0.000	0.000	0.000	0.000	0.6333	1.0000	0.5291	0.201	0.225	0.164	0.933	0.882	0.725	0.7500	1.0000
17	0.3928	0.000	0.000	0.000	0.000	0.000	0.000	0.7750	1.0000	0.4690	0.200	0.259	0.133	0.974	0.827	0.756	0.7500	1.0000
18	0.3558	0.000	0.000	0.000	0.000	0.000	0.000	0.6667	1.0000	0.5108	0.225	0.262	0.183	0.840	0.910	0.430	0.6667	1.0000
19	0.3278	0.000	0.000	0.000	0.000	0.000	0.000	0.7143	1.0000	0.4966	0.200	0.270	0.131	0.958	0.872	0.639	0.7500	1.0000
20	0.3614	0.279	0.467	0.195	0.933	0.832	0.831	0.7750	1.0000	0.5616	0.208	0.207	0.160	0.962	0.915	0.664	0.7083	1.0000
21	0.3661	0.000	0.000	0.000	0.000	0.000	0.000	0.7083	1.0000	0.4808	0.215	0.276	0.168	0.914	0.884	0.736	0.6667	1.0000
22	0.3331	0.000	0.000	0.000	0.000	0.000	0.000	0.6833	1.0000	0.4563	0.285	0.336	0.200	0.854	0.820	0.592	0.7083	1.0000
23	0.3366	0.000	0.000	0.000	0.000	0.000	0.000	0.7500	1.0000	0.4661	0.232	0.269	0.172	0.827	0.977	0.430	0.7083	1.0000
24	0.3755	0.000	0.000	0.000	0.000	0.000	0.000	0.6786	1.0000	0.5369	0.186	0.243	0.147	0.927	0.913	0.581	0.6667	1.0000
25	0.3507	0.293	0.376	0.199	0.964	0.855	0.924	0.7250	1.0000	0.4070	0.254	0.334	0.202	0.876	0.887	0.712	0.7083	1.0000
26	0.3277	0.000	0.000	0.000	0.000	0.000	0.000	0.7500	1.0000	0.4153	0.204	0.265	0.184	0.816	0.983	0.570	0.6667	1.0000
27	0.2857	0.000	0.000	0.000	0.000	0.000	0.000	0.7083	1.0000	0.5149	0.278	0.300	0.234	0.873	0.808	0.632	0.6667	1.0000
28	0.2731	0.000	0.000	0.000	0.000	0.000	0.000	0.7679	1.0000	0.5676	0.311	0.344	0.196	0.727	0.729	0.500	0.6667	1.0000
29	0.3122	0.000	0.000	0.000	0.000	0.000	0.000	0.7115	1.0000	0.6035	0.331	0.326	0.230	0.778	0.774	0.478	0.7083	1.0000
30	0.2993	0.323	0.454	0.253	0.958	0.917	0.943	0.6500	1.0000	0.5073	0.213	0.301	0.110	0.867	0.848	0.439	0.7500	1.0000
31	0.2940	0.000	0.000	0.000	0.000	0.000	0.000	0.7500	1.0000	0.6157	0.287	0.309	0.180	0.821	0.856	0.500	0.6667	1.0000
32	0.2636	0.000	0.000	0.000	0.000	0.000	0.000	0.6806	1.0000	0.4975	0.310	0.366	0.221	0.832	0.830	0.566	0.7083	1.0000
