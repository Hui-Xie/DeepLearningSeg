=============training from sratch============
Program ID: 21606

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt', '/home/hxie1/data/OvarianCancerCT/pixelSize223/numpy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '0', '0']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
            8    at July 30th 03:00, 2019:
                    add learning rate print;
                    use convStride =2;
                    add filter number by 2 times along deeper layers.
            9    at July 30th, 10:13, 2019:
                    add MaxPool2d in stage1;
                    add final filters to 2048.
            10   at July 30th, 15:23, 2019
                    final conv layer filter number: 1024
            11   at Aug 10th, 2019:
                    A. Add new patient data; and exclude non-standard patient data;
                    B. test the k-th fold,  validation on the (k+1)th fold;
                    C. new inputsize: 231*251*251 with pixels size 3*2*2 mm
                    D. data normalize into [0,1] after window level shresthold [0,300]
                    E. put data padding in to converting from nrrd to numpy;
                    
                                                        
                    
            
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   total 220 patients, 5-fold cross validation, test 45, validation 45, and training 130.  

Training strategy: 

          

Program starting Time: 2019-08-10 14:40:24.524621
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt/20190810_144024

Info: this is the 0th fold leave for test in the 5-fold cross-validation.

Infor: In all data of 220 files, label 0 has 77 files,
	  and label 1 has 143 files, where positive response rate = 0.65 in full data
5-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.
test dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
training dataset:	 total 130 files, where 1 has 85.0 with rate of 0.6538461538461539
validation dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
=== Network trains from scratch ====
Network has total 90,389,606 parameters.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	LearningRate		TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	1.0000e-04		0.4850	0.5462	0.5882	0.4667		0.4800	0.3556	0.0000	1.0000		0.4800	0.3556	0.0000	1.0000
5	1.0000e-04		0.5051	0.4923	0.5294	0.4222		0.5092	0.3556	0.0000	1.0000		0.5221	0.3556	0.0000	1.0000
10	9.5000e-05		0.5190	0.6462	0.8941	0.1778		0.4711	0.6000	0.6552	0.5000		0.4894	0.5111	0.5862	0.3750
15	9.5000e-05		0.4796	0.6077	0.6941	0.4444		0.4937	0.5333	0.5172	0.5625		0.4952	0.4889	0.3103	0.8125
20	9.0250e-05		0.5473	0.5308	0.6824	0.2444		0.4983	0.4889	0.5172	0.4375		0.4839	0.4889	0.4483	0.5625
25	9.0250e-05		0.4909	0.6154	0.7882	0.2889		0.4874	0.4889	0.5862	0.3125		0.5213	0.5333	0.6207	0.3750
30	8.5737e-05		0.5060	0.4462	0.3647	0.6000		0.4695	0.6000	0.6552	0.5000		0.4948	0.5778	0.6552	0.4375
35	8.5737e-05		0.5159	0.5308	0.5529	0.4889		0.5588	0.6222	0.8621	0.1875		0.4993	0.7111	0.9655	0.2500
40	8.1451e-05		0.5362	0.6462	0.8824	0.2000		0.4720	0.5111	0.3103	0.8750		0.4910	0.4889	0.2759	0.8750
45	8.1451e-05		0.4967	0.4000	0.1647	0.8444		0.4464	0.5778	0.6207	0.5000		0.5369	0.5111	0.6552	0.2500
50	7.7378e-05		0.5208	0.5308	0.6235	0.3556		0.4551	0.5778	0.6897	0.3750		0.5329	0.5333	0.6897	0.2500
55	7.7378e-05		0.4929	0.5154	0.5294	0.4889		0.4798	0.6222	0.6552	0.5625		0.5141	0.5556	0.6897	0.3125
60	7.3509e-05		0.4879	0.6000	0.7294	0.3556		0.4769	0.6444	0.7586	0.4375		0.5501	0.6000	0.7931	0.2500
65	7.3509e-05		0.5132	0.5538	0.6353	0.4000		0.4967	0.4889	0.4483	0.5625		0.5181	0.5111	0.5517	0.4375
70	6.9834e-05		0.5175	0.4385	0.3176	0.6667		0.5045	0.4889	0.4138	0.6250		0.4942	0.5111	0.4138	0.6875
75	6.9834e-05		0.4753	0.5154	0.3765	0.7778		0.4747	0.5556	0.5517	0.5625		0.5055	0.5333	0.6552	0.3125
80	6.6342e-05		0.4966	0.4692	0.3176	0.7556		0.5024	0.5333	0.5862	0.4375		0.5032	0.5333	0.5862	0.4375
85	6.6342e-05		0.4995	0.5538	0.4941	0.6667		0.5282	0.6222	0.8276	0.2500		0.5495	0.6444	0.8966	0.1875
90	6.3025e-05		0.4747	0.6462	0.7647	0.4222		0.4707	0.5111	0.4828	0.5625		0.4696	0.5333	0.4828	0.6250
95	6.3025e-05		0.4698	0.6538	0.7882	0.4000		0.4668	0.6222	0.6552	0.5625		0.5217	0.5556	0.7241	0.2500
100	5.9874e-05		0.4886	0.4923	0.4353	0.6000		0.5238	0.5333	0.6552	0.3125		0.5469	0.6222	0.8276	0.2500
105	5.9874e-05		0.5043	0.4231	0.2824	0.6889		0.4913	0.5778	0.6552	0.4375		0.4833	0.6222	0.8621	0.1875
110	5.6880e-05		0.4997	0.5769	0.6824	0.3778		0.5008	0.6000	0.7241	0.3750		0.5231	0.5778	0.7931	0.1875
115	5.6880e-05		0.4839	0.5769	0.6588	0.4222		0.4782	0.6444	0.7241	0.5000		0.4904	0.6222	0.7931	0.3125
120	5.4036e-05		0.4722	0.5308	0.5294	0.5333		0.4669	0.5556	0.6552	0.3750		0.4774	0.6000	0.6552	0.5000
125	5.4036e-05		0.4917	0.5000	0.4471	0.6000		0.4899	0.6222	0.6207	0.6250		0.4942	0.6000	0.7241	0.3750
130	5.1334e-05		0.4900	0.5615	0.6941	0.3111		0.4778	0.5778	0.5862	0.5625		0.4934	0.5778	0.6207	0.5000
135	5.1334e-05		0.4817	0.5615	0.5882	0.5111		0.4994	0.6222	0.7241	0.4375		0.5553	0.5111	0.7241	0.1250
140	4.8767e-05		0.4947	0.4462	0.2471	0.8222		0.5099	0.4889	0.4138	0.6250		0.5162	0.4889	0.5172	0.4375
145	4.8767e-05		0.4727	0.5692	0.5529	0.6000		0.5260	0.6667	0.8276	0.3750		0.5372	0.6000	0.7931	0.2500
150	4.6329e-05		0.4789	0.5692	0.6118	0.4889		0.5671	0.5333	0.5862	0.4375		0.5658	0.5111	0.7241	0.1250
155	4.6329e-05		0.4708	0.5769	0.5882	0.5556		0.4991	0.5111	0.4483	0.6250		0.5057	0.4889	0.4483	0.5625
160	4.4013e-05		0.4889	0.5692	0.6588	0.4000		0.4999	0.6444	0.7586	0.4375		0.5831	0.5111	0.7241	0.1250
165	4.4013e-05		0.4850	0.5462	0.5647	0.5111		0.4646	0.5111	0.4828	0.5625		0.5460	0.4444	0.3793	0.5625
170	4.1812e-05		0.4781	0.5846	0.6235	0.5111		0.5034	0.5333	0.5172	0.5625		0.5104	0.4000	0.4138	0.3750
175	4.1812e-05		0.4900	0.5769	0.6706	0.4000		0.4739	0.5778	0.5172	0.6875		0.5145	0.4889	0.5862	0.3125
180	3.9721e-05		0.4787	0.5615	0.5176	0.6444		0.4840	0.5111	0.3793	0.7500		0.4836	0.4667	0.4828	0.4375
185	3.9721e-05		0.4760	0.5692	0.6706	0.3778		0.5058	0.6444	0.7586	0.4375		0.5487	0.5778	0.7586	0.2500
190	3.7735e-05		0.4663	0.5154	0.3882	0.7556		0.5018	0.4889	0.3448	0.7500		0.4876	0.4889	0.3793	0.6875
195	3.7735e-05		0.4975	0.5923	0.7294	0.3333		0.4951	0.5556	0.5517	0.5625		0.4943	0.5111	0.5862	0.3750
200	3.5849e-05		0.4727	0.5385	0.4706	0.6667		0.4981	0.5556	0.4483	0.7500		0.5129	0.5111	0.5862	0.3750
205	3.5849e-05		0.4803	0.5000	0.5294	0.4444		0.5047	0.5556	0.4828	0.6875		0.4944	0.5778	0.6207	0.5000
210	3.4056e-05		0.4702	0.6077	0.6235	0.5778		0.4806	0.6222	0.6207	0.6250		0.5121	0.5333	0.6207	0.3750
215	3.4056e-05		0.4711	0.5692	0.5882	0.5333		0.4800	0.5778	0.6207	0.5000		0.5562	0.4667	0.6207	0.1875
220	3.2353e-05		0.4831	0.5846	0.6706	0.4222		0.4959	0.6444	0.6552	0.6250		0.5316	0.5778	0.7241	0.3125
225	3.2353e-05		0.4789	0.5462	0.5059	0.6222		0.4958	0.5778	0.5517	0.6250		0.5324	0.4889	0.5172	0.4375
230	3.0736e-05		0.4765	0.5308	0.4235	0.7333		0.4782	0.6667	0.6897	0.6250		0.5361	0.5111	0.6897	0.1875
235	3.0736e-05		0.4763	0.5077	0.4235	0.6667		0.5115	0.5111	0.3793	0.7500		0.5145	0.4889	0.4138	0.6250
240	2.9199e-05		0.4766	0.5692	0.7176	0.2889		0.4717	0.5778	0.6207	0.5000		0.5047	0.6222	0.6897	0.5000
245	2.9199e-05		0.4728	0.5615	0.5294	0.6222		0.4723	0.5556	0.5517	0.5625		0.5120	0.5556	0.7241	0.2500
250	2.7739e-05		0.4687	0.5538	0.6471	0.3778		0.4846	0.5333	0.5172	0.5625		0.5004	0.5111	0.5517	0.4375
255	2.7739e-05		0.4689	0.5385	0.5176	0.5778		0.4775	0.6000	0.5517	0.6875		0.5500	0.4222	0.4138	0.4375
260	2.6352e-05		0.4754	0.5692	0.5882	0.5333		0.5168	0.5778	0.5517	0.6250		0.4978	0.4667	0.4828	0.4375
265	2.6352e-05		0.4695	0.6231	0.6471	0.5778		0.4817	0.6000	0.5517	0.6875		0.5257	0.5333	0.5517	0.5000
270	2.5034e-05		0.4784	0.5846	0.6000	0.5556		0.4589	0.6444	0.6552	0.6250		0.5011	0.5556	0.5862	0.5000
275	2.5034e-05		0.4670	0.5923	0.5647	0.6444		0.4660	0.6000	0.5862	0.6250		0.5021	0.5333	0.5172	0.5625
280	2.3783e-05		0.4919	0.5231	0.5294	0.5111		0.4955	0.5778	0.5862	0.5625		0.5177	0.5333	0.5862	0.4375
285	2.3783e-05		0.4667	0.6308	0.7176	0.4667		0.4989	0.5778	0.6207	0.5000		0.5392	0.4667	0.5517	0.3125
290	2.2594e-05		0.4652	0.6077	0.6824	0.4667		0.4728	0.6667	0.6897	0.6250		0.5395	0.4889	0.5517	0.3750
295	2.2594e-05		0.4614	0.6154	0.5765	0.6889		0.4835	0.6000	0.5862	0.6250		0.5162	0.5333	0.6207	0.3750
300	2.1464e-05		0.4611	0.6000	0.6118	0.5778		0.5172	0.5556	0.5862	0.5000		0.5495	0.5556	0.6552	0.3750
305	2.1464e-05		0.4695	0.5385	0.5059	0.6000		0.5141	0.5556	0.5172	0.6250		0.5240	0.4667	0.4828	0.4375
310	2.0391e-05		0.4581	0.5385	0.4118	0.7778		0.4982	0.6222	0.5517	0.7500		0.5185	0.4889	0.4483	0.5625
315	2.0391e-05		0.4740	0.5923	0.6471	0.4889		0.5050	0.6667	0.7586	0.5000		0.5286	0.5333	0.7586	0.1250
320	1.9371e-05		0.4812	0.5846	0.6824	0.4000		0.4695	0.6667	0.6897	0.6250		0.4969	0.5111	0.5862	0.3750
325	1.9371e-05		0.4716	0.5308	0.4471	0.6889		0.4749	0.6889	0.6552	0.7500		0.5170	0.5111	0.5862	0.3750
330	1.8403e-05		0.4542	0.6000	0.5412	0.7111		0.4503	0.6444	0.6207	0.6875		0.5285	0.4889	0.4828	0.5000
335	1.8403e-05		0.4717	0.6000	0.6235	0.5556		0.4620	0.6000	0.6552	0.5000		0.5174	0.4889	0.4828	0.5000
340	1.7482e-05		0.4655	0.5692	0.5529	0.6000		0.4547	0.5778	0.5172	0.6875		0.5126	0.5333	0.5172	0.5625
345	1.7482e-05		0.4724	0.5769	0.5529	0.6222		0.4469	0.5556	0.4483	0.7500		0.5116	0.4444	0.3793	0.5625
350	1.6608e-05		0.4511	0.5615	0.5412	0.6000		0.4546	0.6222	0.5862	0.6875		0.4943	0.5778	0.5862	0.5625
355	1.6608e-05		0.4836	0.5077	0.4588	0.6000		0.4595	0.6000	0.5517	0.6875		0.5022	0.5111	0.5517	0.4375
360	1.5778e-05		0.4666	0.5462	0.5176	0.6000		0.4660	0.6667	0.6552	0.6875		0.5126	0.5333	0.5862	0.4375
365	1.5778e-05		0.4617	0.5923	0.6000	0.5778		0.4807	0.6889	0.7241	0.6250		0.5046	0.5333	0.6207	0.3750
370	1.4989e-05		0.4717	0.5308	0.5412	0.5111		0.4667	0.6667	0.6897	0.6250		0.5243	0.6222	0.7586	0.3750
375	1.4989e-05		0.4858	0.5077	0.4824	0.5556		0.4854	0.6000	0.5862	0.6250		0.5224	0.5333	0.5862	0.4375
380	1.4240e-05		0.4591	0.6231	0.6235	0.6222		0.4741	0.6444	0.6207	0.6875		0.5137	0.5333	0.5862	0.4375
385	1.4240e-05		0.4434	0.6692	0.6588	0.6889		0.4759	0.6889	0.7241	0.6250		0.5321	0.5556	0.6897	0.3125
390	1.3528e-05		0.4598	0.5846	0.5294	0.6889		0.4729	0.7333	0.7931	0.6250		0.5247	0.5556	0.6552	0.3750
395	1.3528e-05		0.4511	0.6462	0.5882	0.7556		0.4669	0.6444	0.6552	0.6250		0.5118	0.5333	0.5862	0.4375
400	1.2851e-05		0.4709	0.5538	0.5765	0.5111		0.4767	0.6444	0.6552	0.6250		0.5047	0.5556	0.6207	0.4375
405	1.2851e-05		0.4655	0.6231	0.6706	0.5333		0.4648	0.6889	0.6897	0.6875		0.5022	0.5556	0.6552	0.3750
410	1.2209e-05		0.4771	0.5154	0.4706	0.6000		0.4961	0.6667	0.6897	0.6250		0.5309	0.4889	0.5517	0.3750
415	1.2209e-05		0.4538	0.6077	0.6118	0.6000		0.4659	0.7111	0.6897	0.7500		0.5181	0.5333	0.5862	0.4375
420	1.1598e-05		0.4622	0.5846	0.6471	0.4667		0.4782	0.6444	0.6552	0.6250		0.5454	0.5556	0.6207	0.4375
425	1.1598e-05		0.4597	0.6231	0.6824	0.5111		0.4572	0.7333	0.7586	0.6875		0.5679	0.5333	0.6552	0.3125
430	1.1018e-05		0.4691	0.6077	0.6588	0.5111		0.4771	0.7111	0.7241	0.6875		0.5285	0.5778	0.6552	0.4375
435	1.1018e-05		0.4621	0.6077	0.5882	0.6444		0.4590	0.7333	0.7241	0.7500		0.5436	0.5556	0.6207	0.4375
440	1.0467e-05		0.4548	0.5769	0.5412	0.6444		0.4671	0.6889	0.6552	0.7500		0.5195	0.5556	0.6207	0.4375
445	1.0467e-05		0.4369	0.6769	0.6824	0.6667		0.4764	0.7333	0.7241	0.7500		0.5251	0.6000	0.6552	0.5000
450	9.9440e-06		0.4533	0.6538	0.5882	0.7778		0.4851	0.6667	0.6897	0.6250		0.5418	0.4889	0.5517	0.3750
455	9.9440e-06		0.4665	0.5846	0.5412	0.6667		0.4634	0.6889	0.7241	0.6250		0.5455	0.5333	0.6207	0.3750
460	9.4468e-06		0.4502	0.6462	0.6706	0.6000		0.4830	0.6444	0.6897	0.5625		0.5477	0.5556	0.5862	0.5000
465	9.4468e-06		0.4587	0.5846	0.5412	0.6667		0.4683	0.7111	0.7586	0.6250		0.5303	0.5333	0.5862	0.4375
470	8.9745e-06		0.4575	0.5615	0.6118	0.4667		0.4554	0.7333	0.7241	0.7500		0.5434	0.5111	0.5517	0.4375
475	8.9745e-06		0.4677	0.6538	0.6706	0.6222		0.4937	0.6889	0.7241	0.6250		0.5358	0.5333	0.5862	0.4375
480	8.5258e-06		0.4593	0.5692	0.5059	0.6889		0.4761	0.6889	0.6552	0.7500		0.5442	0.5556	0.6207	0.4375
485	8.5258e-06		0.4644	0.6000	0.6000	0.6000		0.4887	0.6222	0.6552	0.5625		0.5467	0.5556	0.6207	0.4375
490	8.0995e-06		0.4541	0.6154	0.6118	0.6222		0.4776	0.6000	0.5517	0.6875		0.5453	0.5333	0.5862	0.4375
