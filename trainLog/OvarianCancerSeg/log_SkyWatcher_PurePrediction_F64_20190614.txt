Program ID of SkyWatcher Network training:16140

Program command: 
 ['TrainSkyWatcherPurePrediction.py', '/home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher/PurePrediction', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/Images_ROI_29_140_140', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json']

Major program changes: 
                      merge train and test dataMgr into one.
                      when epoch %5 ==0, do not use mixup.
                      Directly use 3D data for treatment prediction without segmentation. 
                      Number of filters in encoder is 64.
                      Only epoch %5 ==0, print log
                      Use BatchNorm1d in FC layer, instead of InstanceNorm1d.
                       

Experiment setting for Image3d ROI to response:
Input CT data: 29*140*140  3D CT raw image ROI with spacing size(5*2*2)

Predictive Model: 1,  first 3-layer dense conv block with channel size 128.
                  2,  and 3 dense conv DownBB blocks,  each of which includes a stride 2 conv and 3-layers dense conv block; 
                  3,  and 3 fully connected layers  changes the tensor into size 2*1;
                  4,  final a softmax for binary classification;
                  Total network learning parameters are 8 million.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/SkyWatcherModel.py

response Loss Function:   focus loss  with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.

Data:   training data has 113 patients, and valdiation data has 27 patients with training/test rate 80/20.
        We randomize all data, and then assign same distrubtion of treat reponse 0,1 into to training and test data set.


Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.  

                    Learning Scheduler:  Reduce learning rate on  plateau, and learning rate patience is 30 epochs.                                

            

Program starting Time: 2019-06-14 16:05:05.799612
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher/PurePrediction

Now program get 140 input files.
Infor: In all data of 140 files, respone 0 has 41 files,
	  and response 1 has 99 files, where positive response rate = 0.7071428571428572 in full data
==== Regenerate training set and validation set by random with same distribution of 0 and 1 ==== 
Infor: Validation Set has 27 files,and Training Set has 113 files
Infor: In Validataion set, 19 1's, and positive response rate = 0.7037037037037037
Infor: In trainning set, 80 1's,  positive response rate = 0.7079646017699115
Infor: program is in multi samples running model.
TrainTestData Input:  batchSize=4, depth=29, height=140, width=140

Info: the size of bottle neck in the net = (1, 7, 7)

Info: program uses Mixup with alpha=0.4, and mixupProb = 0.5.
Network trains from scratch.
Network has total 2,040,900 parameters.
Infor: Response Cross Entropy Weight: [3.3333333333333335, 1.4285714285714286] for label[0, 1]
Loss Functions List: FocalCELoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	TrLoss	Accura	TPR_r	TNR_r		TsLoss	Accura	TPR_r	TNR_r
0	0.4378	0.4464	0.3924	0.5758		0.5421	0.6250	0.7500	0.3750
5	0.2189	0.4911	0.4810	0.5152		0.1798	0.6250	0.6111	0.6667
10	0.2054	0.5804	0.6203	0.4848		0.2063	0.5000	0.6875	0.1250
15	0.2152	0.5446	0.6329	0.3333		0.1804	0.6250	0.6875	0.5000
20	0.2034	0.5804	0.6835	0.3333		0.2380	0.2917	0.4118	0.0000
25	0.1801	0.6429	0.7342	0.4242		0.2058	0.5833	0.8235	0.0000
30	0.1846	0.6071	0.6709	0.4545		0.2456	0.4167	0.5556	0.0000
35	0.1923	0.6161	0.6962	0.4242		0.2057	0.5833	0.7647	0.1429
40	0.1799	0.6339	0.7089	0.4545		0.1978	0.4167	0.6250	0.0000
45	0.1722	0.6429	0.7342	0.4242		0.1869	0.6667	0.8824	0.1429
50	0.1733	0.6429	0.7595	0.3636		0.1792	0.5417	0.6875	0.2500
55	0.1746	0.6339	0.7342	0.3939		0.1868	0.7083	0.9444	0.0000
60	0.1855	0.5625	0.6582	0.3333		0.2076	0.4583	0.6875	0.0000
65	0.1792	0.5893	0.7215	0.2727		0.1828	0.6250	0.8824	0.0000
70	0.1737	0.5804	0.7089	0.2727		0.1846	0.6667	0.9412	0.0000
75	0.1765	0.5804	0.6875	0.3125		0.1931	0.6250	0.9375	0.0000
80	0.1663	0.5982	0.7215	0.3030		0.1802	0.6250	0.9375	0.0000
85	0.1739	0.5536	0.6835	0.2424		0.2442	0.3333	0.4118	0.1429
90	0.1736	0.5982	0.7125	0.3125		0.2041	0.4583	0.6471	0.0000
95	0.1626	0.7232	0.8861	0.3333		0.1711	0.6667	0.8824	0.1429
100	0.1693	0.6607	0.8500	0.1875		0.1886	0.5417	0.8125	0.0000
105	0.1674	0.6250	0.7750	0.2500		0.1897	0.5000	0.7500	0.0000
110	0.1642	0.7232	0.8481	0.4242		0.1767	0.6250	0.8824	0.0000
115	0.1635	0.6875	0.8354	0.3333		0.1805	0.6250	0.8824	0.0000
120	0.1679	0.6250	0.7468	0.3333		0.2094	0.4583	0.6471	0.0000
125	0.1664	0.6607	0.7848	0.3636		0.1717	0.6250	0.8824	0.0000
130	0.1645	0.6696	0.8250	0.2812		0.1671	0.6250	0.8824	0.0000
135	0.1670	0.6429	0.7975	0.2727		0.1730	0.6250	0.8824	0.0000
140	0.1656	0.6339	0.7975	0.2424		0.1774	0.6667	0.9412	0.0000
145	0.1595	0.7054	0.8000	0.4688		0.1859	0.5833	0.8750	0.0000
150	0.1558	0.7054	0.7750	0.5312		0.1681	0.6667	0.9412	0.0000
155	0.1641	0.7143	0.8228	0.4545		0.1882	0.6667	1.0000	0.0000
160	0.1622	0.7054	0.8354	0.3939		0.1705	0.6667	0.9412	0.0000
165	0.1659	0.6339	0.7342	0.3939		0.1630	0.6667	0.8889	0.0000
170	0.1641	0.6607	0.7875	0.3438		0.1753	0.6667	1.0000	0.0000
175	0.1607	0.6518	0.7625	0.3750		0.1839	0.6667	1.0000	0.0000
180	0.1636	0.6964	0.8608	0.3030		0.1745	0.6667	0.9412	0.0000
185	0.1714	0.7321	0.8608	0.4242		0.1697	0.6250	0.8333	0.0000
190	0.1611	0.7054	0.8734	0.3030		0.1687	0.7083	1.0000	0.0000
195	0.1669	0.6518	0.7722	0.3636		0.1676	0.6667	0.8889	0.0000
200	0.1678	0.5982	0.7468	0.2424		0.1696	0.6250	0.9375	0.0000
205	0.1635	0.7321	0.8987	0.3333		0.1773	0.7083	1.0000	0.0000
210	0.1664	0.6964	0.8734	0.2727		0.2526	0.5000	0.6667	0.0000
215	0.1785	0.6071	0.7595	0.2424		0.1615	0.7917	1.0000	0.2857
220	0.1728	0.7143	0.9500	0.1250		0.1703	0.7500	1.0000	0.1429
225	0.1712	0.6696	0.9241	0.0606		0.1691	0.7083	1.0000	0.0000
230	0.1696	0.6518	0.8861	0.0909		0.1834	0.5833	0.8235	0.0000
235	0.1763	0.6607	0.8987	0.0909		0.1651	0.7500	1.0000	0.0000
240	0.1623	0.7589	0.8875	0.4375		0.1754	0.7083	1.0000	0.0000
245	0.1675	0.6518	0.7500	0.4062		0.1809	0.6250	0.8824	0.0000
250	0.1684	0.6429	0.8000	0.2500		0.1794	0.5833	0.8125	0.1250
255	0.1668	0.7143	0.8734	0.3333		0.1857	0.5833	0.8750	0.0000
260	0.1636	0.7054	0.8354	0.3939		0.1852	0.5417	0.8125	0.0000
265	0.1696	0.6696	0.8608	0.2121		0.1760	0.5833	0.7778	0.0000
270	0.1631	0.7232	0.8875	0.3125		0.1776	0.6250	0.9375	0.0000
275	0.1692	0.6964	0.8987	0.2121		0.1738	0.6667	1.0000	0.0000
280	0.1712	0.6607	0.8734	0.1515		0.1694	0.7083	0.9412	0.1429
285	0.1715	0.6429	0.8608	0.1212		0.1604	0.7083	0.9444	0.0000
290	0.1733	0.6607	0.8354	0.2424		0.1728	0.7083	0.9444	0.0000
295	0.1705	0.6696	0.7975	0.3636		0.1864	0.6250	0.8824	0.0000
300	0.1641	0.7054	0.8750	0.2812		0.1834	0.5833	0.7500	0.2500
305	0.1678	0.6696	0.8354	0.2727		0.2016	0.6250	0.7059	0.4286
310	0.1629	0.7321	0.8500	0.4375		0.2017	0.5833	0.6875	0.3750
315	0.1691	0.6964	0.8608	0.3030		0.1718	0.6667	0.8824	0.1429
320	0.1634	0.7054	0.8101	0.4545		0.2139	0.6667	0.8824	0.1429
325	0.1623	0.6875	0.7975	0.4242		0.1748	0.7083	0.8889	0.1667
330	0.1661	0.6786	0.7975	0.3939		0.1831	0.6667	0.8889	0.0000
335	0.1617	0.7143	0.8101	0.4848		0.1843	0.6250	0.8235	0.1429
340	0.1686	0.6339	0.7875	0.2500		0.2249	0.6250	0.8750	0.1250
345	0.1651	0.6518	0.7975	0.3030		0.1881	0.5000	0.5882	0.2857
350	0.1705	0.6518	0.8101	0.2727		0.1668	0.7083	0.9444	0.0000
355	0.1714	0.6339	0.7750	0.2812		0.1814	0.5833	0.6875	0.3750
360	0.1626	0.7411	0.8861	0.3939		0.2186	0.5417	0.7059	0.1429
365	0.1624	0.6964	0.8354	0.3636		0.1950	0.6250	0.8750	0.1250
370	0.1649	0.7143	0.8481	0.3939		0.1861	0.5417	0.6875	0.2500
375	0.1680	0.6875	0.8608	0.2727		0.2195	0.6250	0.8125	0.2500
380	0.1641	0.7054	0.8228	0.4242		0.1946	0.6250	0.7500	0.3750
385	0.1554	0.7321	0.8481	0.4545		0.2098	0.6250	0.7500	0.3750
390	0.1628	0.7054	0.8608	0.3333		0.1896	0.6250	0.7500	0.3750
395	0.1685	0.6875	0.8228	0.3636		0.1848	0.5833	0.6875	0.3750
400	0.1663	0.7054	0.9241	0.1818		0.1609	0.7083	0.8889	0.1667
405	0.1542	0.7411	0.8228	0.5455		0.1976	0.5833	0.6875	0.3750
410	0.1721	0.6875	0.9367	0.0909		0.1708	0.5833	0.8750	0.0000
415	0.1630	0.7500	0.9000	0.3750		0.1703	0.6250	0.9375	0.0000
420	0.1588	0.7500	0.8875	0.4062		0.1764	0.6250	0.8750	0.1250
425	0.1784	0.6250	0.8354	0.1212		0.1743	0.6667	0.9412	0.0000
430	0.1733	0.6607	0.8861	0.1212		0.1800	0.6667	0.9412	0.0000
435	0.1695	0.6875	0.9250	0.0938		0.1629	0.7500	1.0000	0.0000
440	0.1692	0.6607	0.8608	0.1818		0.1775	0.7083	1.0000	0.0000
445	0.1651	0.6786	0.8228	0.3333		0.1735	0.6667	0.9375	0.1250
450	0.1642	0.7232	0.8625	0.3750		0.1691	0.7083	1.0000	0.1250
455	0.1657	0.6607	0.7975	0.3333		0.1852	0.4583	0.6250	0.1250
460	0.1612	0.7054	0.7975	0.4848		0.1770	0.6250	0.8750	0.1250
465	0.1577	0.7321	0.8354	0.4848		0.1927	0.6250	0.8125	0.2500
470	0.1604	0.7500	0.8354	0.5455		0.1751	0.6250	0.6842	0.4000
475	0.1619	0.6875	0.7975	0.4242		0.1854	0.7083	0.7647	0.5714
480	0.1557	0.7143	0.8375	0.4062		0.1626	0.7500	0.8235	0.5714
485	0.1617	0.7143	0.8481	0.3939		0.1578	0.7500	0.8889	0.3333
490	0.1600	0.6696	0.7625	0.4375		0.1623	0.6667	0.7647	0.4286
495	0.1465	0.7321	0.7848	0.6061		0.1963	0.5833	0.8235	0.0000
500	0.1551	0.7143	0.8000	0.5000		0.2061	0.6667	1.0000	0.0000
505	0.1401	0.7411	0.8101	0.5758		0.2031	0.5417	0.7500	0.1250
510	0.1567	0.6875	0.7625	0.5000		0.1938	0.5417	0.6667	0.1667
515	0.1577	0.7232	0.7848	0.5758		0.2010	0.5833	0.8750	0.0000
520	0.1485	0.7411	0.7975	0.6061		0.2038	0.5000	0.7059	0.0000
525	0.1393	0.7500	0.7848	0.6667		0.2128	0.6667	0.8889	0.0000
530	0.1417	0.7500	0.8354	0.5455		0.2132	0.5833	0.8750	0.0000
535	0.1383	0.7768	0.8608	0.5758		0.2386	0.5000	0.7059	0.0000
540	0.1058	0.8839	0.9125	0.8125		0.2107	0.5833	0.8750	0.0000
545	0.1280	0.7679	0.8354	0.6061		0.2621	0.5000	0.7059	0.0000
550	0.1582	0.7589	0.8354	0.5758		0.2478	0.6250	0.8750	0.1250
555	0.1385	0.7500	0.7975	0.6364		0.2529	0.5833	0.7222	0.1667
560	0.1348	0.7321	0.7595	0.6667		0.2919	0.4167	0.5000	0.2500
565	0.1177	0.8304	0.8608	0.7576		0.2707	0.4167	0.5294	0.1429
570	0.0948	0.8571	0.8625	0.8438		0.2564	0.6250	0.9375	0.0000
575	0.1261	0.8393	0.8608	0.7879		0.3061	0.5000	0.6875	0.1250
580	0.1044	0.8214	0.8481	0.7576		0.3088	0.4583	0.6250	0.1250
585	0.1158	0.8214	0.8625	0.7188		0.2700	0.5833	0.6111	0.5000
590	0.1093	0.8571	0.8875	0.7812		0.2987	0.5000	0.6875	0.1250
595	0.1112	0.7768	0.8481	0.6061		0.2717	0.6667	0.7895	0.2000
600	0.1048	0.8482	0.9114	0.6970		0.3053	0.6250	0.7647	0.2857
605	0.0860	0.9196	0.9500	0.8438		0.4017	0.5833	0.7059	0.2857

