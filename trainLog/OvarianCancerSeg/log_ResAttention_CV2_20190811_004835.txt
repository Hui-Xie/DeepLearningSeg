=============training from sratch============
Program ID: 28773

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt', '/home/hxie1/data/OvarianCancerCT/pixelSize223/numpy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '2', '1']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
            8    at July 30th 03:00, 2019:
                    add learning rate print;
                    use convStride =2;
                    add filter number by 2 times along deeper layers.
            9    at July 30th, 10:13, 2019:
                    add MaxPool2d in stage1;
                    add final filters to 2048.
            10   at July 30th, 15:23, 2019
                    final conv layer filter number: 1024
            11   at Aug 10th, 2019:
                    A. Add new patient data; and exclude non-standard patient data;
                    B. test the k-th fold,  validation on the (k+1)th fold;
                    C. new inputsize: 231*251*251 with pixels size 3*2*2 mm
                    D. data normalize into [0,1] after window level shresthold [0,300]
                    E. put data padding in to converting from nrrd to numpy;
                    F. Add STN network;
                    
                    
                                                        
                    
            
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   total 220 patients, 5-fold cross validation, test 45, validation 45, and training 130.  

Training strategy: 

          

Program starting Time: 2019-08-11 00:48:35.068662
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt/20190811_004835

Info: this is the 2th fold leave for test in the 5-fold cross-validation.

Infor: In all data of 220 files, label 0 has 77 files,
	  and label 1 has 143 files, where positive response rate = 0.65 in full data
5-fold cross validation: the 2th fold is for test, the 3th fold is for validation, remaining folds are for training.
test dataset:		 total 44 files, where 1 has 29.0 with rate of 0.6590909090909091
training dataset:		 total 133 files, where 1 has 86.0 with rate of 0.6466165413533834
validation dataset:		 total 43 files, where 1 has 28.0 with rate of 0.6511627906976745
=== Network trains from scratch ====
Network has total 77,852,006 parameters.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	LearningRate		TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	1.0000e-04		0.4941	0.5338	0.5465	0.5106		0.4823	0.6512	1.0000	0.0000		0.4823	0.6591	1.0000	0.0000
5	1.0000e-04		1.0278	0.5414	0.5698	0.4894		0.6930	0.6047	0.8929	0.0667		0.7975	0.6591	0.8621	0.2667
10	9.5000e-05		0.5211	0.4662	0.3721	0.6383		0.6331	0.6512	1.0000	0.0000		0.5340	0.6591	0.9310	0.1333
15	9.5000e-05		0.5662	0.5038	0.4651	0.5745		0.9501	0.6512	1.0000	0.0000		0.8556	0.6591	1.0000	0.0000
20	9.0250e-05		0.5171	0.5188	0.4767	0.5957		0.5312	0.3256	0.2143	0.5333		0.4783	0.6136	0.4828	0.8667
25	9.0250e-05		0.6886	0.5188	0.5116	0.5319		0.5305	0.3953	0.1786	0.8000		0.4990	0.4545	0.2414	0.8667
30	8.5737e-05		0.9630	0.4812	0.4070	0.6170		0.4373	0.7209	0.8214	0.5333		0.5291	0.5227	0.6552	0.2667
35	8.5737e-05		0.5224	0.5113	0.5581	0.4255		0.4667	0.4419	0.3214	0.6667		0.5425	0.4773	0.3448	0.7333
40	8.1451e-05		0.4755	0.6316	0.7674	0.3830		0.5471	0.3488	0.1071	0.8000		0.5731	0.2955	0.1034	0.6667
45	8.1451e-05		0.5325	0.6241	0.7442	0.4043		0.5122	0.3953	0.0714	1.0000		0.5961	0.3636	0.0345	1.0000
50	7.7378e-05		0.4775	0.4361	0.3140	0.6596		0.6018	0.6279	0.9643	0.0000		0.5916	0.6136	0.8966	0.0667
55	7.7378e-05		0.5062	0.5489	0.5814	0.4894		0.4578	0.6279	0.5714	0.7333		0.5053	0.5227	0.5862	0.4000
60	7.3509e-05		0.4941	0.5038	0.4651	0.5745		0.4764	0.5581	0.4643	0.7333		0.5080	0.4545	0.3448	0.6667
65	7.3509e-05		0.4724	0.5489	0.5814	0.4894		0.4704	0.5814	0.6786	0.4000		0.4935	0.5227	0.6897	0.2000
70	6.9834e-05		0.6114	0.3910	0.1279	0.8723		0.4961	0.6047	0.7857	0.2667		0.5353	0.5682	0.7931	0.1333
75	6.9834e-05		0.4860	0.6617	0.7907	0.4255		0.4664	0.5814	0.4286	0.8667		0.5189	0.4318	0.2759	0.7333
80	6.6342e-05		0.4603	0.6090	0.7558	0.3404		0.4584	0.5814	0.4643	0.8000		0.4971	0.5455	0.5862	0.4667
85	6.6342e-05		0.5109	0.6541	0.8605	0.2766		0.5096	0.5814	0.7500	0.2667		0.4888	0.5682	0.7241	0.2667
90	6.3025e-05		0.5059	0.4060	0.1512	0.8723		0.4885	0.6047	0.7143	0.4000		0.4895	0.5682	0.6897	0.3333
95	6.3025e-05		0.5516	0.6391	0.9302	0.1064		0.4767	0.5814	0.5714	0.6000		0.4980	0.4773	0.5172	0.4000
100	5.9874e-05		0.4473	0.6466	0.8023	0.3617		0.5367	0.6047	0.8214	0.2000		0.5139	0.6136	0.7931	0.2667
105	5.9874e-05		0.4572	0.6015	0.6860	0.4468		0.5094	0.6047	0.8214	0.2000		0.5049	0.6591	0.7931	0.4000
110	5.6880e-05		0.4906	0.6015	0.7558	0.3191		0.5166	0.3953	0.2500	0.6667		0.5056	0.4318	0.2069	0.8667
115	5.6880e-05		0.4661	0.5338	0.3605	0.8511		0.5371	0.4651	0.2500	0.8667		0.5217	0.4318	0.1724	0.9333
120	5.4036e-05		0.4415	0.6692	0.7209	0.5745		0.5078	0.5581	0.5714	0.5333		0.4842	0.6136	0.6552	0.5333
125	5.4036e-05		0.4503	0.5639	0.5465	0.5957		0.5054	0.4884	0.5000	0.4667		0.4906	0.5455	0.4828	0.6667
130	5.1334e-05		0.4404	0.4887	0.2791	0.8723		0.5015	0.5116	0.5714	0.4000		0.4979	0.5682	0.5862	0.5333
135	5.1334e-05		0.4949	0.4361	0.1395	0.9787		0.5357	0.4651	0.3214	0.7333		0.5197	0.5000	0.3103	0.8667
140	4.8767e-05		0.4887	0.5038	0.3721	0.7447		0.5069	0.4419	0.3214	0.6667		0.5035	0.5000	0.3793	0.7333
145	4.8767e-05		0.4789	0.4737	0.2907	0.8085		0.5362	0.4419	0.3214	0.6667		0.5288	0.4545	0.2759	0.8000
150	4.6329e-05		0.4526	0.5338	0.4186	0.7447		0.5369	0.4884	0.5000	0.4667		0.4920	0.5227	0.4483	0.6667
155	4.6329e-05		0.4491	0.5338	0.4070	0.7660		0.5323	0.5581	0.6071	0.4667		0.4763	0.6136	0.7586	0.3333
160	4.4013e-05		0.4346	0.5714	0.4767	0.7447		0.5432	0.5349	0.5714	0.4667		0.4702	0.5682	0.6552	0.4000
165	4.4013e-05		0.4771	0.4812	0.3140	0.7872		0.5663	0.3721	0.1429	0.8000		0.5274	0.4318	0.2069	0.8667
170	4.1812e-05		0.5014	0.6165	0.6628	0.5319		0.5418	0.5116	0.5714	0.4000		0.4560	0.6136	0.6207	0.6000
175	4.1812e-05		0.4667	0.5263	0.3953	0.7660		0.5464	0.3953	0.2500	0.6667		0.5204	0.4318	0.2414	0.8000
180	3.9721e-05		0.4977	0.6466	0.8140	0.3404		0.5445	0.4884	0.5000	0.4667		0.4726	0.6136	0.5517	0.7333
185	3.9721e-05		0.4912	0.5789	0.6279	0.4894		0.5302	0.4419	0.4286	0.4667		0.4755	0.5682	0.5172	0.6667
190	3.7735e-05		0.4609	0.4586	0.2326	0.8723		0.5495	0.4419	0.4286	0.4667		0.4985	0.4773	0.3448	0.7333
195	3.7735e-05		0.4429	0.6165	0.5465	0.7447		0.5409	0.4419	0.4286	0.4667		0.4678	0.5227	0.4138	0.7333
200	3.5849e-05		0.4317	0.7444	0.8605	0.5319		0.5450	0.5116	0.5714	0.4000		0.4862	0.5455	0.5172	0.6000
205	3.5849e-05		0.4384	0.5564	0.3721	0.8936		0.5497	0.5116	0.5357	0.4667		0.4803	0.5455	0.5517	0.5333
210	3.4056e-05		0.4286	0.6241	0.6512	0.5745		0.5545	0.5349	0.6071	0.4000		0.4734	0.6136	0.7931	0.2667
215	3.4056e-05		0.4404	0.6541	0.7442	0.4894		0.5812	0.5581	0.6429	0.4000		0.4772	0.5909	0.6897	0.4000
220	3.2353e-05		0.4746	0.6541	0.6977	0.5745		0.5761	0.4651	0.4643	0.4667		0.4798	0.5909	0.5172	0.7333
