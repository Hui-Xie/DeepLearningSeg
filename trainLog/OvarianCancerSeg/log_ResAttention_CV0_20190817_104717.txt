=============training from sratch============
Program ID: 27149

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt', '1', '/home/hxie1/data/OvarianCancerCT/pixelSize223/numpy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '0', '2,3']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
            8    at July 30th 03:00, 2019:
                    add learning rate print;
                    use convStride =2;
                    add filter number by 2 times along deeper layers.
            9    at July 30th, 10:13, 2019:
                    add MaxPool2d in stage1;
                    add final filters to 2048.
            10   at July 30th, 15:23, 2019
                    final conv layer filter number: 1024
            11   at Aug 10th, 2019:
                    A. Add new patient data; and exclude non-standard patient data;
                    B. test the k-th fold,  validation on the (k+1)th fold;
                    C. new inputsize: 231*251*251 with pixels size 3*2*2 mm
                    D. window level shresthold [0,300]
                    E. put data padding in to converting from nrrd to numpy;
                    F. Add STN network as first laye of network
                    G. change input data into gaussian normalization in slice by slice; (x-mean)/std;
            12   Aug 12th, 2019
                    A. fix the bug that SITk reads int imape, and normalization into interger error;
                    B  test without SPN.           
            13   Aug 13th, 2019
                    A change input data into Gausssian distribution with non-zero mean,
                      it will make the padding zero do not occupy meaning of gaussian distribution.        
                    B add filters in the ResNeXt network to 128 at first stage, and reduce batchSize to 3. 
                    C Add STN at the beginning of the ResNeXt network
            14   Aug 14th, 2019
                    A. Fix the Bug the STN convert all image into 0 problem;
                    B. change DataTransform: change shear into [-30, 30], and add scale[0.6,1.2]
                    C  put STN at teh begginning of the network;
                    D  change optimizer from SGD to Adam; and change learning rate decay with gamma=0.5 per 20 steps.                                             
            15   Aug 16th, 2019
                    A The affine matrix in the STN is divided by its spectral norm;
                    B All conv layer add spectralNorm, while all FC layer do not add spectral Norm;
                    C reduce initial LR at 0.0001, and decay step to 30.
            16   Aug 17th, 2019
                    A  Cancel STN;
                    B  Add LeakyReLU;
                    C  before final FC layer, add ReLU and LocalResponseNorm   
            17   Aug 17th, 2019:10:44am
                    A enable STN;
                    B STN support LeakyReLU and localResponseNorm        
                                
            
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   total 220 patients, 5-fold cross validation, test 45, validation 45, and training 130.  

Training strategy: 

          

Program starting Time: 2019-08-17 10:47:17.197076
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt/20190817_104717

Info: this is the 0th fold leave for test in the 5-fold cross-validation.

Infor: In all data of 220 files, label 0 has 77 files,
	  and label 1 has 143 files, where positive response rate = 0.65 in full data
5-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.
training dataset:	 total 130 files, where 1 has 85.0 with rate of 0.6538461538461539
validation dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
test dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
=== Network trains from scratch ====
Network has total 323,951,148 parameters.
Info: program will use GPU [2, 3] from all 4 GPUs.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	LearningRate		TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	1.0000e-04		2.3516	0.5077	0.4824	0.5556		0.4958	0.3556	0.0000	1.0000		0.4958	0.3556	0.0000	1.0000
5	1.0000e-04		0.6421	0.5231	0.5529	0.4667		0.5775	0.4667	0.6207	0.1875		0.6466	0.4444	0.5172	0.3125
10	1.0000e-04		0.5841	0.5308	0.6471	0.3111		0.5348	0.5111	0.5862	0.3750		0.4845	0.6000	0.6897	0.4375
15	1.0000e-04		0.5767	0.4308	0.4353	0.4222		0.9746	0.3556	0.0000	1.0000		0.7919	0.3556	0.0000	1.0000
20	1.0000e-04		0.4891	0.3462	0.1294	0.7556		0.4677	0.4000	0.0690	1.0000		0.4802	0.3556	0.0690	0.8750
25	1.0000e-04		0.4817	0.4231	0.1765	0.8889		0.5010	0.3778	0.0690	0.9375		0.4934	0.4000	0.1034	0.9375
30	1.0000e-04		0.4922	0.5231	0.6941	0.2000		0.5961	0.3556	0.0000	1.0000		0.5569	0.3778	0.0345	1.0000
35	1.0000e-04		0.4921	0.4615	0.3412	0.6889		0.4940	0.3111	0.0000	0.8750		0.4827	0.3556	0.0000	1.0000
40	5.0000e-05		0.4879	0.4615	0.3882	0.6000		0.5207	0.3333	0.0000	0.9375		0.5061	0.4000	0.0690	1.0000
45	5.0000e-05		0.4876	0.4308	0.3294	0.6222		0.5970	0.3556	0.0000	1.0000		0.5896	0.3556	0.0000	1.0000
50	5.0000e-05		0.4917	0.5692	0.7529	0.2222		0.5215	0.5556	0.7241	0.2500		0.4985	0.6000	0.7931	0.2500
55	5.0000e-05		0.4795	0.6385	0.7882	0.3556		0.4907	0.6000	0.7931	0.2500		0.4764	0.6000	0.7931	0.2500
60	5.0000e-05		0.4941	0.5692	0.7176	0.2889		0.5238	0.6000	0.6897	0.4375		0.5017	0.6000	0.7586	0.3125
65	5.0000e-05		0.4848	0.5385	0.6118	0.4000		0.5285	0.6000	0.6552	0.5000		0.5242	0.6222	0.6897	0.5000
70	5.0000e-05		0.4863	0.4769	0.5529	0.3333		0.5033	0.3556	0.0000	1.0000		0.4965	0.4000	0.0690	1.0000
75	5.0000e-05		0.4871	0.5692	0.8118	0.1111		0.5122	0.4000	0.1034	0.9375		0.4814	0.4222	0.1034	1.0000
80	2.5000e-05		0.4661	0.5308	0.4471	0.6889		0.5173	0.3556	0.0000	1.0000		0.4888	0.4000	0.0690	1.0000
85	2.5000e-05		0.4821	0.5692	0.7412	0.2444		0.6346	0.5111	0.4138	0.6875		0.6254	0.5556	0.5172	0.6250
90	2.5000e-05		0.4835	0.5385	0.6588	0.3111		0.4867	0.5333	0.6207	0.3750		0.4803	0.5333	0.6207	0.3750
95	2.5000e-05		0.4858	0.5154	0.5176	0.5111		0.5095	0.3778	0.1724	0.7500		0.4898	0.3333	0.2069	0.5625
100	2.5000e-05		0.4756	0.5385	0.6000	0.4222		1.1595	0.3778	0.0345	1.0000		0.9158	0.4222	0.1034	1.0000
105	2.5000e-05		0.4871	0.5538	0.6588	0.3556		0.5608	0.4444	0.3793	0.5625		0.5367	0.5556	0.5172	0.6250
110	2.5000e-05		0.4771	0.6308	0.7059	0.4889		0.5082	0.5111	0.5862	0.3750		0.4826	0.6444	0.8276	0.3125
115	2.5000e-05		0.4786	0.6231	0.8235	0.2444		0.5002	0.5111	0.6207	0.3125		0.4822	0.6000	0.7931	0.2500
120	1.2500e-05		0.4849	0.5308	0.6000	0.4000		0.5478	0.4889	0.3448	0.7500		0.5028	0.2667	0.1379	0.5000
125	1.2500e-05		0.4832	0.6000	0.7765	0.2667		0.5664	0.4889	0.5172	0.4375		0.5147	0.6000	0.6897	0.4375
130	1.2500e-05		0.4806	0.5846	0.7529	0.2667		0.4896	0.5556	0.6897	0.3125		0.4753	0.5556	0.7586	0.1875
135	1.2500e-05		0.4879	0.4923	0.5059	0.4667		0.6810	0.4889	0.3793	0.6875		0.5790	0.6222	0.6552	0.5625
140	1.2500e-05		0.4923	0.5615	0.7529	0.2000		0.5273	0.5111	0.5172	0.5000		0.4924	0.6222	0.6207	0.6250
145	1.2500e-05		0.4697	0.7000	0.8235	0.4667		0.6900	0.4444	0.3103	0.6875		0.6310	0.6000	0.5172	0.7500
150	1.2500e-05		0.4901	0.6000	0.7765	0.2667		0.5471	0.4667	0.3793	0.6250		0.5185	0.5556	0.5172	0.6250
155	1.2500e-05		0.4801	0.5308	0.4941	0.6000		0.6712	0.3556	0.0690	0.8750		0.5934	0.3556	0.0690	0.8750
160	6.2500e-06		0.4770	0.5692	0.6706	0.3778		0.7114	0.3556	0.0000	1.0000		0.6398	0.3778	0.0345	1.0000
165	6.2500e-06		0.4914	0.5846	0.8000	0.1778		0.5005	0.5333	0.4138	0.7500		0.4835	0.6000	0.5517	0.6875
170	6.2500e-06		0.4831	0.5385	0.7412	0.1556		0.6222	0.4222	0.3103	0.6250		0.5531	0.5778	0.5517	0.6250
175	6.2500e-06		0.4827	0.6154	0.8235	0.2222		0.7292	0.4667	0.3793	0.6250		0.6204	0.6444	0.6207	0.6875
180	6.2500e-06		0.4746	0.5846	0.7529	0.2667		0.4951	0.5333	0.6552	0.3125		0.4777	0.6222	0.7586	0.3750
185	6.2500e-06		0.4782	0.6154	0.8118	0.2444		0.5307	0.5333	0.5517	0.5000		0.4883	0.6667	0.7586	0.5000
190	6.2500e-06		0.4939	0.4385	0.4588	0.4000		0.5966	0.4889	0.5172	0.4375		0.5208	0.5778	0.5862	0.5625
195	6.2500e-06		0.4811	0.5923	0.7647	0.2667		0.4911	0.5556	0.6207	0.4375		0.4742	0.5778	0.6207	0.5000
200	3.1250e-06		0.4720	0.6154	0.7529	0.3556		0.6509	0.4889	0.4828	0.5000		0.5755	0.6444	0.6897	0.5625
205	3.1250e-06		0.4784	0.4923	0.5529	0.3778		0.6588	0.4000	0.2069	0.7500		0.5656	0.4000	0.2414	0.6875
210	3.1250e-06		0.4861	0.5462	0.6706	0.3111		0.5874	0.5556	0.5517	0.5625		0.5238	0.5556	0.5862	0.5000
215	3.1250e-06		0.4841	0.5615	0.7412	0.2222		0.9164	0.5111	0.4483	0.6250		0.7371	0.5111	0.5172	0.5000
220	3.1250e-06		0.4720	0.6077	0.7294	0.3778		0.5171	0.5333	0.5172	0.5625		0.4892	0.5778	0.6552	0.4375
225	3.1250e-06		0.4893	0.5000	0.7059	0.1111		0.6163	0.4222	0.4138	0.4375		0.5436	0.5778	0.6207	0.5000
230	3.1250e-06		0.4752	0.6077	0.7882	0.2667		0.5775	0.5333	0.5517	0.5000		0.5206	0.5556	0.5862	0.5000
235	3.1250e-06		0.4860	0.5077	0.6118	0.3111		2.1860	0.4222	0.2069	0.8125		1.4807	0.5333	0.4483	0.6875
240	1.5625e-06		0.4816	0.6154	0.8000	0.2667		0.5959	0.4889	0.4483	0.5625		0.5255	0.6000	0.6207	0.5625
245	1.5625e-06		0.4801	0.6462	0.8235	0.3111		0.6661	0.4889	0.4828	0.5000		0.5804	0.6000	0.6552	0.5000
250	1.5625e-06		0.4653	0.6615	0.8353	0.3333		0.5699	0.5778	0.5862	0.5625		0.5165	0.6444	0.7241	0.5000
255	1.5625e-06		0.4801	0.6231	0.8235	0.2444		0.5553	0.5111	0.5517	0.4375		0.5078	0.6444	0.6897	0.5625
260	1.5625e-06		0.4820	0.5077	0.6118	0.3111		0.5296	0.5111	0.5517	0.4375		0.4911	0.6000	0.6552	0.5000
265	1.5625e-06		0.4737	0.6692	0.9059	0.2222		0.7225	0.4889	0.4483	0.5625		0.5891	0.6000	0.6207	0.5625
270	1.5625e-06		0.4823	0.6077	0.7647	0.3111		0.4993	0.5111	0.5862	0.3750		0.4734	0.6444	0.6897	0.5625
275	1.5625e-06		0.4740	0.6308	0.8235	0.2667		0.5749	0.5333	0.5517	0.5000		0.5188	0.6000	0.6552	0.5000
280	7.8125e-07		0.4793	0.6077	0.7765	0.2889		0.4925	0.4889	0.5517	0.3750		0.4760	0.6667	0.6897	0.6250
285	7.8125e-07		0.4806	0.5692	0.7529	0.2222		0.5848	0.4889	0.4483	0.5625		0.5144	0.6222	0.6207	0.6250
290	7.8125e-07		0.4699	0.6692	0.9059	0.2222		0.5834	0.5111	0.5172	0.5000		0.5189	0.6222	0.6207	0.6250
295	7.8125e-07		0.4740	0.6000	0.7882	0.2444		0.6514	0.4444	0.3793	0.5625		0.5445	0.5778	0.5517	0.6250
300	7.8125e-07		0.4747	0.6308	0.8118	0.2889		0.5175	0.4444	0.4828	0.3750		0.4799	0.6444	0.6552	0.6250
305	7.8125e-07		0.4786	0.6077	0.8000	0.2444		0.5172	0.5333	0.5517	0.5000		0.4874	0.6222	0.6207	0.6250
310	7.8125e-07		0.4849	0.5846	0.7647	0.2444		0.5078	0.4889	0.5517	0.3750		0.4773	0.6444	0.6552	0.6250
315	7.8125e-07		0.4774	0.6462	0.8588	0.2444		0.7004	0.5333	0.5172	0.5625		0.5994	0.6222	0.6552	0.5625
320	3.9063e-07		0.4812	0.5769	0.7765	0.2000		0.6484	0.4889	0.4483	0.5625		0.5750	0.5778	0.5862	0.5625
325	3.9063e-07		0.4735	0.6692	0.8588	0.3111		0.6514	0.5111	0.4828	0.5625		0.5614	0.5778	0.6207	0.5000
330	3.9063e-07		0.4840	0.5615	0.7529	0.2000		0.6512	0.4667	0.4483	0.5000		0.5643	0.6000	0.5862	0.6250
335	3.9063e-07		0.4739	0.6385	0.8235	0.2889		0.5439	0.5333	0.5172	0.5625		0.5092	0.5778	0.6207	0.5000
340	3.9063e-07		0.4696	0.6538	0.8235	0.3333		0.5652	0.5778	0.5862	0.5625		0.5247	0.5333	0.6207	0.3750
345	3.9063e-07		0.4800	0.6308	0.8235	0.2667		0.7570	0.4444	0.4138	0.5000		0.6609	0.5778	0.5172	0.6875
350	3.9063e-07		0.4908	0.5538	0.7176	0.2444		0.4809	0.5778	0.6897	0.3750		0.4689	0.6444	0.6552	0.6250
355	3.9063e-07		0.4862	0.6000	0.8118	0.2000		0.6284	0.5556	0.5517	0.5625		0.5722	0.5778	0.6207	0.5000
360	1.9531e-07		0.4802	0.6538	0.8706	0.2444		0.6667	0.4667	0.4138	0.5625		0.5793	0.5778	0.5517	0.6250
365	1.9531e-07		0.4700	0.6231	0.8118	0.2667		0.5187	0.4667	0.5172	0.3750		0.4875	0.6444	0.6552	0.6250
370	1.9531e-07		0.4900	0.6000	0.7529	0.3111		1.0423	0.4222	0.2414	0.7500		0.8095	0.5778	0.4828	0.7500
375	1.9531e-07		0.4869	0.5769	0.7765	0.2000		0.6526	0.5111	0.4828	0.5625		0.5567	0.6222	0.6552	0.5625
380	1.9531e-07		0.4842	0.5769	0.7765	0.2000		0.7876	0.4000	0.3448	0.5000		0.6477	0.6000	0.5517	0.6875
385	1.9531e-07		0.4797	0.5923	0.7882	0.2222		0.6980	0.4667	0.4138	0.5625		0.5877	0.5778	0.5862	0.5625
390	1.9531e-07		0.4856	0.5769	0.7412	0.2667		0.5151	0.5333	0.5517	0.5000		0.4863	0.6222	0.6552	0.5625
395	1.9531e-07		0.4740	0.6231	0.7882	0.3111		0.5075	0.4444	0.4483	0.4375		0.4795	0.6444	0.6207	0.6875
400	9.7656e-08		0.4767	0.6385	0.8118	0.3111		0.4939	0.4444	0.4828	0.3750		0.4793	0.6222	0.5862	0.6875
405	9.7656e-08		0.4760	0.6154	0.8235	0.2222		0.5879	0.5333	0.5172	0.5625		0.5229	0.5778	0.6207	0.5000
410	9.7656e-08		0.4764	0.6462	0.8471	0.2667		1.1177	0.4000	0.2759	0.6250		0.8565	0.6000	0.5517	0.6875
415	9.7656e-08		0.4859	0.6308	0.8471	0.2222		0.6337	0.4667	0.4138	0.5625		0.5626	0.6444	0.6207	0.6875
