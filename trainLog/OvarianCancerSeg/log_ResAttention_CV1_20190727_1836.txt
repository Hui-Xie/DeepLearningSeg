Program ID: 16304

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/images_npy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '1']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   training data has 169 patients 

Training strategy: 

          

Program starting Time: 2019-07-27 18:36:47.800809
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten

Info: this is the 1th fold leave for test in the 4-fold cross-validation, with 20.0% of data for independent test.

Infor: In all data of 169 files, label 0 has 51 files,
	  and label 1 has 118 files, where positive response rate = 0.6982248520710059 in full data
Infor: the independent test set has 34 files,and Training including validation Set has 135 files which will be divided into 4 folds.
In the independent test set of 34 files, 0 has 10, 1 has 24, the rate of 1s is 0.7058823529411765
In this test_1 dataset, total 34 files, where 1 has 24 with rate of 0.7058823529411765
In this train_1 dataset, total 101 files, where 1 has 70 with rate of 0.693069306930693
In this validation_1 dataset, total 34 files, where 1 has 24 with rate of 0.7058823529411765
=== Network trains from scratch ====
Network has total 1,235,220 parameters.
Info: program will use 2 GPUs.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	0.4856	0.6040	0.7286	0.3226		0.3920	0.2941	0.0000	1.0000		0.3920	0.2941	0.0000	1.0000
5	0.3858	0.6832	0.6000	0.8710		0.6055	0.4118	0.3333	0.6000		0.5635	0.4412	0.2917	0.8000
10	0.3209	0.7426	0.7571	0.7097		0.9510	0.4118	0.4583	0.3000		0.6302	0.5882	0.5000	0.8000
15	0.2588	0.8020	0.8714	0.6452		1.0028	0.3824	0.4583	0.2000		0.8411	0.5588	0.5833	0.5000
20	0.1544	0.9109	0.9571	0.8065		1.4800	0.3824	0.4167	0.3000		1.1468	0.5000	0.5000	0.5000
25	0.2814	0.8119	0.8286	0.7742		1.1294	0.3235	0.2500	0.5000		0.8029	0.4412	0.2917	0.8000
30	0.1882	0.9109	0.9286	0.8710		1.3111	0.5294	0.6667	0.2000		0.6302	0.7059	0.7500	0.6000
35	0.1108	0.9307	0.9286	0.9355		1.5165	0.5000	0.6667	0.1000		1.2357	0.6471	0.7500	0.4000
40	0.1020	0.9307	0.9143	0.9677		2.0511	0.5000	0.6667	0.1000		1.4563	0.6176	0.7500	0.3000
45	0.1165	0.9208	0.9429	0.8710		1.8209	0.3235	0.3333	0.3000		1.4597	0.4412	0.3750	0.6000
50	0.1220	0.9109	0.8857	0.9677		1.5546	0.6176	0.8333	0.1000		0.9217	0.6765	0.7917	0.4000
55	0.0414	0.9802	0.9857	0.9677		1.7589	0.6176	0.7917	0.2000		1.2084	0.6471	0.7083	0.5000
60	0.1072	0.9010	0.8714	0.9677		2.2107	0.3529	0.2917	0.5000		2.1221	0.4706	0.3750	0.7000
65	0.1045	0.9208	0.9000	0.9677		1.7703	0.5294	0.6667	0.2000		1.3722	0.6176	0.7500	0.3000
70	0.1493	0.9307	0.9286	0.9355		1.6592	0.2941	0.3333	0.2000		1.2994	0.4118	0.3333	0.6000
75	0.0932	0.9604	0.9571	0.9677		1.6686	0.5882	0.7500	0.2000		1.0160	0.6765	0.7917	0.4000
80	0.0807	0.9802	0.9857	0.9677		1.7359	0.5588	0.7083	0.2000		0.9889	0.6176	0.7083	0.4000
85	0.1944	0.9604	0.9857	0.9032		1.7096	0.4412	0.5417	0.2000		1.2214	0.5000	0.3750	0.8000
90	0.0985	0.9208	0.9143	0.9355		1.7227	0.3824	0.4583	0.2000		1.0952	0.5882	0.5833	0.6000
95	0.0595	0.9703	0.9714	0.9677		2.0139	0.4118	0.4583	0.3000		1.5131	0.4706	0.3750	0.7000
100	0.0978	0.9703	0.9857	0.9355		1.6994	0.4412	0.5417	0.2000		1.2653	0.5294	0.4583	0.7000
105	0.0541	0.9703	0.9714	0.9677		1.8669	0.5000	0.6250	0.2000		1.1349	0.7059	0.7500	0.6000
110	0.0510	0.9505	0.9429	0.9677		1.8851	0.6176	0.7917	0.2000		1.2138	0.6176	0.7500	0.3000
115	0.0851	0.9406	0.9143	1.0000		1.8987	0.4706	0.5833	0.2000		1.2472	0.6176	0.6250	0.6000
120	0.0444	0.9901	1.0000	0.9677		1.7702	0.4412	0.5417	0.2000		1.1877	0.5882	0.5833	0.6000
125	0.0399	0.9901	1.0000	0.9677		1.8450	0.4118	0.5000	0.2000		1.1640	0.5882	0.5833	0.6000
130	0.0421	0.9802	0.9857	0.9677		1.8021	0.5294	0.6667	0.2000		1.1464	0.6765	0.7500	0.5000
135	0.1078	0.9604	0.9571	0.9677		1.8227	0.5000	0.6250	0.2000		1.2085	0.6471	0.6667	0.6000
140	0.0460	1.0000	1.0000	1.0000		1.7348	0.4706	0.5833	0.2000		1.1060	0.5882	0.5833	0.6000
145	0.0453	0.9604	0.9429	1.0000		1.9233	0.5588	0.7083	0.2000		1.1704	0.7059	0.7500	0.6000
150	0.0511	0.9802	0.9857	0.9677		2.0195	0.4412	0.5417	0.2000		1.2879	0.5588	0.5417	0.6000
155	0.0255	0.9901	0.9857	1.0000		2.0465	0.5000	0.6250	0.2000		1.2417	0.6471	0.7083	0.5000
160	0.0766	0.9802	0.9857	0.9677		1.9911	0.4412	0.5417	0.2000		1.2343	0.5882	0.5833	0.6000
165	0.0251	0.9901	0.9857	1.0000		1.9266	0.4412	0.5417	0.2000		1.1927	0.5882	0.5833	0.6000
170	0.0282	0.9802	0.9714	1.0000		1.8517	0.4412	0.5417	0.2000		1.1695	0.5882	0.5833	0.6000
175	0.0714	0.9703	0.9714	0.9677		2.0489	0.4706	0.5833	0.2000		1.1678	0.6471	0.6667	0.6000
180	0.0889	0.9505	0.9429	0.9677		1.9950	0.4706	0.5833	0.2000		1.1943	0.5588	0.5417	0.6000
185	0.0397	0.9802	0.9714	1.0000		1.9867	0.4706	0.5833	0.2000		1.1951	0.6176	0.6250	0.6000
190	0.0232	0.9901	0.9857	1.0000		1.9012	0.4706	0.5833	0.2000		1.1358	0.6176	0.6250	0.6000
195	0.0671	0.9703	0.9714	0.9677		1.9790	0.4706	0.5833	0.2000		1.1635	0.6176	0.6250	0.6000
200	0.0452	0.9802	0.9857	0.9677		1.8997	0.4118	0.5000	0.2000		1.1675	0.5588	0.5417	0.6000
205	0.0768	0.9802	0.9857	0.9677		1.9142	0.4412	0.5417	0.2000		1.1537	0.5588	0.5417	0.6000
210	0.0449	0.9901	1.0000	0.9677		1.8755	0.5588	0.7083	0.2000		1.0451	0.6176	0.6667	0.5000
215	0.0423	0.9802	0.9714	1.0000		1.8403	0.5588	0.7083	0.2000		1.0364	0.6176	0.6667	0.5000
220	0.1442	0.9208	0.9429	0.8710		1.8334	0.4706	0.5833	0.2000		1.0803	0.5882	0.5833	0.6000
225	0.0570	0.9802	0.9714	1.0000		1.8390	0.4706	0.5833	0.2000		1.0897	0.5588	0.5417	0.6000
230	0.0202	0.9901	0.9857	1.0000		1.8148	0.5000	0.6250	0.2000		1.0282	0.6471	0.6667	0.6000
235	0.0782	0.9703	0.9714	0.9677		1.8802	0.4706	0.5833	0.2000		1.1063	0.6176	0.6250	0.6000
240	0.0815	0.9703	0.9714	0.9677		1.8682	0.5588	0.7083	0.2000		1.0260	0.6765	0.7500	0.5000
245	0.0695	0.9604	0.9571	0.9677		1.9271	0.4706	0.5833	0.2000		1.1113	0.6176	0.6250	0.6000
250	0.0436	0.9901	0.9857	1.0000		1.8501	0.5000	0.6250	0.2000		1.0743	0.6176	0.6667	0.5000
255	0.0346	0.9901	1.0000	0.9677		1.8769	0.4706	0.5833	0.2000		1.1030	0.5882	0.6250	0.5000
260	0.0889	0.9703	0.9714	0.9677		1.8926	0.4706	0.5833	0.2000		1.0771	0.6176	0.6250	0.6000
265	0.0247	0.9901	0.9857	1.0000		1.8315	0.5294	0.6667	0.2000		1.0235	0.6176	0.6667	0.5000
270	0.0244	1.0000	1.0000	1.0000		1.8448	0.5000	0.6250	0.2000		1.0557	0.6471	0.6667	0.6000
275	0.0488	0.9901	0.9857	1.0000		1.8399	0.4412	0.5417	0.2000		1.0739	0.5882	0.5833	0.6000
280	0.0732	0.9802	0.9857	0.9677		1.7711	0.4706	0.5833	0.2000		1.0304	0.5882	0.5833	0.6000
285	0.0436	0.9604	0.9571	0.9677		1.8568	0.5294	0.6667	0.2000		1.0291	0.6176	0.6667	0.5000
290	0.0381	0.9802	0.9714	1.0000		1.9486	0.4412	0.5417	0.2000		1.1804	0.5882	0.6250	0.5000
295	0.0274	0.9901	0.9857	1.0000		1.8497	0.5294	0.6667	0.2000		1.1128	0.6471	0.7083	0.5000
300	0.0532	0.9802	0.9857	0.9677		1.8332	0.5000	0.6250	0.2000		1.0591	0.6471	0.6667	0.6000
305	0.0713	0.9901	1.0000	0.9677		1.8555	0.5000	0.6250	0.2000		1.0968	0.5882	0.6250	0.5000
310	0.0791	0.9703	0.9714	0.9677		1.8372	0.4706	0.5833	0.2000		1.1362	0.6176	0.6667	0.5000
315	0.0335	0.9901	0.9857	1.0000		1.9544	0.5294	0.6667	0.2000		1.1053	0.6471	0.6667	0.6000
320	0.0279	1.0000	1.0000	1.0000		1.8546	0.4706	0.5833	0.2000		1.0935	0.6176	0.6250	0.6000
325	0.0391	0.9901	0.9857	1.0000		1.9124	0.4706	0.5833	0.2000		1.1324	0.6176	0.6250	0.6000
330	0.0201	0.9802	0.9714	1.0000		1.9037	0.5294	0.6667	0.2000		1.1211	0.6176	0.6667	0.5000
335	0.0451	0.9703	0.9571	1.0000		1.8390	0.4412	0.5417	0.2000		1.1345	0.5588	0.5417	0.6000
340	0.0438	0.9604	0.9429	1.0000		1.8474	0.4706	0.5833	0.2000		1.1335	0.5588	0.5417	0.6000
345	0.0680	0.9703	0.9571	1.0000		1.8026	0.4412	0.5417	0.2000		1.1017	0.5882	0.5833	0.6000
350	0.0626	0.9802	0.9857	0.9677		1.8531	0.5294	0.6667	0.2000		1.0890	0.6176	0.6667	0.5000
355	0.0756	0.9901	1.0000	0.9677		1.9074	0.5294	0.6667	0.2000		1.1169	0.6471	0.7083	0.5000
360	0.0331	0.9901	0.9857	1.0000		1.8512	0.4706	0.5833	0.2000		1.1224	0.6176	0.6667	0.5000
365	0.0183	1.0000	1.0000	1.0000		1.8458	0.5294	0.6667	0.2000		1.0470	0.6471	0.6667	0.6000


 training loss less than 0.02, Program exit.


=============END of Training of ResAttentionNet Predict Model =================

Program Ending Time: 2019-07-27 19:39:02.219058
