=============training from sratch============
Program ID: 2457

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/images_npy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '1', '1']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
                    
            
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   training data has 169 patients 

Training strategy: 

          

Program starting Time: 2019-07-29 13:29:17.413775
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten/20190729_132917

Info: this is the 1th fold leave for test in the 4-fold cross-validation, with 20.0% of data for independent test.

Infor: In all data of 169 files, label 0 has 51 files,
	  and label 1 has 118 files, where positive response rate = 0.6982248520710059 in full data
Infor: the independent test set has 34 files,and Training including validation Set has 135 files which will be divided into 4 folds.
In the independent test set of 34 files, 0 has 10, 1 has 24, the rate of 1s is 0.7058823529411765
In this test_1 dataset, total 34 files, where 1 has 24 with rate of 0.7058823529411765
In this train_1 dataset, total 101 files, where 1 has 70 with rate of 0.693069306930693
In this validation_1 dataset, total 34 files, where 1 has 24 with rate of 0.7058823529411765
=== Network trains from scratch ====
Network has total 247,588 parameters.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	0.4796	0.6040	0.7571	0.2581		0.3943	0.2941	0.0000	1.0000		0.3942	0.2941	0.0000	1.0000
5	0.4262	0.4059	0.2714	0.7097		0.4201	0.2941	0.0000	1.0000		0.4112	0.2941	0.0000	1.0000
10	0.4121	0.6139	0.8286	0.1290		0.3921	0.7059	1.0000	0.0000		0.3834	0.7059	1.0000	0.0000
15	0.4095	0.4554	0.3286	0.7419		0.3865	0.6471	0.8333	0.2000		0.3725	0.7941	0.9583	0.4000
20	0.4260	0.6832	0.9857	0.0000		0.3904	0.5882	0.7500	0.2000		0.3895	0.6765	0.8333	0.3000
25	0.4245	0.5149	0.5571	0.4194		0.3928	0.5588	0.6667	0.3000		0.3992	0.6471	0.7500	0.4000
30	0.3984	0.6634	0.7286	0.5161		0.4706	0.4706	0.5833	0.2000		0.4068	0.5882	0.6250	0.5000
35	0.3748	0.6634	0.6571	0.6774		0.5423	0.5294	0.6667	0.2000		0.4112	0.7059	0.8750	0.3000
40	0.3751	0.6040	0.5143	0.8065		0.4628	0.6765	0.9167	0.1000		0.3161	0.7059	0.9583	0.1000
45	0.3288	0.7030	0.6571	0.8065		0.8271	0.3235	0.1250	0.8000		0.8419	0.3235	0.0833	0.9000
50	0.3885	0.6931	0.6571	0.7742		0.5861	0.5294	0.5833	0.4000		0.3918	0.5882	0.5833	0.6000
55	0.3388	0.7030	0.6714	0.7742		0.4406	0.6471	0.7500	0.4000		0.3310	0.6765	0.7083	0.6000
60	0.3459	0.6436	0.5714	0.8065		0.6057	0.5882	0.7500	0.2000		0.4161	0.6471	0.7917	0.3000
65	0.2456	0.7723	0.7429	0.8387		0.9636	0.3824	0.3750	0.4000		0.8429	0.5294	0.5417	0.5000
70	0.2094	0.8812	0.9143	0.8065		0.6816	0.5588	0.6667	0.3000		0.5931	0.5588	0.7083	0.2000
75	0.1841	0.8614	0.8286	0.9355		0.9834	0.4118	0.4583	0.3000		1.0384	0.4706	0.5000	0.4000
80	0.1998	0.8614	0.8429	0.9032		1.3256	0.6765	0.9583	0.0000		1.2316	0.7059	1.0000	0.0000
85	0.2250	0.8713	0.8429	0.9355		1.0753	0.4118	0.2917	0.7000		1.1604	0.4706	0.3333	0.8000
90	0.1729	0.8713	0.8571	0.9032		0.6902	0.6176	0.7500	0.3000		0.6912	0.6765	0.7917	0.4000
95	0.0905	0.9406	0.9143	1.0000		0.8329	0.5000	0.5000	0.5000		0.8652	0.5294	0.5417	0.5000
100	0.1538	0.9010	0.8857	0.9355		0.9847	0.5882	0.7917	0.1000		1.0034	0.5882	0.7083	0.3000
105	0.1514	0.8911	0.8857	0.9032		0.8234	0.6765	0.8333	0.3000		0.7621	0.6471	0.7917	0.3000
110	0.1015	0.9406	0.9143	1.0000		0.7620	0.5588	0.6667	0.3000		0.8796	0.6471	0.7500	0.4000
115	0.0794	0.9802	0.9714	1.0000		0.8667	0.4706	0.5000	0.4000		0.8979	0.5588	0.5833	0.5000
120	0.0713	0.9406	0.9286	0.9677		0.8889	0.5000	0.5833	0.3000		1.0377	0.5588	0.5833	0.5000
125	0.2959	0.9307	0.9429	0.9032		1.0888	0.5588	0.6250	0.4000		1.0785	0.5882	0.7083	0.3000
130	0.1137	0.9109	0.9000	0.9355		1.0581	0.5588	0.5833	0.5000		1.0046	0.5882	0.6250	0.5000
135	0.0990	0.9109	0.8857	0.9677		0.9274	0.3824	0.2917	0.6000		0.8519	0.4706	0.3333	0.8000
140	0.0899	0.9208	0.9143	0.9355		0.8864	0.5882	0.7083	0.3000		1.0461	0.5882	0.6667	0.4000
145	0.0934	0.9307	0.9143	0.9677		1.0737	0.5000	0.6250	0.2000		0.9773	0.6176	0.7083	0.4000
150	0.0802	0.9703	0.9714	0.9677		0.9191	0.5882	0.6667	0.4000		0.9454	0.5882	0.6667	0.4000
155	0.1289	0.9505	0.9429	0.9677		1.1204	0.4706	0.4583	0.5000		1.0791	0.5882	0.5417	0.7000
160	0.1374	0.9307	0.9429	0.9032		0.9585	0.5294	0.5833	0.4000		1.0188	0.5588	0.6250	0.4000
165	0.1024	0.9406	0.9286	0.9677		1.1003	0.5294	0.5833	0.4000		1.0969	0.5588	0.6250	0.4000
170	0.1000	0.9406	0.9571	0.9032		1.0928	0.5588	0.6667	0.3000		1.0862	0.5882	0.7083	0.3000
175	0.0471	0.9802	0.9714	1.0000		1.1110	0.5294	0.6250	0.3000		1.1510	0.5588	0.6250	0.4000
180	0.0958	0.9604	0.9429	1.0000		0.9307	0.5294	0.5833	0.4000		1.0632	0.5882	0.6667	0.4000
185	0.0547	0.9703	0.9571	1.0000		0.9925	0.5882	0.6667	0.4000		1.0663	0.5882	0.7083	0.3000
190	0.0522	0.9901	0.9857	1.0000		1.0538	0.5588	0.6250	0.4000		1.1120	0.5588	0.6250	0.4000
195	0.1186	0.9010	0.8714	0.9677		1.0534	0.5882	0.7083	0.3000		1.0364	0.6176	0.7083	0.4000
200	0.0349	0.9901	0.9857	1.0000		1.0383	0.5588	0.6250	0.4000		1.0006	0.6471	0.6667	0.6000
205	0.0390	0.9703	0.9571	1.0000		0.9544	0.5294	0.5833	0.4000		0.9300	0.5882	0.6667	0.4000
210	0.0541	0.9406	0.9429	0.9355		0.9857	0.5294	0.5833	0.4000		0.9569	0.5882	0.6250	0.5000
215	0.0432	0.9604	0.9429	1.0000		0.9488	0.6176	0.6667	0.5000		0.9493	0.6176	0.7083	0.4000
220	0.1285	0.9208	0.9143	0.9355		0.9840	0.5294	0.5833	0.4000		1.0603	0.5882	0.6250	0.5000
225	0.1052	0.9703	0.9571	1.0000		1.0975	0.5882	0.7083	0.3000		1.0751	0.6176	0.7083	0.4000
230	0.0673	0.9604	0.9429	1.0000		0.9842	0.5588	0.6250	0.4000		0.9834	0.5882	0.6250	0.5000
235	0.0386	0.9901	0.9857	1.0000		0.9858	0.5294	0.5833	0.4000		1.0409	0.6176	0.6667	0.5000
240	0.0493	0.9802	0.9714	1.0000		1.0053	0.4706	0.5000	0.4000		1.0658	0.5882	0.6250	0.5000
245	0.0542	0.9802	0.9714	1.0000		1.0143	0.5294	0.5833	0.4000		1.0748	0.5882	0.6250	0.5000
250	0.0837	0.9604	0.9571	0.9677		1.0724	0.5588	0.6250	0.4000		1.1050	0.5882	0.6667	0.4000
255	0.0913	0.9901	0.9857	1.0000		1.0286	0.5588	0.6250	0.4000		1.0083	0.6176	0.6667	0.5000
260	0.0425	0.9802	0.9714	1.0000		0.9935	0.5294	0.6250	0.3000		0.9809	0.6176	0.6667	0.5000
265	0.0408	0.9901	0.9857	1.0000		1.0300	0.5588	0.6667	0.3000		1.0524	0.6176	0.6667	0.5000
270	0.0900	0.9604	0.9429	1.0000		1.0571	0.5294	0.5833	0.4000		1.1320	0.5882	0.6250	0.5000
275	0.1159	0.9604	0.9571	0.9677		0.9917	0.5588	0.6667	0.3000		1.0020	0.6176	0.6667	0.5000
280	0.0647	0.9901	0.9857	1.0000		0.9568	0.5588	0.6250	0.4000		1.0022	0.5882	0.6250	0.5000
285	0.0631	0.9604	0.9429	1.0000		1.0894	0.5588	0.6667	0.3000		1.0770	0.6176	0.6667	0.5000
290	0.0692	0.9604	0.9429	1.0000		1.0170	0.5000	0.5417	0.4000		1.0725	0.6176	0.6250	0.6000
295	0.0836	0.9604	0.9429	1.0000		0.9847	0.5882	0.6667	0.4000		0.9941	0.6176	0.6667	0.5000
300	0.0413	0.9802	0.9857	0.9677		0.9954	0.5882	0.7083	0.3000		0.9694	0.6176	0.7083	0.4000
305	0.0730	0.9604	0.9429	1.0000		1.0391	0.5882	0.7083	0.3000		1.0257	0.6176	0.7083	0.4000
310	0.0787	0.9505	0.9429	0.9677		1.0173	0.5000	0.5417	0.4000		1.0575	0.6176	0.6250	0.6000
315	0.0825	0.9703	0.9571	1.0000		0.9963	0.5000	0.5417	0.4000		1.0593	0.5882	0.6250	0.5000
320	0.0616	0.9604	0.9571	0.9677		0.9957	0.5000	0.5417	0.4000		1.0452	0.6176	0.6250	0.6000
325	0.0528	0.9802	0.9857	0.9677		1.0130	0.5588	0.6667	0.3000		1.0356	0.6176	0.6667	0.5000
330	0.0640	0.9505	0.9429	0.9677		1.0331	0.5882	0.6667	0.4000		1.0800	0.6176	0.6667	0.5000
335	0.0416	0.9802	0.9714	1.0000		0.9743	0.5588	0.6667	0.3000		0.9574	0.6176	0.6667	0.5000
340	0.1119	0.9109	0.9143	0.9032		1.0407	0.5882	0.7083	0.3000		1.0026	0.6176	0.7083	0.4000
345	0.0710	0.9604	0.9429	1.0000		1.0327	0.5882	0.7083	0.3000		0.9923	0.6176	0.7083	0.4000
350	0.0348	1.0000	1.0000	1.0000		0.9947	0.5882	0.6667	0.4000		1.0236	0.5882	0.6250	0.5000
355	0.0522	0.9802	0.9714	1.0000		0.9849	0.5588	0.6667	0.3000		0.9922	0.6471	0.7083	0.5000
360	0.2643	0.9604	0.9571	0.9677		1.0556	0.5588	0.6250	0.4000		1.0332	0.6176	0.6250	0.6000
365	0.0860	0.9505	0.9571	0.9355		1.0210	0.5588	0.6667	0.3000		0.9979	0.6176	0.6667	0.5000
370	0.0668	0.9604	0.9429	1.0000		1.0480	0.5882	0.6667	0.4000		1.0677	0.6176	0.6667	0.5000
375	0.0575	0.9703	0.9571	1.0000		0.9888	0.5882	0.6667	0.4000		1.0149	0.6176	0.6667	0.5000
380	0.1284	0.9307	0.9286	0.9355		1.0171	0.5882	0.6667	0.4000		1.0382	0.6176	0.6667	0.5000
385	0.0330	0.9604	0.9429	1.0000		0.9777	0.5882	0.6667	0.4000		0.9832	0.5882	0.6250	0.5000
390	0.0648	0.9703	0.9714	0.9677		1.0057	0.5294	0.5833	0.4000		1.0318	0.5882	0.6250	0.5000
395	0.0302	0.9901	0.9857	1.0000		0.9833	0.5294	0.5833	0.4000		0.9924	0.5882	0.6250	0.5000
400	0.0973	0.9307	0.9143	0.9677		1.0664	0.5000	0.5417	0.4000		1.1054	0.5882	0.6250	0.5000
405	0.0285	0.9901	0.9857	1.0000		0.9883	0.5588	0.6667	0.3000		0.9824	0.6176	0.6667	0.5000
410	0.0340	0.9604	0.9429	1.0000		1.0095	0.5882	0.7083	0.3000		1.0233	0.5882	0.6667	0.4000
415	0.1990	0.9802	0.9857	0.9677		1.0624	0.5294	0.5833	0.4000		1.1070	0.5882	0.6250	0.5000
420	0.0460	0.9604	0.9571	0.9677		1.0459	0.5882	0.6667	0.4000		1.0341	0.5588	0.6250	0.4000
425	0.0796	0.9604	0.9429	1.0000		1.0477	0.5000	0.5417	0.4000		1.0971	0.6176	0.6250	0.6000
430	0.0525	0.9604	0.9429	1.0000		0.9917	0.5588	0.6667	0.3000		0.9989	0.6471	0.6667	0.6000
435	0.0840	0.9604	0.9429	1.0000		0.9982	0.5000	0.5417	0.4000		1.0374	0.6176	0.6250	0.6000
440	0.0471	0.9703	0.9571	1.0000		1.0165	0.5000	0.5417	0.4000		1.0575	0.5882	0.6250	0.5000
