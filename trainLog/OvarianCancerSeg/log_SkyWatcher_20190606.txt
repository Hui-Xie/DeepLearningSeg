Program ID of SkyWatcher Network training:27152

Program command: 
 ['TrainSkyWatcher.py', '/home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainImages_ROI_147_281_281', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/testImages_ROI_147_281_281', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/patientResponseDict.json']

Major program changes: 
                     delete the m_k in the DataMgr class.
                      

Experiment setting for Image3d ROI to response:
Input CT data: 147*281*281  3D CT raw image ROI
segmentation label: 127*255*255 segmentation label with value (0,1,2) which erases lymph node label

This is a multi-task learning. 

Predictive Model: 1,  first 3-layer dense conv block with channel size 6.
                  2,  and 6 dense conv DownBB blocks,  each of which includes a stride 2 conv and 3-layers dense conv block; 
                  3,  and 3 fully connected layers  changes the tensor into size 2*1;
                  4,  final a softmax for binary classification;
                  Total network learning parameters are 25K.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/Image3dPredictModel.py

response Loss Function:   focus loss  with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.
segmentation loss function: focus loss  with weight [1.0416883685076772, 39.37007874015748, 68.39945280437757] for label (0, 1, 2)

Data:   training data has 130 patients, and test data has 32 patients with training/test rate 80/20.
        We used patient ID as index to order all patients data, and then used about the first 80% of patients as training data, 
        and the remaining 20% of patients as test data. 
        Sorting with patient ID is to make sure the division of training and test set is blind to the patient's detailed stage, 
        shape and size of cancer.  
        Therefore you will see that patient IDs of all test data are beginning at 8 or 9. 
        This training/test division is exactly same with segmentation network experiment before. 

Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.  

                    Learning Scheduler:  Reduce learning rate on  plateau, and learning rate patience is 30 epochs.                                

            

Program starting Time: 2019-06-06 10:34:41.228519
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher

Now program get 130 input files.
Now program get 32 input files.
TrainData Input:  batchSize=4, depth=147, height=281, width=281

TestData Input:  batchSize=4, depth=147, height=281, width=281

Info: the size of bottle neck in the net = 6* (1, 3, 3)

Info: program uses Mixup with alpha=0.4, and mixupProb = 0.5.
Network trains from scratch.
Network has total 27,596 parameters.
Infor: Cross Entropy Weight: [3.3333333333333335, 1.4285714285714286] for label[0, 1]
Infor: Segmentation Cross Entropy Weight: [1.0416883685076772, 39.37007874015748, 68.39945280437757] for label (0, 1, 2)
Info: program will use 4 GPUs.
Loss Functions List: FocalCELoss with weight of 1; 	FocalCELoss with weight of 1; 
Hints: Test Dice_0 is the dice coeff for all non-zero labels
Hints: Test Dice_1 is for primary cancer(green), 	
 test Dice_2 is for metastasis(yellow), 	
 and test Dice_3 is for invaded lymph node(brown).
Hints: Test TPR_0 is the TPR for all non-zero labels
Hints: Test TPR_1 is for primary cancer(green), 	
 TPR_2 is for metastasis(yellow), 	
 and TPR_3 is for invaded lymph node(brown).

Dice is based on all 2D segmented slices in the volume from weak annotation, not real 3D dice.

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	TrLoss	Dice0	Dice1	Dice2	TPR_0	TPR_1	TPR_2	Accura	TsLoss	Dice0	Dice1	Dice2	TPR_0	TPR_1	TPR_2	Accura
0	0.7565	0.174	0.205	0.003	0.347	0.328	0.001	0.6964	0.8401	0.000	0.000	0.000	0.000	0.000	0.000	0.8125
1	0.7024	0.000	0.000	0.000	0.000	0.000	0.000	0.6667	0.8098	0.000	0.000	0.000	0.000	0.000	0.000	0.8125
2	0.6331	0.000	0.000	0.000	0.000	0.000	0.000	0.6346	2.6765	0.116	0.102	0.075	0.910	0.229	0.671	0.8125
3	0.5921	0.000	0.000	0.000	0.000	0.000	0.000	0.6607	0.6428	0.187	0.215	0.027	0.630	0.406	0.020	0.8125
4	0.5674	0.000	0.000	0.000	0.000	0.000	0.000	0.6765	0.6233	0.240	0.336	0.034	0.758	0.641	0.030	0.8125
5	0.5578	0.268	0.324	0.106	0.887	0.696	0.112	0.6071	0.6570	0.217	0.254	0.000	0.555	0.386	0.000	0.8125
6	0.5570	0.000	0.000	0.000	0.000	0.000	0.000	0.6250	0.6048	0.258	0.296	0.056	0.763	0.563	0.051	0.8125
7	0.5402	0.000	0.000	0.000	0.000	0.000	0.000	0.7188	0.6372	0.244	0.326	0.013	0.592	0.489	0.014	0.8125
8	0.5385	0.000	0.000	0.000	0.000	0.000	0.000	0.7812	0.6355	0.243	0.305	0.041	0.594	0.538	0.065	0.8125
9	0.5498	0.000	0.000	0.000	0.000	0.000	0.000	0.6176	0.5854	0.240	0.254	0.107	0.706	0.445	0.145	0.8125
10	0.5430	0.269	0.197	0.175	0.906	0.420	0.478	0.6250	0.7072	0.160	0.204	0.006	0.273	0.219	0.006	0.8125
11	0.5313	0.000	0.000	0.000	0.000	0.000	0.000	0.7250	0.5593	0.237	0.229	0.136	0.776	0.460	0.324	0.8125
12	0.5271	0.000	0.000	0.000	0.000	0.000	0.000	0.6471	0.5592	0.237	0.240	0.117	0.745	0.386	0.236	0.8125
13	0.5352	0.000	0.000	0.000	0.000	0.000	0.000	0.6974	0.5637	0.214	0.163	0.153	0.823	0.263	0.352	0.8125
14	0.5287	0.000	0.000	0.000	0.000	0.000	0.000	0.6528	0.6124	0.180	0.160	0.140	0.875	0.313	0.457	0.8125
15	0.5216	0.277	0.177	0.202	0.944	0.319	0.585	0.7000	0.6282	0.202	0.106	0.150	0.859	0.160	0.695	0.8125
16	0.5165	0.000	0.000	0.000	0.000	0.000	0.000	0.7000	0.5645	0.240	0.161	0.108	0.653	0.256	0.284	0.8125
17	0.5150	0.000	0.000	0.000	0.000	0.000	0.000	0.7167	0.5676	0.231	0.228	0.111	0.687	0.369	0.288	0.8125
18	0.5095	0.000	0.000	0.000	0.000	0.000	0.000	0.6562	0.5705	0.204	0.135	0.149	0.810	0.149	0.662	0.8125
19	0.5096	0.000	0.000	0.000	0.000	0.000	0.000	0.6250	0.5881	0.216	0.183	0.131	0.709	0.246	0.359	0.8125
20	0.5091	0.282	0.127	0.214	0.939	0.198	0.717	0.6324	0.5463	0.206	0.181	0.150	0.834	0.293	0.515	0.8125
21	0.5106	0.000	0.000	0.000	0.000	0.000	0.000	0.7143	0.9526	0.127	0.064	0.105	0.936	0.190	0.814	0.8125
22	0.5188	0.000	0.000	0.000	0.000	0.000	0.000	0.6562	0.5591	0.199	0.141	0.118	0.756	0.193	0.457	0.8125
23	0.5084	0.000	0.000	0.000	0.000	0.000	0.000	0.7024	0.6101	0.204	0.168	0.113	0.607	0.227	0.293	0.8125
24	0.5036	0.000	0.000	0.000	0.000	0.000	0.000	0.6842	0.5910	0.218	0.212	0.099	0.584	0.305	0.204	0.8125
25	0.4923	0.304	0.224	0.162	0.952	0.434	0.411	0.6875	0.5839	0.213	0.164	0.126	0.679	0.261	0.290	0.8125
26	0.4898	0.000	0.000	0.000	0.000	0.000	0.000	0.7222	0.6284	0.227	0.193	0.124	0.572	0.225	0.324	0.8125
27	0.4949	0.000	0.000	0.000	0.000	0.000	0.000	0.6806	0.5846	0.233	0.317	0.071	0.569	0.568	0.069	0.8125
28	0.4919	0.000	0.000	0.000	0.000	0.000	0.000	0.6458	0.5725	0.234	0.215	0.137	0.699	0.331	0.293	0.8125
29	0.4855	0.000	0.000	0.000	0.000	0.000	0.000	0.6818	0.6350	0.213	0.210	0.112	0.516	0.260	0.251	0.8125
30	0.4859	0.318	0.200	0.208	0.943	0.303	0.664	0.7125	0.6217	0.232	0.146	0.141	0.612	0.170	0.375	0.8125
31	0.4925	0.000	0.000	0.000	0.000	0.000	0.000	0.6579	0.6323	0.226	0.195	0.133	0.579	0.257	0.298	0.8125
32	0.4862	0.000	0.000	0.000	0.000	0.000	0.000	0.6176	0.6011	0.216	0.231	0.116	0.513	0.324	0.236	0.8125
33	0.4909	0.000	0.000	0.000	0.000	0.000	0.000	0.7500	0.5861	0.228	0.191	0.138	0.722	0.247	0.408	0.8125
34	0.4934	0.000	0.000	0.000	0.000	0.000	0.000	0.7000	0.5775	0.215	0.290	0.094	0.703	0.568	0.130	0.8125
35	0.4898	0.314	0.330	0.189	0.947	0.624	0.476	0.7321	0.5964	0.198	0.183	0.120	0.602	0.236	0.392	0.8125
36	0.4815	0.000	0.000	0.000	0.000	0.000	0.000	0.6625	0.6113	0.198	0.187	0.126	0.675	0.282	0.372	0.8125
37	0.4833	0.000	0.000	0.000	0.000	0.000	0.000	0.6176	0.6272	0.192	0.211	0.114	0.568	0.272	0.318	0.8125
38	0.4782	0.000	0.000	0.000	0.000	0.000	0.000	0.7143	0.6855	0.195	0.126	0.116	0.555	0.110	0.428	0.8125
39	0.4680	0.000	0.000	0.000	0.000	0.000	0.000	0.7059	0.6796	0.191	0.216	0.121	0.531	0.341	0.285	0.8125
40	0.4719	0.318	0.336	0.198	0.944	0.605	0.623	0.7059	0.6214	0.212	0.167	0.135	0.566	0.194	0.423	0.8125
41	0.4649	0.000	0.000	0.000	0.000	0.000	0.000	0.7083	0.6221	0.209	0.075	0.140	0.635	0.113	0.567	0.8125
42	0.4611	0.000	0.000	0.000	0.000	0.000	0.000	0.7083	0.6440	0.178	0.230	0.106	0.645	0.594	0.206	0.8125
43	0.4779	0.000	0.000	0.000	0.000	0.000	0.000	0.6167	0.5776	0.217	0.245	0.154	0.792	0.438	0.462	0.8125
44	0.4811	0.000	0.000	0.000	0.000	0.000	0.000	0.6500	0.5904	0.235	0.198	0.175	0.663	0.247	0.524	0.8125
