Program ID 12914

Program command: 
 ['/home/hxie1/Projects/OvarianCancer/TrainSegV.py', '/home/hxie1/temp_netParameters/OvarianCancer/Label0_1', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainImages', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainLabels', '2D', '0,1']

Major program changes: ConvResidual use BatchNorm-reLU-Conv structure; 
                       ConsDense also use BatchNorm-reLU-Conv structure.
                       Add ConvSegDecreaseChannels
                       and each block has 5 layers, 
                       Residual connect to each Conv, 
                       skip at least 2 layers.
                       output layer use conv with 3*3 fiter instead of 1*1 filter. 
                       use boundary loss with weight 0 at beginning, and pretrain CE loss. 
                       special convInput Module
                       convOutput moudel uses 1*1 conv to tranparent gradident 
                       ConvOutput use residual module.
                       Use Dense Net in the Building Block
                       add ConvBlock to wrapp the ConvResidual and ConvDense
                       first layer filter = 64, reducing from 128 of previous experiment
                       the nLayers in block is 4, increase from 2 of previous experiment
                       
            

Program starting Time: 2019-05-09 13:51:25.613816
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/Label0_1

Infor: program test labels: [0, 1]
Infor: program suppressed labels: [2, 3]
Infor: program test labels: [0, 1]
Infor: program suppressed labels: [2, 3]
Building the Segmented Slice Tuple list, which may need 8 mins, please waiting......
Directory of /home/hxie1/data/OvarianCancerCT/Extract_uniform/trainLabels has 164 segmented slices for remained labels [0, 1].
Building the Segmented Slice Tuple list, which may need 8 mins, please waiting......
Directory of /home/hxie1/data/OvarianCancerCT/Extract_uniform/testLabels has 31 segmented slices for remained labels [0, 1].
Info: program uses 2D input.
TrainData Input:  batchSize=8, depth=1, height=281, width=281, NumClassfication=2

TestData Input:  batchSize=8, depth=1, height=281, width=281, NumClassfication=2

Network trains from scratch.
Network has total 51554754 parameters.
Info: network dropout rate = 0.3
Infor: Cross Entropy Weight: [1.0260619741432382, 39.37007874015748]

====================Net Architecture===========================
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 281, 281]             640
       BatchNorm2d-2         [-1, 64, 281, 281]             128
              ReLU-3         [-1, 64, 281, 281]               0
            Conv2d-4         [-1, 64, 281, 281]           4,160
       BatchNorm2d-5         [-1, 64, 281, 281]             128
              ReLU-6         [-1, 64, 281, 281]               0
            Conv2d-7         [-1, 16, 281, 281]           9,232
       BatchNorm2d-8         [-1, 80, 281, 281]             160
              ReLU-9         [-1, 80, 281, 281]               0
           Conv2d-10         [-1, 64, 281, 281]           5,184
      BatchNorm2d-11         [-1, 64, 281, 281]             128
             ReLU-12         [-1, 64, 281, 281]               0
           Conv2d-13         [-1, 16, 281, 281]           9,232
      BatchNorm2d-14         [-1, 96, 281, 281]             192
             ReLU-15         [-1, 96, 281, 281]               0
           Conv2d-16         [-1, 64, 281, 281]           6,208
      BatchNorm2d-17         [-1, 64, 281, 281]             128
             ReLU-18         [-1, 64, 281, 281]               0
           Conv2d-19         [-1, 16, 281, 281]           9,232
      BatchNorm2d-20        [-1, 112, 281, 281]             224
             ReLU-21        [-1, 112, 281, 281]               0
           Conv2d-22         [-1, 64, 281, 281]           7,232
      BatchNorm2d-23         [-1, 64, 281, 281]             128
             ReLU-24         [-1, 64, 281, 281]               0
           Conv2d-25         [-1, 16, 281, 281]           9,232
      BatchNorm2d-26        [-1, 128, 281, 281]             256
             ReLU-27        [-1, 128, 281, 281]               0
           Conv2d-28         [-1, 64, 281, 281]           8,256
        ConvDense-29         [-1, 64, 281, 281]               0
        ConvBlock-30         [-1, 64, 281, 281]               0
        ConvInput-31         [-1, 64, 281, 281]               0
      BatchNorm2d-32         [-1, 64, 281, 281]             128
           Conv2d-33         [-1, 64, 139, 139]         102,464
      BatchNorm2d-34         [-1, 64, 139, 139]             128
             ReLU-35         [-1, 64, 139, 139]               0
           Conv2d-36         [-1, 64, 139, 139]           4,160
      BatchNorm2d-37         [-1, 64, 139, 139]             128
             ReLU-38         [-1, 64, 139, 139]               0
           Conv2d-39         [-1, 16, 139, 139]           9,232
      BatchNorm2d-40         [-1, 80, 139, 139]             160
             ReLU-41         [-1, 80, 139, 139]               0
           Conv2d-42         [-1, 64, 139, 139]           5,184
      BatchNorm2d-43         [-1, 64, 139, 139]             128
             ReLU-44         [-1, 64, 139, 139]               0
           Conv2d-45         [-1, 16, 139, 139]           9,232
      BatchNorm2d-46         [-1, 96, 139, 139]             192
             ReLU-47         [-1, 96, 139, 139]               0
           Conv2d-48         [-1, 64, 139, 139]           6,208
      BatchNorm2d-49         [-1, 64, 139, 139]             128
             ReLU-50         [-1, 64, 139, 139]               0
           Conv2d-51         [-1, 16, 139, 139]           9,232
      BatchNorm2d-52        [-1, 112, 139, 139]             224
             ReLU-53        [-1, 112, 139, 139]               0
           Conv2d-54         [-1, 64, 139, 139]           7,232
      BatchNorm2d-55         [-1, 64, 139, 139]             128
             ReLU-56         [-1, 64, 139, 139]               0
           Conv2d-57         [-1, 16, 139, 139]           9,232
      BatchNorm2d-58        [-1, 128, 139, 139]             256
             ReLU-59        [-1, 128, 139, 139]               0
           Conv2d-60         [-1, 64, 139, 139]           8,256
        ConvDense-61         [-1, 64, 139, 139]               0
        ConvBlock-62         [-1, 64, 139, 139]               0
         Down2dBB-63         [-1, 64, 139, 139]               0
        Dropout2d-64         [-1, 64, 139, 139]               0
      BatchNorm2d-65         [-1, 64, 139, 139]             128
           Conv2d-66          [-1, 128, 69, 69]          73,856
      BatchNorm2d-67          [-1, 128, 69, 69]             256
             ReLU-68          [-1, 128, 69, 69]               0
           Conv2d-69          [-1, 128, 69, 69]          16,512
      BatchNorm2d-70          [-1, 128, 69, 69]             256
             ReLU-71          [-1, 128, 69, 69]               0
           Conv2d-72           [-1, 32, 69, 69]          36,896
      BatchNorm2d-73          [-1, 160, 69, 69]             320
             ReLU-74          [-1, 160, 69, 69]               0
           Conv2d-75          [-1, 128, 69, 69]          20,608
      BatchNorm2d-76          [-1, 128, 69, 69]             256
             ReLU-77          [-1, 128, 69, 69]               0
           Conv2d-78           [-1, 32, 69, 69]          36,896
      BatchNorm2d-79          [-1, 192, 69, 69]             384
             ReLU-80          [-1, 192, 69, 69]               0
           Conv2d-81          [-1, 128, 69, 69]          24,704
      BatchNorm2d-82          [-1, 128, 69, 69]             256
             ReLU-83          [-1, 128, 69, 69]               0
           Conv2d-84           [-1, 32, 69, 69]          36,896
      BatchNorm2d-85          [-1, 224, 69, 69]             448
             ReLU-86          [-1, 224, 69, 69]               0
           Conv2d-87          [-1, 128, 69, 69]          28,800
      BatchNorm2d-88          [-1, 128, 69, 69]             256
             ReLU-89          [-1, 128, 69, 69]               0
           Conv2d-90           [-1, 32, 69, 69]          36,896
      BatchNorm2d-91          [-1, 256, 69, 69]             512
             ReLU-92          [-1, 256, 69, 69]               0
           Conv2d-93          [-1, 128, 69, 69]          32,896
        ConvDense-94          [-1, 128, 69, 69]               0
        ConvBlock-95          [-1, 128, 69, 69]               0
         Down2dBB-96          [-1, 128, 69, 69]               0
        Dropout2d-97          [-1, 128, 69, 69]               0
      BatchNorm2d-98          [-1, 128, 69, 69]             256
           Conv2d-99          [-1, 256, 33, 33]         819,456
     BatchNorm2d-100          [-1, 256, 33, 33]             512
            ReLU-101          [-1, 256, 33, 33]               0
          Conv2d-102          [-1, 256, 33, 33]          65,792
     BatchNorm2d-103          [-1, 256, 33, 33]             512
            ReLU-104          [-1, 256, 33, 33]               0
          Conv2d-105           [-1, 64, 33, 33]         147,520
     BatchNorm2d-106          [-1, 320, 33, 33]             640
            ReLU-107          [-1, 320, 33, 33]               0
          Conv2d-108          [-1, 256, 33, 33]          82,176
     BatchNorm2d-109          [-1, 256, 33, 33]             512
            ReLU-110          [-1, 256, 33, 33]               0
          Conv2d-111           [-1, 64, 33, 33]         147,520
     BatchNorm2d-112          [-1, 384, 33, 33]             768
            ReLU-113          [-1, 384, 33, 33]               0
          Conv2d-114          [-1, 256, 33, 33]          98,560
     BatchNorm2d-115          [-1, 256, 33, 33]             512
            ReLU-116          [-1, 256, 33, 33]               0
          Conv2d-117           [-1, 64, 33, 33]         147,520
     BatchNorm2d-118          [-1, 448, 33, 33]             896
            ReLU-119          [-1, 448, 33, 33]               0
          Conv2d-120          [-1, 256, 33, 33]         114,944
     BatchNorm2d-121          [-1, 256, 33, 33]             512
            ReLU-122          [-1, 256, 33, 33]               0
          Conv2d-123           [-1, 64, 33, 33]         147,520
     BatchNorm2d-124          [-1, 512, 33, 33]           1,024
            ReLU-125          [-1, 512, 33, 33]               0
          Conv2d-126          [-1, 256, 33, 33]         131,328
       ConvDense-127          [-1, 256, 33, 33]               0
       ConvBlock-128          [-1, 256, 33, 33]               0
        Down2dBB-129          [-1, 256, 33, 33]               0
       Dropout2d-130          [-1, 256, 33, 33]               0
     BatchNorm2d-131          [-1, 256, 33, 33]             512
          Conv2d-132          [-1, 512, 15, 15]       3,277,312
     BatchNorm2d-133          [-1, 512, 15, 15]           1,024
            ReLU-134          [-1, 512, 15, 15]               0
          Conv2d-135          [-1, 512, 15, 15]         262,656
     BatchNorm2d-136          [-1, 512, 15, 15]           1,024
            ReLU-137          [-1, 512, 15, 15]               0
          Conv2d-138          [-1, 128, 15, 15]         589,952
     BatchNorm2d-139          [-1, 640, 15, 15]           1,280
            ReLU-140          [-1, 640, 15, 15]               0
          Conv2d-141          [-1, 512, 15, 15]         328,192
     BatchNorm2d-142          [-1, 512, 15, 15]           1,024
            ReLU-143          [-1, 512, 15, 15]               0
          Conv2d-144          [-1, 128, 15, 15]         589,952
     BatchNorm2d-145          [-1, 768, 15, 15]           1,536
            ReLU-146          [-1, 768, 15, 15]               0
          Conv2d-147          [-1, 512, 15, 15]         393,728
     BatchNorm2d-148          [-1, 512, 15, 15]           1,024
            ReLU-149          [-1, 512, 15, 15]               0
          Conv2d-150          [-1, 128, 15, 15]         589,952
     BatchNorm2d-151          [-1, 896, 15, 15]           1,792
            ReLU-152          [-1, 896, 15, 15]               0
          Conv2d-153          [-1, 512, 15, 15]         459,264
     BatchNorm2d-154          [-1, 512, 15, 15]           1,024
            ReLU-155          [-1, 512, 15, 15]               0
          Conv2d-156          [-1, 128, 15, 15]         589,952
     BatchNorm2d-157         [-1, 1024, 15, 15]           2,048
            ReLU-158         [-1, 1024, 15, 15]               0
          Conv2d-159          [-1, 512, 15, 15]         524,800
       ConvDense-160          [-1, 512, 15, 15]               0
       ConvBlock-161          [-1, 512, 15, 15]               0
        Down2dBB-162          [-1, 512, 15, 15]               0
       Dropout2d-163          [-1, 512, 15, 15]               0
     BatchNorm2d-164          [-1, 512, 15, 15]           1,024
          Conv2d-165           [-1, 1024, 7, 7]       4,719,616
     BatchNorm2d-166           [-1, 1024, 7, 7]           2,048
            ReLU-167           [-1, 1024, 7, 7]               0
          Conv2d-168           [-1, 1024, 7, 7]       1,049,600
     BatchNorm2d-169           [-1, 1024, 7, 7]           2,048
            ReLU-170           [-1, 1024, 7, 7]               0
          Conv2d-171            [-1, 256, 7, 7]       2,359,552
     BatchNorm2d-172           [-1, 1280, 7, 7]           2,560
            ReLU-173           [-1, 1280, 7, 7]               0
          Conv2d-174           [-1, 1024, 7, 7]       1,311,744
     BatchNorm2d-175           [-1, 1024, 7, 7]           2,048
            ReLU-176           [-1, 1024, 7, 7]               0
          Conv2d-177            [-1, 256, 7, 7]       2,359,552
     BatchNorm2d-178           [-1, 1536, 7, 7]           3,072
            ReLU-179           [-1, 1536, 7, 7]               0
          Conv2d-180           [-1, 1024, 7, 7]       1,573,888
     BatchNorm2d-181           [-1, 1024, 7, 7]           2,048
            ReLU-182           [-1, 1024, 7, 7]               0
          Conv2d-183            [-1, 256, 7, 7]       2,359,552
     BatchNorm2d-184           [-1, 1792, 7, 7]           3,584
            ReLU-185           [-1, 1792, 7, 7]               0
          Conv2d-186           [-1, 1024, 7, 7]       1,836,032
     BatchNorm2d-187           [-1, 1024, 7, 7]           2,048
            ReLU-188           [-1, 1024, 7, 7]               0
          Conv2d-189            [-1, 256, 7, 7]       2,359,552
     BatchNorm2d-190           [-1, 2048, 7, 7]           4,096
            ReLU-191           [-1, 2048, 7, 7]               0
          Conv2d-192           [-1, 1024, 7, 7]       2,098,176
       ConvDense-193           [-1, 1024, 7, 7]               0
       ConvBlock-194           [-1, 1024, 7, 7]               0
        Down2dBB-195           [-1, 1024, 7, 7]               0
       Dropout2d-196           [-1, 1024, 7, 7]               0
     BatchNorm2d-197           [-1, 1024, 7, 7]           2,048
 ConvTranspose2d-198          [-1, 512, 15, 15]       4,719,104
     BatchNorm2d-199          [-1, 512, 15, 15]           1,024
            ReLU-200          [-1, 512, 15, 15]               0
          Conv2d-201          [-1, 512, 15, 15]         262,656
     BatchNorm2d-202          [-1, 512, 15, 15]           1,024
            ReLU-203          [-1, 512, 15, 15]               0
          Conv2d-204          [-1, 128, 15, 15]         589,952
     BatchNorm2d-205          [-1, 640, 15, 15]           1,280
            ReLU-206          [-1, 640, 15, 15]               0
          Conv2d-207          [-1, 512, 15, 15]         328,192
     BatchNorm2d-208          [-1, 512, 15, 15]           1,024
            ReLU-209          [-1, 512, 15, 15]               0
          Conv2d-210          [-1, 128, 15, 15]         589,952
     BatchNorm2d-211          [-1, 768, 15, 15]           1,536
            ReLU-212          [-1, 768, 15, 15]               0
          Conv2d-213          [-1, 512, 15, 15]         393,728
     BatchNorm2d-214          [-1, 512, 15, 15]           1,024
            ReLU-215          [-1, 512, 15, 15]               0
          Conv2d-216          [-1, 128, 15, 15]         589,952
     BatchNorm2d-217          [-1, 896, 15, 15]           1,792
            ReLU-218          [-1, 896, 15, 15]               0
          Conv2d-219          [-1, 512, 15, 15]         459,264
     BatchNorm2d-220          [-1, 512, 15, 15]           1,024
            ReLU-221          [-1, 512, 15, 15]               0
          Conv2d-222          [-1, 128, 15, 15]         589,952
     BatchNorm2d-223         [-1, 1024, 15, 15]           2,048
            ReLU-224         [-1, 1024, 15, 15]               0
          Conv2d-225          [-1, 512, 15, 15]         524,800
       ConvDense-226          [-1, 512, 15, 15]               0
       ConvBlock-227          [-1, 512, 15, 15]               0
          Up2dBB-228          [-1, 512, 15, 15]               0
       Dropout2d-229          [-1, 512, 15, 15]               0
     BatchNorm2d-230         [-1, 1024, 15, 15]           2,048
 ConvTranspose2d-231          [-1, 256, 33, 33]       6,553,856
     BatchNorm2d-232          [-1, 256, 33, 33]             512
            ReLU-233          [-1, 256, 33, 33]               0
          Conv2d-234          [-1, 256, 33, 33]          65,792
     BatchNorm2d-235          [-1, 256, 33, 33]             512
            ReLU-236          [-1, 256, 33, 33]               0
          Conv2d-237           [-1, 64, 33, 33]         147,520
     BatchNorm2d-238          [-1, 320, 33, 33]             640
            ReLU-239          [-1, 320, 33, 33]               0
          Conv2d-240          [-1, 256, 33, 33]          82,176
     BatchNorm2d-241          [-1, 256, 33, 33]             512
            ReLU-242          [-1, 256, 33, 33]               0
          Conv2d-243           [-1, 64, 33, 33]         147,520
     BatchNorm2d-244          [-1, 384, 33, 33]             768
            ReLU-245          [-1, 384, 33, 33]               0
          Conv2d-246          [-1, 256, 33, 33]          98,560
     BatchNorm2d-247          [-1, 256, 33, 33]             512
            ReLU-248          [-1, 256, 33, 33]               0
          Conv2d-249           [-1, 64, 33, 33]         147,520
     BatchNorm2d-250          [-1, 448, 33, 33]             896
            ReLU-251          [-1, 448, 33, 33]               0
          Conv2d-252          [-1, 256, 33, 33]         114,944
     BatchNorm2d-253          [-1, 256, 33, 33]             512
            ReLU-254          [-1, 256, 33, 33]               0
          Conv2d-255           [-1, 64, 33, 33]         147,520
     BatchNorm2d-256          [-1, 512, 33, 33]           1,024
            ReLU-257          [-1, 512, 33, 33]               0
          Conv2d-258          [-1, 256, 33, 33]         131,328
       ConvDense-259          [-1, 256, 33, 33]               0
       ConvBlock-260          [-1, 256, 33, 33]               0
          Up2dBB-261          [-1, 256, 33, 33]               0
       Dropout2d-262          [-1, 256, 33, 33]               0
     BatchNorm2d-263          [-1, 512, 33, 33]           1,024
 ConvTranspose2d-264          [-1, 128, 69, 69]       1,638,528
     BatchNorm2d-265          [-1, 128, 69, 69]             256
            ReLU-266          [-1, 128, 69, 69]               0
          Conv2d-267          [-1, 128, 69, 69]          16,512
     BatchNorm2d-268          [-1, 128, 69, 69]             256
            ReLU-269          [-1, 128, 69, 69]               0
          Conv2d-270           [-1, 32, 69, 69]          36,896
     BatchNorm2d-271          [-1, 160, 69, 69]             320
            ReLU-272          [-1, 160, 69, 69]               0
          Conv2d-273          [-1, 128, 69, 69]          20,608
     BatchNorm2d-274          [-1, 128, 69, 69]             256
            ReLU-275          [-1, 128, 69, 69]               0
          Conv2d-276           [-1, 32, 69, 69]          36,896
     BatchNorm2d-277          [-1, 192, 69, 69]             384
            ReLU-278          [-1, 192, 69, 69]               0
          Conv2d-279          [-1, 128, 69, 69]          24,704
     BatchNorm2d-280          [-1, 128, 69, 69]             256
            ReLU-281          [-1, 128, 69, 69]               0
          Conv2d-282           [-1, 32, 69, 69]          36,896
     BatchNorm2d-283          [-1, 224, 69, 69]             448
            ReLU-284          [-1, 224, 69, 69]               0
          Conv2d-285          [-1, 128, 69, 69]          28,800
     BatchNorm2d-286          [-1, 128, 69, 69]             256
            ReLU-287          [-1, 128, 69, 69]               0
          Conv2d-288           [-1, 32, 69, 69]          36,896
     BatchNorm2d-289          [-1, 256, 69, 69]             512
            ReLU-290          [-1, 256, 69, 69]               0
          Conv2d-291          [-1, 128, 69, 69]          32,896
       ConvDense-292          [-1, 128, 69, 69]               0
       ConvBlock-293          [-1, 128, 69, 69]               0
          Up2dBB-294          [-1, 128, 69, 69]               0
       Dropout2d-295          [-1, 128, 69, 69]               0
     BatchNorm2d-296          [-1, 256, 69, 69]             512
 ConvTranspose2d-297         [-1, 64, 139, 139]         147,520
     BatchNorm2d-298         [-1, 64, 139, 139]             128
            ReLU-299         [-1, 64, 139, 139]               0
          Conv2d-300         [-1, 64, 139, 139]           4,160
     BatchNorm2d-301         [-1, 64, 139, 139]             128
            ReLU-302         [-1, 64, 139, 139]               0
          Conv2d-303         [-1, 16, 139, 139]           9,232
     BatchNorm2d-304         [-1, 80, 139, 139]             160
            ReLU-305         [-1, 80, 139, 139]               0
          Conv2d-306         [-1, 64, 139, 139]           5,184
     BatchNorm2d-307         [-1, 64, 139, 139]             128
            ReLU-308         [-1, 64, 139, 139]               0
          Conv2d-309         [-1, 16, 139, 139]           9,232
     BatchNorm2d-310         [-1, 96, 139, 139]             192
            ReLU-311         [-1, 96, 139, 139]               0
          Conv2d-312         [-1, 64, 139, 139]           6,208
     BatchNorm2d-313         [-1, 64, 139, 139]             128
            ReLU-314         [-1, 64, 139, 139]               0
          Conv2d-315         [-1, 16, 139, 139]           9,232
     BatchNorm2d-316        [-1, 112, 139, 139]             224
            ReLU-317        [-1, 112, 139, 139]               0
          Conv2d-318         [-1, 64, 139, 139]           7,232
     BatchNorm2d-319         [-1, 64, 139, 139]             128
            ReLU-320         [-1, 64, 139, 139]               0
          Conv2d-321         [-1, 16, 139, 139]           9,232
     BatchNorm2d-322        [-1, 128, 139, 139]             256
            ReLU-323        [-1, 128, 139, 139]               0
          Conv2d-324         [-1, 64, 139, 139]           8,256
       ConvDense-325         [-1, 64, 139, 139]               0
       ConvBlock-326         [-1, 64, 139, 139]               0
          Up2dBB-327         [-1, 64, 139, 139]               0
       Dropout2d-328         [-1, 64, 139, 139]               0
     BatchNorm2d-329        [-1, 128, 139, 139]             256
 ConvTranspose2d-330         [-1, 64, 281, 281]         204,864
     BatchNorm2d-331         [-1, 64, 281, 281]             128
            ReLU-332         [-1, 64, 281, 281]               0
          Conv2d-333         [-1, 64, 281, 281]           4,160
     BatchNorm2d-334         [-1, 64, 281, 281]             128
            ReLU-335         [-1, 64, 281, 281]               0
          Conv2d-336         [-1, 16, 281, 281]           9,232
     BatchNorm2d-337         [-1, 80, 281, 281]             160
            ReLU-338         [-1, 80, 281, 281]               0
          Conv2d-339         [-1, 64, 281, 281]           5,184
     BatchNorm2d-340         [-1, 64, 281, 281]             128
            ReLU-341         [-1, 64, 281, 281]               0
          Conv2d-342         [-1, 16, 281, 281]           9,232
     BatchNorm2d-343         [-1, 96, 281, 281]             192
            ReLU-344         [-1, 96, 281, 281]               0
          Conv2d-345         [-1, 64, 281, 281]           6,208
     BatchNorm2d-346         [-1, 64, 281, 281]             128
            ReLU-347         [-1, 64, 281, 281]               0
          Conv2d-348         [-1, 16, 281, 281]           9,232
     BatchNorm2d-349        [-1, 112, 281, 281]             224
            ReLU-350        [-1, 112, 281, 281]               0
          Conv2d-351         [-1, 64, 281, 281]           7,232
     BatchNorm2d-352         [-1, 64, 281, 281]             128
            ReLU-353         [-1, 64, 281, 281]               0
          Conv2d-354         [-1, 16, 281, 281]           9,232
     BatchNorm2d-355        [-1, 128, 281, 281]             256
            ReLU-356        [-1, 128, 281, 281]               0
          Conv2d-357         [-1, 64, 281, 281]           8,256
       ConvDense-358         [-1, 64, 281, 281]               0
       ConvBlock-359         [-1, 64, 281, 281]               0
          Up2dBB-360         [-1, 64, 281, 281]               0
       Dropout2d-361         [-1, 64, 281, 281]               0
     BatchNorm2d-362        [-1, 128, 281, 281]             256
            ReLU-363        [-1, 128, 281, 281]               0
          Conv2d-364        [-1, 128, 281, 281]          16,512
     BatchNorm2d-365        [-1, 128, 281, 281]             256
            ReLU-366        [-1, 128, 281, 281]               0
          Conv2d-367         [-1, 32, 281, 281]          36,896
     BatchNorm2d-368        [-1, 160, 281, 281]             320
            ReLU-369        [-1, 160, 281, 281]               0
          Conv2d-370        [-1, 128, 281, 281]          20,608
     BatchNorm2d-371        [-1, 128, 281, 281]             256
            ReLU-372        [-1, 128, 281, 281]               0
          Conv2d-373         [-1, 32, 281, 281]          36,896
     BatchNorm2d-374        [-1, 192, 281, 281]             384
            ReLU-375        [-1, 192, 281, 281]               0
          Conv2d-376        [-1, 128, 281, 281]          24,704
     BatchNorm2d-377        [-1, 128, 281, 281]             256
            ReLU-378        [-1, 128, 281, 281]               0
          Conv2d-379         [-1, 32, 281, 281]          36,896
     BatchNorm2d-380        [-1, 224, 281, 281]             448
            ReLU-381        [-1, 224, 281, 281]               0
          Conv2d-382        [-1, 128, 281, 281]          28,800
     BatchNorm2d-383        [-1, 128, 281, 281]             256
            ReLU-384        [-1, 128, 281, 281]               0
          Conv2d-385         [-1, 32, 281, 281]          36,896
     BatchNorm2d-386        [-1, 256, 281, 281]             512
            ReLU-387        [-1, 256, 281, 281]               0
          Conv2d-388        [-1, 128, 281, 281]          32,896
       ConvDense-389        [-1, 128, 281, 281]               0
       ConvBlock-390        [-1, 128, 281, 281]               0
     BatchNorm2d-391        [-1, 128, 281, 281]             256
          Conv2d-392          [-1, 2, 281, 281]             258
      ConvOutput-393          [-1, 2, 281, 281]               0
================================================================
Total params: 51,554,754
Trainable params: 51,554,754
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.30
Forward/backward pass size (MB): 6318.44
Params size (MB): 196.67
Estimated Total Size (MB): 6515.40
----------------------------------------------------------------
===================End of Net Architecture =====================

Info: program will use 4 GPUs.
Loss Functions List: FocalCELoss with weight of 1; 	BoundaryLoss with weight of 0; 
Hints: Test Dice_0 is the dice coeff for all non-zero labels
Hints: Test Dice_1 is for primary cancer(green), test Dice_2 is for metastasis(yellow), and test Dice_3 is for invaded lymph node(brown).
Hints: Test TPR_0 is the TPR for all non-zero labels
Hints: Test TPR_1 is for primary cancer(green), TPR_2 is for metastasis(yellow), and TPR_3 is for invaded lymph node(brown).

Epoch 	 TrainingLoss 	 TestLoss 	Dice_0	Dice_1	TPR_0	TPR_1
0 	 0.1040 	 0.1178 	0.171	0.171	1.000	1.000
1 	 0.0821 	 0.1001 	0.241	0.241	1.000	1.000
2 	 0.0683 	 0.0700 	0.408	0.408	0.960	0.960
3 	 0.0627 	 0.1967 	0.510	0.510	0.781	0.781
4 	 0.0595 	 0.0533 	0.464	0.464	0.981	0.981
5 	 0.0584 	 0.0599 	0.512	0.512	0.944	0.944
6 	 0.0513 	 0.0494 	0.536	0.536	0.963	0.963
7 	 0.0514 	 0.0470 	0.459	0.459	0.977	0.977
8 	 0.0476 	 0.0749 	0.561	0.561	0.934	0.934
9 	 0.0464 	 0.0455 	0.531	0.531	0.982	0.982
10 	 0.0451 	 0.0455 	0.547	0.547	0.975	0.975
11 	 0.0440 	 0.0717 	0.521	0.521	0.927	0.927
12 	 0.0443 	 0.0511 	0.582	0.582	0.946	0.946
13 	 0.0415 	 0.0512 	0.615	0.615	0.933	0.933
14 	 0.0407 	 0.0347 	0.588	0.588	0.974	0.974
15 	 0.0392 	 0.0694 	0.570	0.570	0.868	0.868
16 	 0.0400 	 0.0402 	0.566	0.566	0.979	0.979
17 	 0.0461 	 0.0456 	0.479	0.479	0.988	0.988
18 	 0.0492 	 0.0449 	0.498	0.498	0.979	0.979
19 	 0.0477 	 0.0544 	0.481	0.481	0.981	0.981
20 	 0.0442 	 0.0459 	0.568	0.568	0.973	0.973
21 	 0.0443 	 0.0427 	0.594	0.594	0.964	0.964
22 	 0.0410 	 0.0573 	0.517	0.517	0.969	0.969
23 	 0.0501 	 0.2783 	0.535	0.535	0.903	0.903
24 	 0.0471 	 0.0385 	0.547	0.547	0.976	0.976
25 	 0.0394 	 0.0360 	0.548	0.548	0.983	0.983
26 	 0.0371 	 0.0419 	0.592	0.592	0.961	0.961
27 	 0.0392 	 0.0774 	0.580	0.580	0.926	0.926
28 	 0.0386 	 0.0657 	0.577	0.577	0.933	0.933
29 	 0.0410 	 0.0600 	0.594	0.594	0.930	0.930
30 	 0.0412 	 0.0329 	0.618	0.618	0.974	0.974
31 	 0.0422 	 0.0969 	0.485	0.485	0.920	0.920
32 	 0.0494 	 0.0463 	0.523	0.523	0.963	0.963
33 	 0.0421 	 0.0463 	0.560	0.560	0.964	0.964
34 	 0.0409 	 0.0384 	0.572	0.572	0.980	0.980
35 	 0.0353 	 0.0425 	0.563	0.563	0.946	0.946
36 	 0.0358 	 0.0382 	0.556	0.556	0.977	0.977
37 	 0.0333 	 0.0498 	0.599	0.599	0.942	0.942
38 	 0.0353 	 0.0551 	0.524	0.524	0.954	0.954
39 	 0.0356 	 0.0407 	0.530	0.530	0.969	0.969
40 	 0.0355 	 0.0473 	0.598	0.598	0.966	0.966
41 	 0.0369 	 0.0453 	0.622	0.622	0.958	0.958
42 	 0.0335 	 0.0398 	0.591	0.591	0.968	0.968
43 	 0.0366 	 0.0934 	0.635	0.635	0.908	0.908
44 	 0.0373 	 0.0371 	0.561	0.561	0.968	0.968
45 	 0.0317 	 0.0451 	0.588	0.588	0.959	0.959
46 	 0.0329 	 0.0466 	0.548	0.548	0.964	0.964
47 	 0.0343 	 0.0345 	0.606	0.606	0.981	0.981
48 	 0.0380 	 0.0441 	0.629	0.629	0.957	0.957
49 	 0.0344 	 0.0355 	0.600	0.600	0.968	0.968
50 	 0.0306 	 0.0310 	0.589	0.589	0.986	0.986
51 	 0.0286 	 0.0415 	0.611	0.611	0.968	0.968
52 	 0.0269 	 0.0433 	0.590	0.590	0.966	0.966
53 	 0.0276 	 0.0380 	0.600	0.600	0.958	0.958
54 	 0.0288 	 0.0419 	0.627	0.627	0.957	0.957
55 	 0.0334 	 0.0330 	0.637	0.637	0.961	0.961
56 	 0.0319 	 0.0577 	0.661	0.661	0.945	0.945
57 	 0.0307 	 0.0837 	0.689	0.689	0.909	0.909
58 	 0.0366 	 0.0426 	0.635	0.635	0.952	0.952
59 	 0.0328 	 0.0366 	0.624	0.624	0.975	0.975
60 	 0.0299 	 0.0482 	0.656	0.656	0.926	0.926
61 	 0.0321 	 0.0346 	0.592	0.592	0.979	0.979
62 	 0.0295 	 0.0360 	0.580	0.580	0.977	0.977
63 	 0.0326 	 0.0435 	0.563	0.563	0.966	0.966
64 	 0.0328 	 0.0525 	0.585	0.585	0.952	0.952
65 	 0.0307 	 0.0435 	0.589	0.589	0.957	0.957
66 	 0.0329 	 0.0556 	0.582	0.582	0.956	0.956
67 	 0.0287 	 0.0318 	0.613	0.613	0.976	0.976
68 	 0.0293 	 0.0414 	0.597	0.597	0.960	0.960
69 	 0.0295 	 0.2626 	0.663	0.663	0.777	0.777
70 	 0.0282 	 0.0346 	0.599	0.599	0.974	0.974
71 	 0.0301 	 0.0559 	0.601	0.601	0.939	0.939
72 	 0.0274 	 0.0350 	0.621	0.621	0.969	0.969
73 	 0.0301 	 0.0367 	0.587	0.587	0.970	0.970
74 	 0.0323 	 0.0435 	0.600	0.600	0.984	0.984
75 	 0.0293 	 0.0434 	0.627	0.627	0.957	0.957
76 	 0.0293 	 0.0554 	0.643	0.643	0.950	0.950
77 	 0.0315 	 0.0418 	0.625	0.625	0.943	0.943
78 	 0.0273 	 0.0435 	0.600	0.600	0.978	0.978
79 	 0.0300 	 0.0311 	0.584	0.584	0.991	0.991
80 	 0.0279 	 0.0494 	0.616	0.616	0.966	0.966
81 	 0.0275 	 0.0396 	0.636	0.636	0.961	0.961
82 	 0.0291 	 0.0372 	0.640	0.640	0.960	0.960
83 	 0.0284 	 0.0322 	0.622	0.622	0.978	0.978
84 	 0.0255 	 0.0399 	0.644	0.644	0.958	0.958
85 	 0.0270 	 0.0389 	0.627	0.627	0.963	0.963
86 	 0.0244 	 0.0416 	0.645	0.645	0.959	0.959
87 	 0.0280 	 0.0359 	0.625	0.625	0.976	0.976
88 	 0.0259 	 0.0424 	0.644	0.644	0.954	0.954
89 	 0.0240 	 0.0331 	0.616	0.616	0.978	0.978
90 	 0.0232 	 0.0365 	0.636	0.636	0.957	0.957
91 	 0.0244 	 0.0353 	0.625	0.625	0.969	0.969
92 	 0.0253 	 0.0315 	0.642	0.642	0.973	0.973
93 	 0.0232 	 0.0406 	0.625	0.625	0.950	0.950
94 	 0.0263 	 0.0335 	0.612	0.612	0.978	0.978
95 	 0.0230 	 0.0315 	0.642	0.642	0.971	0.971
96 	 0.0218 	 0.0399 	0.626	0.626	0.946	0.946
97 	 0.0232 	 0.0373 	0.644	0.644	0.953	0.953
98 	 0.0240 	 0.0357 	0.631	0.631	0.971	0.971
99 	 0.0231 	 0.0400 	0.639	0.639	0.957	0.957
100 	 0.0255 	 0.0491 	0.664	0.664	0.947	0.947
101 	 0.0277 	 0.0394 	0.615	0.615	0.977	0.977
102 	 0.0237 	 0.0314 	0.617	0.617	0.982	0.982
103 	 0.0233 	 0.0329 	0.627	0.627	0.974	0.974
104 	 0.0225 	 0.0520 	0.668	0.668	0.940	0.940
105 	 0.0251 	 0.0509 	0.648	0.648	0.939	0.939
106 	 0.0271 	 0.0451 	0.641	0.641	0.945	0.945
107 	 0.0232 	 0.0358 	0.659	0.659	0.962	0.962
108 	 0.0232 	 0.0351 	0.622	0.622	0.975	0.975
109 	 0.0240 	 0.0426 	0.634	0.634	0.949	0.949
110 	 0.0221 	 0.0348 	0.672	0.672	0.963	0.963
111 	 0.0219 	 0.0341 	0.647	0.647	0.970	0.970
112 	 0.0250 	 0.0381 	0.606	0.606	0.977	0.977
113 	 0.0267 	 0.0411 	0.631	0.631	0.956	0.956
114 	 0.0234 	 0.0394 	0.638	0.638	0.965	0.965
115 	 0.0229 	 0.0281 	0.638	0.638	0.982	0.982
116 	 0.0219 	 0.0366 	0.656	0.656	0.959	0.959
117 	 0.0221 	 0.0324 	0.647	0.647	0.972	0.972
118 	 0.0223 	 0.0304 	0.644	0.644	0.974	0.974
119 	 0.0222 	 0.0309 	0.658	0.658	0.974	0.974
120 	 0.0199 	 0.0357 	0.665	0.665	0.956	0.956
121 	 0.0203 	 0.0351 	0.660	0.660	0.957	0.957
122 	 0.0190 	 0.0357 	0.668	0.668	0.959	0.959
123 	 0.0208 	 0.0376 	0.664	0.664	0.961	0.961
124 	 0.0206 	 0.0316 	0.653	0.653	0.974	0.974
125 	 0.0206 	 0.0346 	0.663	0.663	0.962	0.962
126 	 0.0198 	 0.0380 	0.660	0.660	0.946	0.946
127 	 0.0211 	 0.0403 	0.696	0.696	0.943	0.943
128 	 0.0192 	 0.0364 	0.679	0.679	0.959	0.959
129 	 0.0200 	 0.0360 	0.657	0.657	0.961	0.961
130 	 0.0217 	 0.0375 	0.631	0.631	0.970	0.970
131 	 0.0197 	 0.0351 	0.657	0.657	0.968	0.968
132 	 0.0201 	 0.0347 	0.644	0.644	0.967	0.967
133 	 0.0196 	 0.0358 	0.660	0.660	0.963	0.963
134 	 0.0239 	 0.0449 	0.676	0.676	0.927	0.927
135 	 0.0235 	 0.0326 	0.641	0.641	0.977	0.977
136 	 0.0211 	 0.0320 	0.664	0.664	0.968	0.968
137 	 0.0187 	 0.0348 	0.666	0.666	0.948	0.948
138 	 0.0188 	 0.0350 	0.664	0.664	0.948	0.948
139 	 0.0217 	 0.0399 	0.660	0.660	0.943	0.943
140 	 0.0194 	 0.0347 	0.661	0.661	0.963	0.963
141 	 0.0182 	 0.0377 	0.674	0.674	0.957	0.957
142 	 0.0193 	 0.0436 	0.680	0.680	0.930	0.930
143 	 0.0180 	 0.0324 	0.670	0.670	0.975	0.975
144 	 0.0191 	 0.0289 	0.635	0.635	0.981	0.981
145 	 0.0196 	 0.0379 	0.672	0.672	0.937	0.937
146 	 0.0203 	 0.0348 	0.676	0.676	0.965	0.965
147 	 0.0181 	 0.0330 	0.660	0.660	0.970	0.970
148 	 0.0187 	 0.0344 	0.655	0.655	0.962	0.962
