Program ID: 24216

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/images_npy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '0']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   use slices as features channels in convolution, and use 1*1 convolution along slices to implement z direction convolution followed by 3*3 convolutino slice planes;
                it just use three cascading 2D convolutions (frist z, then xy, and z directon again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-folc cross validation;
            6   add lossweight for ajusting positive sample to 3/7.

Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   training data has 169 patients 

Training strategy: 

          

Program starting Time: 2019-07-26 16:55:22.129194
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten

Info: this is the 0th fold leave for test in the 4-fold cross-validation, with 20.0% of data for independent test.

Infor: In all data of 169 files, label 0 has 51 files,
	  and label 1 has 118 files, where positive response rate = 0.6982248520710059 in full data
Infor: the independent test set has 34 files,and Training including validation Set has 135 files which will be divided into 4 folds.
In the independent test set of 34 files, 0 has 10, 1 has 24, the rate of 1s is 0.7058823529411765
In this test_0 dataset, total 34 files, where 1 has 24 with rate of 0.7058823529411765
In this train_0 dataset, total 100 files, where 1 has 70 with rate of 0.7
In this validation_0 dataset, total 35 files, where 1 has 24 with rate of 0.6857142857142857
=== Network trains from scratch ====
Network has total 71,720,500 parameters.
Info: program will use 2 GPUs.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	2.3685	0.4200	0.3857	0.5000		0.4484	0.3143	0.0000	1.0000		0.4493	0.2941	0.0000	1.0000
5	0.4857	0.6400	0.8429	0.1667		0.4079	0.6857	1.0000	0.0000		0.3884	0.7059	1.0000	0.0000
10	0.6348	0.7100	1.0000	0.0333		0.4662	0.4857	0.2917	0.9091		0.4813	0.3529	0.1667	0.8000
15	0.4349	0.5000	0.3571	0.8333		0.4160	0.4857	0.4167	0.6364		0.4642	0.5000	0.3750	0.8000
20	0.4276	0.4900	0.4286	0.6333		0.4737	0.6000	0.8333	0.0909		0.3104	0.7941	0.9167	0.5000
25	0.5143	0.3000	0.0286	0.9333		0.4995	0.3143	0.0417	0.9091		0.4943	0.2941	0.0417	0.9000
30	0.4284	0.4600	0.3571	0.7000		0.6043	0.4571	0.4583	0.4545		0.4909	0.4118	0.2917	0.7000
35	0.4260	0.4500	0.3286	0.7333		0.4096	0.4286	0.3750	0.5455		0.3972	0.3824	0.2500	0.7000
40	0.4386	0.6400	0.7429	0.4000		0.4622	0.4286	0.4167	0.4545		0.3894	0.5588	0.5000	0.7000
45	0.4036	0.6300	0.7429	0.3667		0.5189	0.6000	0.8333	0.0909		0.4398	0.6176	0.7917	0.2000
50	0.4312	0.4400	0.2286	0.9333		0.5383	0.3429	0.2083	0.6364		0.5121	0.3529	0.1667	0.8000
55	0.3789	0.6200	0.6286	0.6000		1.5111	0.5143	0.5417	0.4545		0.9698	0.5588	0.5417	0.6000
60	0.3920	0.5700	0.4571	0.8333		0.7146	0.5714	0.6667	0.3636		0.5298	0.5294	0.5417	0.5000
65	0.4046	0.5900	0.4571	0.9000		0.4829	0.5714	0.6250	0.4545		0.4360	0.6176	0.6667	0.5000
70	0.3801	0.6500	0.6143	0.7333		0.4663	0.4857	0.4167	0.6364		0.4464	0.4706	0.3750	0.7000
75	0.4118	0.6800	0.6429	0.7667		1.2599	0.4571	0.4583	0.4545		1.1377	0.4412	0.3750	0.6000
80	0.3352	0.7800	0.7857	0.7667		1.2842	0.5714	0.6667	0.3636		1.4544	0.5882	0.6250	0.5000
85	0.2570	0.7900	0.7143	0.9667		0.7983	0.6000	0.7500	0.2727		0.8257	0.6471	0.7500	0.4000
90	0.2591	0.7500	0.7000	0.8667		0.8500	0.6000	0.7083	0.3636		0.8999	0.6471	0.7083	0.5000
95	0.3443	0.8300	0.8286	0.8333		0.7927	0.6000	0.7917	0.1818		0.7752	0.5588	0.7083	0.2000
100	0.2311	0.8400	0.8571	0.8000		0.7995	0.5714	0.7083	0.2727		0.8245	0.5588	0.6250	0.4000
105	0.1765	0.8600	0.8286	0.9333		0.8098	0.5429	0.5833	0.4545		0.9436	0.4706	0.4167	0.6000
110	0.1918	0.8700	0.8571	0.9000		0.8941	0.5143	0.6667	0.1818		0.7617	0.6765	0.7917	0.4000
115	0.2854	0.8900	0.9143	0.8333		1.4511	0.6571	0.8750	0.1818		1.2185	0.6471	0.8750	0.1000
120	0.1814	0.9300	0.9143	0.9667		1.2281	0.6000	0.7917	0.1818		1.0086	0.6765	0.8333	0.3000
125	0.1807	0.9200	0.9000	0.9667		0.8067	0.6286	0.7917	0.2727		0.6133	0.6765	0.7500	0.5000
130	0.1698	0.9100	0.9143	0.9000		0.8694	0.6286	0.7917	0.2727		0.8687	0.6176	0.7083	0.4000
135	0.1984	0.8800	0.8857	0.8667		0.9670	0.6571	0.8750	0.1818		0.9204	0.6176	0.7917	0.2000
140	0.1525	0.9300	0.9143	0.9667		0.8496	0.6571	0.8333	0.2727		0.8856	0.5882	0.6667	0.4000
145	0.1275	0.9300	0.9143	0.9667		0.9268	0.5714	0.7083	0.2727		0.9049	0.5882	0.6667	0.4000
150	0.2578	0.9100	0.9143	0.9000		1.0881	0.6286	0.7917	0.2727		0.8896	0.6176	0.7083	0.4000
155	0.1430	0.9100	0.8857	0.9667		0.9645	0.6000	0.7500	0.2727		0.8412	0.5000	0.5417	0.4000
160	0.1565	0.9300	0.9429	0.9000		1.1514	0.6571	0.8333	0.2727		0.9578	0.5882	0.7083	0.3000
165	0.1043	0.9500	0.9571	0.9333		1.0016	0.6286	0.7917	0.2727		0.9792	0.6471	0.7083	0.5000
170	0.1023	0.9400	0.9429	0.9333		1.0381	0.6286	0.7917	0.2727		0.8711	0.6471	0.7083	0.5000
175	0.1842	0.8700	0.8857	0.8333		1.0034	0.6286	0.7917	0.2727		0.8354	0.5588	0.5833	0.5000
180	0.1679	0.9000	0.8714	0.9667		1.0141	0.5714	0.7083	0.2727		0.9096	0.5294	0.5417	0.5000
185	0.1070	0.9400	0.9429	0.9333		1.1386	0.6286	0.7917	0.2727		0.9683	0.6176	0.7083	0.4000
190	0.1223	0.9300	0.9286	0.9333		1.1150	0.6000	0.7500	0.2727		0.9778	0.5588	0.5833	0.5000
195	0.1233	0.9600	0.9857	0.9000		1.2459	0.6286	0.7917	0.2727		1.1478	0.6471	0.7500	0.4000
200	0.0653	0.9800	0.9857	0.9667		1.1709	0.6286	0.7917	0.2727		0.9660	0.6471	0.7917	0.3000
205	0.2289	0.8900	0.9286	0.8000		1.5051	0.6571	0.8333	0.2727		1.2006	0.6765	0.8333	0.3000
210	0.1494	0.8800	0.8714	0.9000		1.0650	0.5714	0.7083	0.2727		0.9001	0.5294	0.6250	0.3000
215	0.1052	0.9400	0.9571	0.9000		1.2689	0.5714	0.7083	0.2727		1.0531	0.5294	0.5417	0.5000
220	0.1309	0.9200	0.9143	0.9333		1.1988	0.6286	0.7917	0.2727		0.9214	0.6176	0.7083	0.4000
225	0.1265	0.9400	0.9286	0.9667		1.0847	0.5714	0.7083	0.2727		0.7916	0.5882	0.6250	0.5000
230	0.1505	0.9300	0.9143	0.9667		1.1411	0.6286	0.7917	0.2727		0.8166	0.6176	0.7500	0.3000
235	0.1158	0.9900	1.0000	0.9667		1.2399	0.6286	0.7917	0.2727		0.8958	0.5882	0.7083	0.3000
240	0.0768	0.9700	0.9571	1.0000		1.1452	0.6000	0.7500	0.2727		0.9766	0.5588	0.6250	0.4000
245	0.1012	0.9500	0.9429	0.9667		1.1327	0.6286	0.7917	0.2727		0.8511	0.5588	0.6667	0.3000
250	0.1947	0.9200	0.9286	0.9000		1.1337	0.6286	0.7917	0.2727		0.8257	0.5882	0.7083	0.3000
255	0.2822	0.9000	0.8857	0.9333		1.0789	0.6286	0.7500	0.3636		0.9143	0.6176	0.6667	0.5000
260	0.1422	0.9200	0.9143	0.9333		1.0465	0.6571	0.7917	0.3636		0.8501	0.5588	0.6667	0.3000
265	0.0918	0.9500	0.9286	1.0000		1.1336	0.6286	0.7917	0.2727		0.8334	0.6471	0.7500	0.4000
270	0.2698	0.9000	0.9000	0.9000		1.2440	0.6286	0.7917	0.2727		1.0096	0.7059	0.8333	0.4000
275	0.1272	0.9300	0.9286	0.9333		1.2872	0.6000	0.7917	0.1818		1.0122	0.7059	0.8333	0.4000
280	0.0925	0.9600	0.9571	0.9667		1.1859	0.6286	0.7917	0.2727		0.8320	0.5882	0.7083	0.3000
285	0.2265	0.9300	0.9143	0.9667		1.1460	0.5714	0.7083	0.2727		0.9338	0.5882	0.5833	0.6000
290	0.1073	0.9000	0.8857	0.9333		1.0956	0.6286	0.7917	0.2727		0.7553	0.7059	0.7917	0.5000
295	0.1448	0.8800	0.8857	0.8667		1.0522	0.6286	0.7500	0.3636		0.9467	0.6471	0.7083	0.5000
300	0.1633	0.9100	0.9000	0.9333		1.2214	0.6286	0.7917	0.2727		0.9025	0.6471	0.7500	0.4000
305	0.1506	0.9100	0.8714	1.0000		1.1221	0.6286	0.7917	0.2727		0.8733	0.6176	0.7083	0.4000
310	0.0961	0.9500	0.9571	0.9333		1.0716	0.6286	0.7917	0.2727		0.8239	0.5882	0.6667	0.4000
315	0.0781	0.9500	0.9571	0.9333		1.1788	0.6286	0.7917	0.2727		1.0099	0.5294	0.5833	0.4000
320	0.1797	0.9300	0.9571	0.8667		1.2898	0.6571	0.8333	0.2727		1.0014	0.7059	0.8333	0.4000
325	0.1055	0.9200	0.9143	0.9333		1.0975	0.6000	0.7500	0.2727		0.8964	0.5588	0.6250	0.4000
330	0.2057	0.8600	0.8429	0.9000		1.1856	0.6286	0.7917	0.2727		0.8954	0.6176	0.7083	0.4000
335	0.1041	0.9700	0.9714	0.9667		1.1069	0.6286	0.7500	0.3636		0.9590	0.6176	0.7083	0.4000
340	0.2176	0.8900	0.9143	0.8333		1.1805	0.6286	0.7917	0.2727		0.9478	0.7059	0.8333	0.4000
345	0.1497	0.9100	0.9143	0.9000		1.1853	0.6286	0.7917	0.2727		0.8234	0.6471	0.7500	0.4000
350	0.1730	0.9300	0.9429	0.9000		1.0690	0.6286	0.7917	0.2727		0.8377	0.5588	0.6250	0.4000
355	0.0870	0.9700	0.9571	1.0000		1.0336	0.6286	0.7500	0.3636		0.8636	0.5882	0.6250	0.5000
360	0.1467	0.9100	0.9000	0.9333		1.1516	0.6286	0.7917	0.2727		0.8904	0.6176	0.7083	0.4000
365	0.2436	0.9000	0.9143	0.8667		1.0741	0.6571	0.8333	0.2727		0.8519	0.6176	0.7083	0.4000
370	0.1332	0.9100	0.9000	0.9333		1.0618	0.6000	0.7083	0.3636		0.9905	0.5000	0.5000	0.5000
375	0.0999	0.9600	0.9429	1.0000		1.0929	0.6286	0.7917	0.2727		0.9076	0.5882	0.6667	0.4000
380	0.1219	0.9900	1.0000	0.9667		1.1168	0.6286	0.7917	0.2727		0.8650	0.6176	0.7083	0.4000
385	0.1376	0.9000	0.8857	0.9333		1.3791	0.6571	0.8333	0.2727		1.1214	0.7059	0.8333	0.4000
390	0.1639	0.8900	0.8857	0.9000		1.0986	0.5714	0.6667	0.3636		0.8685	0.5588	0.5833	0.5000
395	0.0733	0.9800	0.9857	0.9667		1.0291	0.5714	0.7083	0.2727		0.7783	0.5588	0.6250	0.4000
400	0.1374	0.9300	0.9429	0.9000		1.1099	0.6000	0.7500	0.2727		0.8798	0.5588	0.6667	0.3000
405	0.1222	0.9500	0.9714	0.9000		1.2017	0.6286	0.7917	0.2727		0.9465	0.6471	0.7083	0.5000
410	0.1458	0.9300	0.9286	0.9333		1.0250	0.6571	0.7917	0.3636		0.7928	0.5882	0.6667	0.4000
415	0.1464	0.9200	0.9143	0.9333		1.1759	0.6286	0.7917	0.2727		0.8736	0.6176	0.7083	0.4000
420	0.0924	0.9600	0.9571	0.9667		1.1892	0.6286	0.7917	0.2727		0.9944	0.6471	0.7500	0.4000
425	0.0750	0.9700	0.9714	0.9667		1.1348	0.6000	0.7500	0.2727		0.9354	0.6471	0.7083	0.5000
430	0.1352	0.9200	0.9286	0.9000		1.3168	0.6571	0.8333	0.2727		1.1066	0.7059	0.8333	0.4000
435	0.0576	0.9800	0.9714	1.0000		1.2747	0.6286	0.7917	0.2727		0.9297	0.6765	0.7917	0.4000
440	0.1428	0.9200	0.9143	0.9333		1.0645	0.5714	0.6667	0.3636		0.8299	0.5294	0.5833	0.4000
445	0.1451	0.8900	0.9000	0.8667		1.0976	0.6000	0.7083	0.3636		0.9601	0.5294	0.5417	0.5000
450	0.2611	0.8800	0.8714	0.9000		1.1480	0.6286	0.7917	0.2727		0.8404	0.5588	0.6667	0.3000
455	0.2263	0.9400	0.9429	0.9333		1.2604	0.6286	0.7917	0.2727		1.0388	0.7059	0.8333	0.4000
460	0.0680	0.9700	0.9714	0.9667		1.0604	0.6000	0.7500	0.2727		0.8994	0.5882	0.6250	0.5000
465	0.1123	0.9300	0.9571	0.8667		1.0587	0.6286	0.7917	0.2727		0.7969	0.6176	0.6667	0.5000
470	0.0916	0.9500	0.9286	1.0000		1.0943	0.5714	0.7083	0.2727		0.9344	0.5882	0.6250	0.5000
475	0.1082	0.9300	0.9143	0.9667		1.0734	0.6286	0.7917	0.2727		0.6970	0.6765	0.8333	0.3000
480	0.1635	0.9200	0.9286	0.9000		1.1516	0.6286	0.7917	0.2727		0.8236	0.6176	0.7500	0.3000
485	0.1170	0.9400	0.9429	0.9333		1.1433	0.6000	0.7500	0.2727		0.9320	0.6176	0.6667	0.5000
490	0.0550	1.0000	1.0000	1.0000		1.0632	0.6286	0.7917	0.2727		0.8114	0.5588	0.6667	0.3000


response Train Accuracy == 1, Program exit.


=============END of Training of RexAttentionNet Predict Model =================

Program Ending Time: 2019-07-26 18:45:06.382050
