=============training from sratch============
Program ID: 28035

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/images_npy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '0', '0']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
            8    at July 30th 03:00, 2019:
                    add learning rate print;
                    use convStride =2;
                    add filter number by 2 times along deeper layers.
            9    at July 30th, 10:13, 2019:
                    add MaxPool2d in stage1;
                    add final filters to 2048.
            10   at July 30th, 15:23, 2019
                    final conv layer filter number: 1024
                                                 
                    
            
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   training data has 169 patients 

Training strategy: 

          

Program starting Time: 2019-07-30 15:25:37.125597
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten/20190730_152537

Info: this is the 0th fold leave for test in the 4-fold cross-validation, with 20.0% of data for independent test.

Infor: In all data of 169 files, label 0 has 51 files,
	  and label 1 has 118 files, where positive response rate = 0.6982248520710059 in full data
Infor: the independent test set has 34 files,and Training including validation Set has 135 files which will be divided into 4 folds.
In the independent test set of 34 files, 0 has 10, 1 has 24, the rate of 1s is 0.7058823529411765
In this test_0 dataset, total 34 files, where 1 has 24 with rate of 0.7058823529411765
In this train_0 dataset, total 100 files, where 1 has 70 with rate of 0.7
In this validation_0 dataset, total 35 files, where 1 has 24 with rate of 0.6857142857142857
=== Network trains from scratch ====
Network has total 90,343,924 parameters.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	LearningRate		TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	1.0000e-02		1.2704	0.5100	0.5286	0.4667		0.4235	0.6857	1.0000	0.0000		0.4071	0.7059	1.0000	0.0000
5	1.0000e-02		2.6395	0.6000	0.6714	0.4333		1.5247	0.3429	0.0417	1.0000		1.3569	0.3235	0.0417	1.0000
10	9.5000e-03		0.5188	0.4500	0.3143	0.7667		4.3808	0.6286	0.9167	0.0000		9.4551	0.6765	0.8750	0.2000
15	9.5000e-03		0.8417	0.4800	0.5286	0.3667		1.0429	0.6857	1.0000	0.0000		1.7019	0.7059	1.0000	0.0000
20	9.0250e-03		0.5161	0.3700	0.3286	0.4667		0.4659	0.4000	0.2917	0.6364		0.3740	0.3824	0.2083	0.8000
25	9.0250e-03		0.5042	0.6800	0.9286	0.1000		0.4313	0.6857	1.0000	0.0000		0.3796	0.7353	1.0000	0.1000
30	8.5737e-03		0.4500	0.3000	0.0286	0.9333		0.4082	0.3714	0.0833	1.0000		0.4303	0.3529	0.1250	0.9000
35	8.5737e-03		0.4522	0.4800	0.3714	0.7333		0.4268	0.4286	0.2500	0.8182		1.3951	0.3235	0.1667	0.7000
40	8.1451e-03		0.4211	0.6900	0.9714	0.0333		0.4419	0.6857	1.0000	0.0000		0.4382	0.7059	0.9583	0.1000
45	8.1451e-03		0.4279	0.5900	0.7714	0.1667		0.4197	0.7143	1.0000	0.0909		0.8133	0.7059	0.9167	0.2000
50	7.7378e-03		0.4403	0.6900	0.9571	0.0667		0.4276	0.5714	0.8333	0.0000		1.0178	0.7059	1.0000	0.0000
55	7.7378e-03		0.4190	0.6200	0.8714	0.0333		0.4163	0.6857	1.0000	0.0000		0.4481	0.6765	0.9583	0.0000
60	7.3509e-03		0.4221	0.5400	0.5571	0.5000		0.4179	0.6571	0.9167	0.0909		0.4092	0.6471	0.8750	0.1000
65	7.3509e-03		0.4245	0.7000	0.9571	0.1000		0.4263	0.6857	1.0000	0.0000		0.4905	0.7059	1.0000	0.0000
70	6.9834e-03		0.4285	0.6700	0.9143	0.1000		0.4191	0.6857	0.9167	0.1818		0.4034	0.6765	0.8333	0.3000
75	6.9834e-03		0.4253	0.6100	0.7857	0.2000		0.4200	0.6857	0.9167	0.1818		0.6117	0.6765	0.8750	0.2000
80	6.6342e-03		0.4147	0.3000	0.0571	0.8667		0.4192	0.3143	0.0833	0.8182		0.3966	0.3235	0.1250	0.8000
85	6.6342e-03		0.4156	0.6900	0.9286	0.1333		0.4225	0.6857	0.9583	0.0909		0.4307	0.6765	0.9167	0.1000
90	6.3025e-03		0.4244	0.3500	0.1000	0.9333		0.4164	0.3429	0.0833	0.9091		0.3890	0.3235	0.1250	0.8000
95	6.3025e-03		0.4088	0.6400	0.8000	0.2667		0.4238	0.6571	0.9167	0.0909		0.4072	0.7059	0.9167	0.2000
100	5.9874e-03		0.4183	0.3900	0.1429	0.9667		0.4174	0.3429	0.0833	0.9091		0.3959	0.2647	0.0417	0.8000
105	5.9874e-03		0.4173	0.6900	0.9143	0.1667		0.4202	0.6286	0.9167	0.0000		0.3995	0.6471	0.8333	0.2000
110	5.6880e-03		0.4191	0.5400	0.5714	0.4667		0.4234	0.3143	0.0417	0.9091		0.4158	0.2941	0.0417	0.9000
115	5.6880e-03		0.4084	0.6500	0.8714	0.1333		0.4272	0.6571	0.9167	0.0909		0.4273	0.6471	0.8750	0.1000
120	5.4036e-03		0.4026	0.7100	0.9429	0.1667		0.4191	0.6857	0.9583	0.0909		0.4071	0.6765	0.8750	0.2000
125	5.4036e-03		0.4049	0.7200	0.9571	0.1667		0.4231	0.6857	0.9583	0.0909		0.4052	0.7647	0.9583	0.3000
130	5.1334e-03		0.4044	0.6500	0.8714	0.1333		0.4242	0.6571	0.9167	0.0909		0.4235	0.6765	0.8750	0.2000
135	5.1334e-03		0.4019	0.5700	0.6857	0.3000		0.4215	0.6857	0.9167	0.1818		0.4048	0.6765	0.8333	0.3000
140	4.8767e-03		0.4400	0.6500	0.9143	0.0333		0.4201	0.6857	1.0000	0.0000		0.4270	0.6765	0.8750	0.2000
145	4.8767e-03		0.4084	0.6800	0.9143	0.1333		0.4192	0.6857	1.0000	0.0000		0.3999	0.6471	0.8750	0.1000
150	4.6329e-03		0.4061	0.6700	0.9000	0.1333		0.4206	0.6857	0.9583	0.0909		0.4099	0.6765	0.8750	0.2000
155	4.6329e-03		0.4130	0.6600	0.8857	0.1333		0.4182	0.6571	0.9583	0.0000		0.4012	0.6765	0.8750	0.2000
160	4.4013e-03		0.4248	0.6000	0.7857	0.1667		0.4188	0.6857	0.9583	0.0909		0.4127	0.6471	0.7917	0.3000
165	4.4013e-03		0.4207	0.6600	0.9000	0.1000		0.4187	0.6571	0.9167	0.0909		0.3983	0.6765	0.8333	0.3000
170	4.1812e-03		0.4029	0.6100	0.6714	0.4667		0.4222	0.6857	0.9167	0.1818		0.4245	0.6471	0.7917	0.3000
175	4.1812e-03		0.3994	0.6700	0.8857	0.1667		0.4223	0.6571	0.9167	0.0909		0.4446	0.7059	0.8750	0.3000
180	3.9721e-03		0.4107	0.6500	0.8714	0.1333		0.4171	0.6857	1.0000	0.0000		0.3955	0.6765	0.9167	0.1000
185	3.9721e-03		0.3933	0.6800	0.9143	0.1333		0.4198	0.6857	1.0000	0.0000		0.4266	0.7059	0.9167	0.2000
190	3.7735e-03		0.4161	0.5500	0.6714	0.2667		0.4164	0.6857	0.9583	0.0909		0.4066	0.6471	0.8750	0.1000
195	3.7735e-03		0.4122	0.5500	0.5143	0.6333		0.4173	0.5714	0.7917	0.0909		0.4034	0.6471	0.7917	0.3000
200	3.5849e-03		0.4117	0.6700	0.8857	0.1667		0.4181	0.6857	0.9583	0.0909		0.4053	0.7059	0.8750	0.3000
205	3.5849e-03		0.4028	0.6900	0.9000	0.2000		0.4175	0.6857	0.9583	0.0909		0.4028	0.6471	0.8750	0.1000
210	3.4056e-03		0.4100	0.6200	0.8571	0.0667		0.4217	0.6857	0.9583	0.0909		0.4168	0.6765	0.8750	0.2000
215	3.4056e-03		0.4071	0.6500	0.9143	0.0333		0.4193	0.6857	0.9583	0.0909		0.4027	0.7059	0.8750	0.3000
220	3.2353e-03		0.4386	0.6600	0.8857	0.1333		0.4215	0.6857	0.9583	0.0909		0.4108	0.6765	0.8333	0.3000
225	3.2353e-03		0.4126	0.6500	0.8714	0.1333		0.4203	0.6857	0.9583	0.0909		0.4124	0.6471	0.7917	0.3000
230	3.0736e-03		0.4175	0.5600	0.7000	0.2333		0.4191	0.6857	0.9583	0.0909		0.4055	0.6176	0.7500	0.3000
235	3.0736e-03		0.4048	0.6600	0.8143	0.3000		0.4164	0.6857	0.9583	0.0909		0.3997	0.6471	0.7917	0.3000
240	2.9199e-03		0.4174	0.6500	0.8571	0.1667		0.4183	0.6857	0.9583	0.0909		0.4002	0.6765	0.8333	0.3000
245	2.9199e-03		0.4192	0.6300	0.7857	0.2667		0.4215	0.6286	0.8750	0.0909		0.4180	0.6176	0.7500	0.3000
250	2.7739e-03		0.4141	0.6500	0.8857	0.1000		0.4156	0.6857	0.9583	0.0909		0.3998	0.6471	0.8750	0.1000
255	2.7739e-03		0.4097	0.7100	0.9286	0.2000		0.4188	0.6857	0.9583	0.0909		0.4004	0.6471	0.8333	0.2000
