=============training from sratch============
Program ID: 14657

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/images_npy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '1', '1']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
            8    at July 30th 03:00, 2019:
                    add learning rate print;
                    use convStride =2;
                    add filter number by 2 times along deeper layers.
            9    at July 30th, 10:13, 2019:
                    add MaxPool2d in stage1;
                    add final filters to 2048.
                                            
                    
            
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   training data has 169 patients 

Training strategy: 

          

Program starting Time: 2019-07-30 10:30:08.089944
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten/20190730_103008

Info: this is the 1th fold leave for test in the 4-fold cross-validation, with 20.0% of data for independent test.

Infor: In all data of 169 files, label 0 has 51 files,
	  and label 1 has 118 files, where positive response rate = 0.6982248520710059 in full data
Infor: the independent test set has 34 files,and Training including validation Set has 135 files which will be divided into 4 folds.
In the independent test set of 34 files, 0 has 10, 1 has 24, the rate of 1s is 0.7058823529411765
In this test_1 dataset, total 34 files, where 1 has 24 with rate of 0.7058823529411765
In this train_1 dataset, total 101 files, where 1 has 70 with rate of 0.693069306930693
In this validation_1 dataset, total 34 files, where 1 has 24 with rate of 0.7058823529411765
=== Network trains from scratch ====
Network has total 294,823,412 parameters.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	LearningRate		TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	1.0000e-02		1.1578	0.4653	0.4143	0.5806		0.4091	0.2941	0.0000	1.0000		0.4091	0.2941	0.0000	1.0000
5	1.0000e-02		1.6277	0.5347	0.6286	0.3226		3.0479	0.4706	0.4583	0.5000		1.3769	0.5294	0.3333	1.0000
10	9.5000e-03		0.8288	0.4851	0.4429	0.5806		0.8405	0.7059	1.0000	0.0000		0.4010	0.7647	1.0000	0.2000
15	9.5000e-03		0.7432	0.5149	0.5429	0.4516		0.5373	0.6471	0.9167	0.0000		0.6166	0.7059	0.9167	0.2000
20	9.0250e-03		0.5613	0.5545	0.6286	0.3871		0.5089	0.3235	0.0417	1.0000		0.5293	0.3235	0.0417	1.0000
25	9.0250e-03		0.4341	0.3663	0.1857	0.7742		0.3862	0.4412	0.2917	0.8000		0.4243	0.5294	0.3750	0.9000
30	8.5737e-03		0.4782	0.5941	0.7143	0.3226		0.4696	0.5588	0.7083	0.2000		0.3908	0.5000	0.7083	0.0000
35	8.5737e-03		0.4759	0.4158	0.2286	0.8387		0.4173	0.4118	0.3333	0.6000		0.4033	0.5882	0.4167	1.0000
40	8.1451e-03		0.4104	0.4356	0.2143	0.9355		0.3394	0.7647	1.0000	0.2000		0.3477	0.6471	0.9167	0.0000
45	8.1451e-03		0.3979	0.4752	0.2714	0.9355		0.3446	0.5000	0.3333	0.9000		0.3600	0.5882	0.4167	1.0000
50	7.7378e-03		0.4120	0.4356	0.2429	0.8710		0.3512	0.5588	0.3750	1.0000		0.3488	0.5882	0.4167	1.0000
55	7.7378e-03		0.4013	0.6139	0.6857	0.4516		0.7048	0.4412	0.3333	0.7000		0.3172	0.6176	0.4583	1.0000
60	7.3509e-03		0.3995	0.5842	0.6286	0.4839		0.3818	0.5294	0.3750	0.9000		0.3727	0.6176	0.4583	1.0000
65	7.3509e-03		0.3882	0.4554	0.2429	0.9355		0.3621	0.4706	0.2917	0.9000		0.3718	0.5294	0.3333	1.0000
70	6.9834e-03		0.4300	0.4257	0.2429	0.8387		0.3990	0.4412	0.2917	0.8000		0.3816	0.6176	0.4583	1.0000
75	6.9834e-03		0.4054	0.5149	0.3429	0.9032		0.3729	0.4706	0.2917	0.9000		0.3733	0.5588	0.3750	1.0000
80	6.6342e-03		0.3957	0.4653	0.2714	0.9032		0.3663	0.5000	0.2917	1.0000		0.3911	0.4118	0.1667	1.0000
85	6.6342e-03		0.3757	0.5050	0.3143	0.9355		0.4371	0.4412	0.3333	0.7000		0.3236	0.5882	0.4167	1.0000
90	6.3025e-03		0.4069	0.6832	0.9286	0.1290		0.3936	0.5000	0.4167	0.7000		0.3375	0.5882	0.4167	1.0000
95	6.3025e-03		0.4093	0.5347	0.4143	0.8065		0.3861	0.5294	0.4583	0.7000		0.3541	0.6176	0.5000	0.9000
100	5.9874e-03		0.5036	0.5446	0.4000	0.8710		0.4364	0.5000	0.4167	0.7000		0.3300	0.5588	0.4583	0.8000
105	5.9874e-03		0.3841	0.5644	0.4000	0.9355		0.3496	0.5588	0.4583	0.8000		0.3390	0.5882	0.4167	1.0000
110	5.6880e-03		0.3871	0.5644	0.4286	0.8710		0.3559	0.5000	0.4167	0.7000		0.3492	0.6471	0.5000	1.0000
115	5.6880e-03		0.3675	0.6337	0.5286	0.8710		0.3640	0.5294	0.4167	0.8000		0.3340	0.6176	0.5417	0.8000
120	5.4036e-03		0.3805	0.6139	0.4714	0.9355		0.3897	0.5000	0.4583	0.6000		0.3407	0.6471	0.5417	0.9000
125	5.4036e-03		0.4519	0.6634	0.6000	0.8065		0.3791	0.4706	0.3750	0.7000		0.3825	0.5294	0.4167	0.8000
130	5.1334e-03		0.3421	0.6139	0.4714	0.9355		0.3489	0.5882	0.4583	0.9000		0.6006	0.5294	0.4583	0.7000
135	5.1334e-03		0.3270	0.6535	0.5000	1.0000		0.4178	0.5294	0.5000	0.6000		0.4188	0.5882	0.5000	0.8000
140	4.8767e-03		0.3293	0.6535	0.5429	0.9032		0.4913	0.5000	0.4583	0.6000		0.6891	0.5588	0.4583	0.8000
145	4.8767e-03		0.3234	0.6337	0.4857	0.9677		0.4052	0.5000	0.3750	0.8000		0.4314	0.5000	0.3750	0.8000
150	4.6329e-03		0.2726	0.7129	0.5857	1.0000		0.4715	0.5000	0.4167	0.7000		0.6410	0.4706	0.4167	0.6000
155	4.6329e-03		0.2891	0.7129	0.6000	0.9677		0.5377	0.5294	0.5417	0.5000		0.5209	0.6176	0.5833	0.7000
160	4.4013e-03		0.3211	0.6634	0.5857	0.8387		0.6757	0.5294	0.5000	0.6000		0.3849	0.6471	0.5833	0.8000
165	4.4013e-03		0.2881	0.7129	0.6286	0.9032		0.5219	0.5294	0.5000	0.6000		0.4955	0.5294	0.4583	0.7000
170	4.1812e-03		0.2865	0.7525	0.7143	0.8387		0.6430	0.4412	0.3750	0.6000		0.6169	0.5882	0.5000	0.8000
175	4.1812e-03		0.3416	0.5644	0.3714	1.0000		0.4822	0.5000	0.4167	0.7000		0.3734	0.5588	0.4167	0.9000
180	3.9721e-03		0.2591	0.7822	0.7429	0.8710		0.6173	0.5588	0.5417	0.6000		0.4077	0.6471	0.6667	0.6000
185	3.9721e-03		0.3831	0.7525	0.7857	0.6774		0.8405	0.4706	0.4583	0.5000		0.6139	0.5882	0.4583	0.9000
190	3.7735e-03		0.2830	0.7327	0.7000	0.8065		0.6318	0.5294	0.5833	0.4000		0.4941	0.5882	0.5833	0.6000
195	3.7735e-03		0.2919	0.7228	0.6714	0.8387		0.5417	0.5588	0.5417	0.6000		0.5142	0.4706	0.5417	0.3000
200	3.5849e-03		0.3663	0.7426	0.7571	0.7097		0.6729	0.5000	0.5417	0.4000		0.5572	0.5882	0.6667	0.4000
205	3.5849e-03		0.2181	0.8317	0.8143	0.8710		0.7333	0.5588	0.6250	0.4000		0.5663	0.5588	0.5417	0.6000
210	3.4056e-03		0.3138	0.8020	0.7714	0.8710		0.9629	0.5000	0.5417	0.4000		0.6266	0.6176	0.6250	0.6000
215	3.4056e-03		0.2943	0.6832	0.6000	0.8710		0.5397	0.4706	0.5000	0.4000		0.4194	0.6176	0.5833	0.7000
220	3.2353e-03		0.5388	0.5545	0.5286	0.6129		1.1922	0.6765	0.9167	0.1000		1.3428	0.6765	0.9167	0.1000
225	3.2353e-03		0.4138	0.5941	0.7571	0.2258		0.4757	0.5588	0.7917	0.0000		0.4062	0.6471	0.8333	0.2000
230	3.0736e-03		0.4186	0.6436	0.7286	0.4516		0.4368	0.5294	0.5417	0.5000		0.3917	0.6176	0.6667	0.5000
235	3.0736e-03		0.3844	0.7129	0.8429	0.4194		0.4704	0.5000	0.5833	0.3000		0.4009	0.6765	0.7500	0.5000
240	2.9199e-03		0.4164	0.6040	0.7000	0.3871		0.4241	0.5588	0.6250	0.4000		0.3813	0.6765	0.8333	0.3000
245	2.9199e-03		0.3751	0.6535	0.6571	0.6452		0.5214	0.5000	0.5417	0.4000		0.4533	0.5588	0.7083	0.2000
250	2.7739e-03		0.3798	0.6535	0.6714	0.6129		0.5729	0.5000	0.5417	0.4000		0.4255	0.5882	0.6667	0.4000
255	2.7739e-03		0.3739	0.6436	0.6571	0.6129		0.5600	0.5000	0.5417	0.4000		0.3960	0.6176	0.6667	0.5000
260	2.6352e-03		0.3819	0.6832	0.7143	0.6129		0.6036	0.4706	0.5000	0.4000		0.4313	0.6176	0.7083	0.4000
265	2.6352e-03		0.3807	0.6733	0.7143	0.5806		0.7021	0.4118	0.4167	0.4000		0.5446	0.5588	0.6250	0.4000
270	2.5034e-03		0.3206	0.7228	0.7571	0.6452		0.6412	0.3824	0.3750	0.4000		0.4424	0.6176	0.6667	0.5000
275	2.5034e-03		0.3291	0.7228	0.7286	0.7097		0.7183	0.3824	0.3750	0.4000		0.5210	0.5588	0.6250	0.4000
280	2.3783e-03		0.3039	0.7723	0.8000	0.7097		0.7906	0.4118	0.3750	0.5000		0.6133	0.4706	0.5000	0.4000
285	2.3783e-03		0.2916	0.7129	0.7429	0.6452		0.7795	0.4118	0.4583	0.3000		0.6222	0.5588	0.6667	0.3000
290	2.2594e-03		0.2509	0.8020	0.8143	0.7742		0.9088	0.4412	0.5000	0.3000		0.5427	0.6765	0.7500	0.5000
295	2.2594e-03		0.3452	0.6832	0.6714	0.7097		0.7192	0.3235	0.2917	0.4000		0.5054	0.5588	0.5000	0.7000
300	2.1464e-03		0.2909	0.7525	0.7571	0.7419		0.6814	0.4706	0.5000	0.4000		0.5780	0.5294	0.5833	0.4000
305	2.1464e-03		0.2742	0.8416	0.8571	0.8065		0.8750	0.5294	0.6667	0.2000		0.6566	0.6471	0.7083	0.5000
310	2.0391e-03		0.2582	0.8218	0.8571	0.7419		0.8370	0.4118	0.5000	0.2000		0.6084	0.5882	0.6667	0.4000
315	2.0391e-03		0.2786	0.8218	0.8714	0.7097		0.7479	0.3824	0.4583	0.2000		0.4729	0.7059	0.7500	0.6000
320	1.9371e-03		0.2180	0.8713	0.9000	0.8065		0.9175	0.5000	0.5833	0.3000		0.7081	0.7059	0.8750	0.3000
325	1.9371e-03		0.2668	0.7822	0.7857	0.7742		0.9355	0.5294	0.6250	0.3000		0.5829	0.5882	0.6667	0.4000
330	1.8403e-03		0.2324	0.7921	0.8143	0.7419		0.8744	0.5000	0.5417	0.4000		0.6123	0.5588	0.6250	0.4000
335	1.8403e-03		0.1889	0.8515	0.8571	0.8387		0.9655	0.5294	0.6667	0.2000		0.6773	0.6765	0.7500	0.5000
340	1.7482e-03		0.2094	0.8515	0.8714	0.8065		0.9314	0.5588	0.6250	0.4000		0.9279	0.5294	0.6667	0.2000
345	1.7482e-03		0.2474	0.8020	0.8286	0.7419		0.9249	0.5000	0.5417	0.4000		0.6435	0.7059	0.7917	0.5000
350	1.6608e-03		0.1364	0.9307	0.9429	0.9032		0.7927	0.6176	0.6667	0.5000		0.6898	0.5588	0.6667	0.3000
355	1.6608e-03		0.2224	0.8119	0.7857	0.8710		0.7869	0.5588	0.5833	0.5000		0.5815	0.6471	0.6250	0.7000
360	1.5778e-03		0.1194	0.9109	0.9286	0.8710		1.2557	0.6765	0.7500	0.5000		0.9184	0.7353	0.8750	0.4000
365	1.5778e-03		0.1963	0.8515	0.8143	0.9355		1.0270	0.3235	0.2500	0.5000		0.7061	0.5588	0.5417	0.6000
370	1.4989e-03		0.1798	0.9010	0.9429	0.8065		1.3606	0.3824	0.2917	0.6000		1.0436	0.4706	0.4583	0.5000
375	1.4989e-03		0.1904	0.9010	0.9143	0.8710		1.0770	0.4706	0.4583	0.5000		0.6109	0.6471	0.6667	0.6000
380	1.4240e-03		0.1771	0.9010	0.9286	0.8387		1.1675	0.5588	0.5833	0.5000		0.8476	0.5882	0.7083	0.3000
385	1.4240e-03		0.2455	0.7921	0.8000	0.7742		1.3472	0.4412	0.4583	0.4000		0.9734	0.5294	0.5833	0.4000
390	1.3528e-03		0.1329	0.9307	0.9714	0.8387		1.1881	0.6471	0.7083	0.5000		0.8730	0.5882	0.7083	0.3000
395	1.3528e-03		0.1448	0.8911	0.8714	0.9355		1.1933	0.5588	0.5833	0.5000		1.0243	0.4412	0.5000	0.3000
400	1.2851e-03		0.1873	0.8812	0.9000	0.8387		1.1468	0.6176	0.6667	0.5000		0.8644	0.6176	0.7500	0.3000
405	1.2851e-03		0.1284	0.9109	0.9429	0.8387		1.2506	0.6176	0.6667	0.5000		0.9646	0.5588	0.6667	0.3000
410	1.2209e-03		0.0621	0.9703	0.9857	0.9355		1.7108	0.4706	0.4583	0.5000		1.3601	0.5588	0.5000	0.7000
415	1.2209e-03		0.1541	0.9307	0.9571	0.8710		1.7198	0.5882	0.6250	0.5000		1.4643	0.6176	0.7083	0.4000
420	1.1598e-03		0.1758	0.8812	0.9000	0.8387		1.5552	0.4118	0.3750	0.5000		1.0008	0.5882	0.5417	0.7000
425	1.1598e-03		0.1570	0.8713	0.8714	0.8710		1.4847	0.3824	0.2500	0.7000		1.1665	0.3824	0.2500	0.7000
430	1.1018e-03		0.0458	0.9703	0.9571	1.0000		1.3084	0.7059	0.8333	0.4000		0.8177	0.6765	0.7500	0.5000
435	1.1018e-03		0.1176	0.9208	0.9143	0.9355		1.4424	0.5294	0.5417	0.5000		1.2546	0.5294	0.6250	0.3000
440	1.0467e-03		0.1365	0.9109	0.9286	0.8710		2.0382	0.3529	0.2917	0.5000		1.5939	0.4706	0.5000	0.4000
445	1.0467e-03		0.2226	0.9109	0.9000	0.9355		1.6985	0.5882	0.6667	0.4000		1.2719	0.5882	0.7083	0.3000
450	9.9440e-04		0.0836	0.9703	0.9714	0.9677		1.3770	0.6176	0.6667	0.5000		0.9284	0.7059	0.8333	0.4000
455	9.9440e-04		0.1008	0.9406	0.9571	0.9032		1.5657	0.5000	0.5000	0.5000		1.0343	0.5588	0.5417	0.6000
460	9.4468e-04		0.1195	0.9406	0.9286	0.9677		1.7661	0.5882	0.6667	0.4000		1.1346	0.6471	0.7083	0.5000
465	9.4468e-04		0.0594	0.9406	0.9143	1.0000		1.6434	0.5588	0.5833	0.5000		1.2744	0.5588	0.6667	0.3000
470	8.9745e-04		0.0701	0.9505	0.9429	0.9677		1.8431	0.6765	0.8333	0.3000		1.2728	0.6471	0.7917	0.3000
475	8.9745e-04		0.1499	0.9109	0.8857	0.9677		2.1521	0.3824	0.3333	0.5000		1.5467	0.4412	0.4583	0.4000
480	8.5258e-04		0.0752	0.9406	0.9429	0.9355		1.5985	0.5588	0.5833	0.5000		1.0750	0.6471	0.7083	0.5000
485	8.5258e-04		0.0856	0.9406	0.9286	0.9677		1.6893	0.6471	0.7083	0.5000		1.2287	0.5882	0.7500	0.2000
490	8.0995e-04		0.0720	0.9604	0.9429	1.0000		1.8370	0.5294	0.5417	0.5000		1.3078	0.5882	0.7083	0.3000
495	8.0995e-04		0.1188	0.9208	0.9286	0.9032		1.5139	0.4706	0.4583	0.5000		1.1839	0.5294	0.5833	0.4000
500	7.6945e-04		0.0491	0.9703	0.9571	1.0000		1.5963	0.5294	0.5417	0.5000		1.0472	0.7059	0.7083	0.7000
505	7.6945e-04		0.0902	0.9406	0.9429	0.9355		1.4098	0.5588	0.6667	0.3000		1.0180	0.5588	0.6667	0.3000
510	7.3098e-04		0.1809	0.8713	0.8714	0.8710		1.8694	0.5000	0.5000	0.5000		1.5110	0.5000	0.5417	0.4000
515	7.3098e-04		0.0562	0.9901	0.9857	1.0000		1.7386	0.5882	0.6667	0.4000		1.1638	0.6176	0.7500	0.3000
520	6.9443e-04		0.0978	0.9307	0.9571	0.8710		1.7885	0.4706	0.4583	0.5000		1.1059	0.5000	0.5833	0.3000
525	6.9443e-04		0.0662	0.9406	0.9286	0.9677		1.7394	0.5882	0.6667	0.4000		0.9494	0.6176	0.6667	0.5000
530	6.5971e-04		0.1003	0.9208	0.9429	0.8710		1.9666	0.5882	0.6250	0.5000		1.1311	0.6471	0.7083	0.5000
535	6.5971e-04		0.1018	0.9307	0.9000	1.0000		2.0505	0.5000	0.5000	0.5000		1.2654	0.5294	0.6250	0.3000
540	6.2672e-04		0.0466	0.9604	0.9571	0.9677		2.0441	0.5882	0.6667	0.4000		1.1790	0.6176	0.7083	0.4000
545	6.2672e-04		0.0502	0.9703	0.9857	0.9355		2.1119	0.5294	0.5417	0.5000		1.2124	0.5588	0.5833	0.5000
550	5.9539e-04		0.0918	0.9604	0.9857	0.9032		2.2794	0.6471	0.7917	0.3000		1.4084	0.6471	0.7917	0.3000
555	5.9539e-04		0.0656	0.9604	0.9429	1.0000		2.1058	0.5882	0.6667	0.4000		1.3984	0.6471	0.7083	0.5000
560	5.6562e-04		0.0307	0.9901	1.0000	0.9677		2.2153	0.5000	0.5000	0.5000		1.5898	0.5294	0.5833	0.4000
565	5.6562e-04		0.1028	0.9406	0.9429	0.9355		2.4460	0.5294	0.5833	0.4000		1.6138	0.6471	0.7083	0.5000
570	5.3734e-04		0.0266	0.9802	0.9714	1.0000		2.3764	0.5294	0.5417	0.5000		1.2657	0.6471	0.6667	0.6000
575	5.3734e-04		0.0617	0.9703	0.9857	0.9355		2.5475	0.5882	0.6250	0.5000		1.7185	0.6176	0.7083	0.4000
580	5.1047e-04		0.0717	0.9406	0.9143	1.0000		1.9653	0.5294	0.5833	0.4000		1.4697	0.5588	0.6250	0.4000
585	5.1047e-04		0.0678	0.9208	0.9143	0.9355		2.0757	0.4706	0.4583	0.5000		1.4482	0.5294	0.6250	0.3000
590	4.8495e-04		0.0225	0.9802	0.9714	1.0000		2.1466	0.5882	0.6250	0.5000		1.4519	0.5882	0.7083	0.3000
595	4.8495e-04		0.0213	0.9703	0.9571	1.0000		2.3051	0.5588	0.6250	0.4000		1.3354	0.6765	0.7917	0.4000
600	4.6070e-04		0.0387	0.9604	0.9429	1.0000		2.1869	0.5588	0.5833	0.5000		1.3532	0.6176	0.7083	0.4000
605	4.6070e-04		0.1468	0.9703	0.9857	0.9355		2.3828	0.5882	0.6667	0.4000		1.6089	0.5588	0.7083	0.2000
610	4.3766e-04		0.0676	0.9505	0.9571	0.9355		2.3443	0.5294	0.5417	0.5000		1.6444	0.5000	0.5833	0.3000
615	4.3766e-04		0.0630	0.9208	0.9000	0.9677		2.1119	0.5882	0.6667	0.4000		1.4021	0.6176	0.7500	0.3000
620	4.1578e-04		0.0418	0.9703	0.9571	1.0000		2.0722	0.7059	0.7917	0.5000		1.3936	0.5588	0.7500	0.1000
625	4.1578e-04		0.1568	0.9505	0.9714	0.9032		2.2377	0.5294	0.5417	0.5000		1.3963	0.6176	0.6667	0.5000
630	3.9499e-04		0.0609	0.9802	0.9857	0.9677		2.1363	0.5588	0.5833	0.5000		1.4061	0.5882	0.7083	0.3000
635	3.9499e-04		0.0523	0.9901	0.9857	1.0000		2.1877	0.5588	0.5833	0.5000		1.5573	0.5588	0.6667	0.3000
640	3.7524e-04		0.0479	0.9703	0.9714	0.9677		2.2841	0.5000	0.5000	0.5000		1.6234	0.5294	0.5833	0.4000
645	3.7524e-04		0.0588	0.9505	0.9429	0.9677		2.3392	0.5882	0.6667	0.4000		1.5734	0.6471	0.7500	0.4000
650	3.5648e-04		0.0266	0.9802	0.9857	0.9677		2.2102	0.5000	0.5000	0.5000		1.4776	0.5882	0.6667	0.4000
655	3.5648e-04		0.0278	0.9703	0.9714	0.9677		2.2994	0.5000	0.5000	0.5000		1.5793	0.5000	0.5833	0.3000
660	3.3866e-04		0.0275	0.9802	0.9857	0.9677		2.3728	0.4706	0.4583	0.5000		1.4585	0.5294	0.5833	0.4000
665	3.3866e-04		0.0391	0.9802	0.9857	0.9677		2.5098	0.5000	0.5000	0.5000		1.6038	0.5000	0.5417	0.4000
670	3.2172e-04		0.0311	0.9802	0.9857	0.9677		2.4478	0.5588	0.6250	0.4000		1.4681	0.6471	0.7083	0.5000
675	3.2172e-04		0.1578	0.9703	0.9714	0.9677		2.5229	0.5882	0.6667	0.4000		1.5618	0.6471	0.7083	0.5000
680	3.0564e-04		0.1533	0.9109	0.9000	0.9355		2.0853	0.5294	0.6250	0.3000		0.9512	0.7353	0.7500	0.7000
685	3.0564e-04		0.0360	0.9802	0.9857	0.9677		1.9210	0.5000	0.5000	0.5000		1.6002	0.5000	0.5417	0.4000
690	2.9035e-04		0.0351	0.9703	0.9857	0.9355		1.9982	0.5588	0.5833	0.5000		1.5781	0.5294	0.5833	0.4000
695	2.9035e-04		0.1774	0.9406	0.9571	0.9032		2.2905	0.5000	0.5000	0.5000		1.8182	0.4706	0.5000	0.4000
700	2.7584e-04		0.0679	0.9505	0.9429	0.9677		2.2586	0.5588	0.6250	0.4000		1.2626	0.6471	0.7500	0.4000
705	2.7584e-04		0.0214	0.9802	0.9714	1.0000		2.1198	0.5294	0.5417	0.5000		1.4151	0.5000	0.5000	0.5000
710	2.6205e-04		0.0368	0.9703	0.9857	0.9355		2.1690	0.5294	0.5417	0.5000		1.3448	0.5588	0.5833	0.5000
715	2.6205e-04		0.0579	0.9604	0.9571	0.9677		2.2522	0.5294	0.5417	0.5000		1.5478	0.5294	0.5833	0.4000
720	2.4894e-04		0.1059	0.9604	0.9429	1.0000		2.2060	0.4706	0.4583	0.5000		1.4629	0.4706	0.5000	0.4000
725	2.4894e-04		0.0313	0.9901	0.9857	1.0000		2.3006	0.5588	0.6250	0.4000		1.3953	0.6471	0.7500	0.4000
730	2.3650e-04		0.0286	0.9802	0.9857	0.9677		2.4112	0.5294	0.5833	0.4000		1.2881	0.5588	0.6250	0.4000
735	2.3650e-04		0.0590	0.9703	0.9714	0.9677		2.0642	0.4412	0.4583	0.4000		1.3945	0.5000	0.5833	0.3000
740	2.2467e-04		0.0849	0.9307	0.9286	0.9355		2.2642	0.4706	0.4583	0.5000		1.6009	0.5000	0.5000	0.5000
745	2.2467e-04		0.0238	0.9901	0.9857	1.0000		2.1213	0.4706	0.4583	0.5000		1.2438	0.5588	0.5833	0.5000
750	2.1344e-04		0.0107	1.0000	1.0000	1.0000		2.3698	0.5294	0.5833	0.4000		1.3749	0.6176	0.7083	0.4000


 training loss less than 0.02, Program exit.
