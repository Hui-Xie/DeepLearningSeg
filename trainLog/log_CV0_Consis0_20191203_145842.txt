=============training from sratch============
Program ID: 7898

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/full3DGTPlus90%/training/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/full3DGTPlus90%/training/labels_npy', '0', '1', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
      Oct 23th, 2019
      1   change to MaxPool with 2*2*2 with stride 2;
      2   Loss use GeneralizedDiceLoss function;
      
      Oct 25th, 2019
      1   add trainAllData switch
      
      Oct 29th, 2019
      1   revise reduce learnging rate at training loss;
      2   reduce min_lr = 1e-9
      
      Nov 1st, 2019
      1   erase batchNorm, for training.
      
      Nov 2nd, 2019
      1   Use InstanceNormalization, instead of BatchNorm
      
      Nov 25th, 2019
      1   Add boundary loss into training.  
      
      Dec 3rd, 2019
      1  cancel Boundary Loss;
      2  add 13 network generated files with accuracy >88% into 3D ground truth, then train;
      
      
        
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-12-03 14:58:42.086844
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191203_145842

Info: this is the 0th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191203_145842.
program re-initializes all input files list, which will lead previous all K_fold cross validation invalid.
All files are in one partition.

all dataset: total 48 image files.
Network has total 254,616,050 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		0.6183		0.36785
5	1.0000e-02		0.4566		0.50559
10	1.0000e-02		0.4194		0.54860
15	1.0000e-02		0.4061		0.55049
20	1.0000e-02		0.4152		0.55826
25	1.0000e-02		0.3852		0.58904
30	1.0000e-02		0.3663		0.60236
35	1.0000e-02		0.3620		0.61188
40	1.0000e-02		0.3313		0.64067
45	1.0000e-02		0.3109		0.66394
50	1.0000e-02		0.3277		0.64645
55	1.0000e-02		0.3057		0.67441
60	1.0000e-02		0.3154		0.66583
65	1.0000e-02		0.2816		0.70164
70	1.0000e-02		0.2413		0.74476
75	1.0000e-02		0.2729		0.70816
80	1.0000e-02		0.2333		0.75372
85	1.0000e-02		0.2771		0.70422
90	1.0000e-02		0.2609		0.71916
95	1.0000e-02		0.2414		0.74439
100	1.0000e-02		0.2628		0.72094
105	1.0000e-02		0.2809		0.70011
110	1.0000e-02		0.2127		0.77171
115	1.0000e-02		0.2243		0.76297
120	1.0000e-02		0.2248		0.75771
125	1.0000e-02		0.2002		0.78398
130	1.0000e-02		0.2054		0.78297
135	1.0000e-02		0.1881		0.79892
140	1.0000e-02		0.2673		0.71257
145	1.0000e-02		0.2319		0.75498
150	1.0000e-02		0.2120		0.77560
155	1.0000e-02		0.2257		0.75572
160	1.0000e-02		0.2532		0.72843
165	1.0000e-02		0.1767		0.81233
170	1.0000e-02		0.1762		0.81195
175	1.0000e-02		0.2084		0.77999
180	1.0000e-02		0.1746		0.81428
185	1.0000e-02		0.1757		0.81245
190	1.0000e-02		0.1905		0.79680
195	1.0000e-02		0.1932		0.79317
200	1.0000e-02		0.1797		0.80903
205	1.0000e-02		0.2087		0.77599
210	1.0000e-02		0.2170		0.77078
215	1.0000e-02		0.1841		0.79675
220	1.0000e-02		0.1608		0.82873
225	1.0000e-02		0.1685		0.81929
230	1.0000e-02		0.1716		0.81060
235	1.0000e-02		0.1620		0.82811
240	1.0000e-02		0.1633		0.82798
245	1.0000e-02		0.1582		0.83092
250	1.0000e-02		0.1513		0.83777
255	1.0000e-02		0.1694		0.81882
260	1.0000e-02		0.1737		0.81648
265	1.0000e-02		0.1554		0.83357
270	1.0000e-02		0.1600		0.83050
275	1.0000e-02		0.2120		0.77661
280	1.0000e-02		0.1518		0.83813
285	1.0000e-02		0.1587		0.83285
290	1.0000e-02		0.1508		0.84012
295	1.0000e-02		0.1617		0.82858
300	1.0000e-02		0.1586		0.83237
305	1.0000e-02		0.1367		0.85425
310	1.0000e-02		0.1747		0.81540
315	1.0000e-02		0.1524		0.83537
320	1.0000e-02		0.1232		0.86908
325	1.0000e-02		0.1376		0.85322
330	1.0000e-02		0.1464		0.84178
335	1.0000e-02		0.1106		0.88069
340	1.0000e-02		0.1404		0.84867
345	1.0000e-02		0.1429		0.84673
350	1.0000e-02		0.1318		0.85897
355	1.0000e-02		0.1695		0.82159
360	1.0000e-02		0.1218		0.87009
365	1.0000e-02		0.1476		0.84453
370	1.0000e-02		0.1235		0.86837
375	1.0000e-02		0.1174		0.87415
380	1.0000e-02		0.1432		0.84722
385	1.0000e-02		0.1393		0.85249
390	1.0000e-03		0.1594		0.83296
395	1.0000e-03		0.1184		0.87273
400	1.0000e-03		0.1138		0.87641
405	1.0000e-03		0.0989		0.89374
410	1.0000e-03		0.0953		0.89780
415	1.0000e-03		0.0983		0.89501
420	1.0000e-03		0.1018		0.89191
425	1.0000e-03		0.0925		0.90185
430	1.0000e-03		0.0947		0.89910
435	1.0000e-03		0.0923		0.90183
440	1.0000e-03		0.0960		0.89787
445	1.0000e-03		0.0937		0.89918
450	1.0000e-03		0.0872		0.90680
455	1.0000e-03		0.0907		0.90352
460	1.0000e-03		0.0939		0.89916
465	1.0000e-03		0.0986		0.89580
470	1.0000e-03		0.0858		0.90799
475	1.0000e-03		0.0821		0.91234
480	1.0000e-03		0.0879		0.90591
485	1.0000e-03		0.0874		0.90632
490	1.0000e-03		0.0958		0.89519
495	1.0000e-03		0.0887		0.90480
500	1.0000e-03		0.0873		0.90688
505	1.0000e-03		0.0851		0.90876
510	1.0000e-03		0.0862		0.90775
515	1.0000e-03		0.0883		0.90516
520	1.0000e-03		0.0788		0.91586
525	1.0000e-03		0.0898		0.90287
530	1.0000e-03		0.0818		0.91224
535	1.0000e-03		0.0886		0.90550
540	1.0000e-03		0.0881		0.90675
545	1.0000e-03		0.0878		0.90574
550	1.0000e-03		0.0809		0.91359
555	1.0000e-03		0.0853		0.90870
560	1.0000e-03		0.0820		0.91276
565	1.0000e-03		0.0830		0.91131
570	1.0000e-03		0.0831		0.91112
575	1.0000e-04		0.0839		0.91015
580	1.0000e-04		0.0800		0.91479
585	1.0000e-04		0.0794		0.91529
590	1.0000e-04		0.0787		0.91623
595	1.0000e-04		0.0780		0.91704
600	1.0000e-04		0.0851		0.90940
605	1.0000e-04		0.0836		0.91117
610	1.0000e-04		0.0763		0.91805
615	1.0000e-04		0.0848		0.90991
620	1.0000e-04		0.0793		0.91504
625	1.0000e-04		0.0816		0.91104
630	1.0000e-04		0.0769		0.91815
635	1.0000e-04		0.0785		0.91595
640	1.0000e-04		0.0792		0.91576
645	1.0000e-04		0.0852		0.90811
650	1.0000e-04		0.0764		0.91842
655	1.0000e-04		0.0752		0.91949
660	1.0000e-04		0.0792		0.91550
665	1.0000e-04		0.0755		0.91943
670	1.0000e-04		0.0802		0.91416
675	1.0000e-04		0.0817		0.91337
680	1.0000e-04		0.0752		0.91925
685	1.0000e-04		0.0835		0.91020
690	1.0000e-04		0.0767		0.91767
695	1.0000e-04		0.0778		0.91659
700	1.0000e-04		0.0732		0.92152
705	1.0000e-04		0.0792		0.91562
710	1.0000e-04		0.0769		0.91808
715	1.0000e-04		0.0817		0.91252
720	1.0000e-04		0.0802		0.91434
725	1.0000e-04		0.0749		0.91989
730	1.0000e-04		0.0760		0.91914
735	1.0000e-04		0.0753		0.91921
740	1.0000e-04		0.0771		0.91812
745	1.0000e-04		0.0797		0.91408
750	1.0000e-04		0.0765		0.91774
755	1.0000e-05		0.0794		0.91528
760	1.0000e-05		0.0791		0.91538
765	1.0000e-05		0.0833		0.91051
770	1.0000e-05		0.0830		0.91208
775	1.0000e-05		0.0804		0.91379
780	1.0000e-05		0.0794		0.91568
785	1.0000e-05		0.0754		0.91929
790	1.0000e-05		0.0738		0.92114
795	1.0000e-05		0.0753		0.91928
800	1.0000e-05		0.0783		0.91579
805	1.0000e-05		0.0793		0.91613
810	1.0000e-06		0.0760		0.91817
815	1.0000e-06		0.0776		0.91707
820	1.0000e-06		0.0765		0.91845
825	1.0000e-06		0.0793		0.91603
830	1.0000e-06		0.0786		0.91657
835	1.0000e-06		0.0819		0.91195
840	1.0000e-06		0.0778		0.91712
845	1.0000e-06		0.0801		0.91432
850	1.0000e-06		0.0760		0.91902
855	1.0000e-06		0.0788		0.91636
860	1.0000e-06		0.0799		0.91480
865	1.0000e-07		0.0864		0.90745
870	1.0000e-07		0.0787		0.91566
875	1.0000e-07		0.0760		0.91879
880	1.0000e-07		0.0787		0.91601
885	1.0000e-07		0.0777		0.91650
890	1.0000e-07		0.0773		0.91766
895	1.0000e-07		0.0793		0.91527
900	1.0000e-07		0.0808		0.91343
905	1.0000e-07		0.0829		0.91149
910	1.0000e-07		0.0826		0.91172
915	1.0000e-07		0.0790		0.91565
920	1.0000e-08		0.0795		0.91482
925	1.0000e-08		0.0893		0.90456
930	1.0000e-08		0.0759		0.91886
935	1.0000e-08		0.0790		0.91565
940	1.0000e-08		0.0734		0.92175
945	1.0000e-08		0.0768		0.91793
950	1.0000e-08		0.0722		0.92287
955	1.0000e-08		0.0782		0.91587
960	1.0000e-08		0.0771		0.91774
965	1.0000e-08		0.0712		0.92412
970	1.0000e-08		0.0812		0.91350
975	1.0000e-08		0.0826		0.91232
980	1.0000e-08		0.0776		0.91749
985	1.0000e-08		0.0798		0.91536
990	1.0000e-08		0.0787		0.91602
995	1.0000e-08		0.0780		0.91744
1000	1.0000e-08		0.0804		0.91399
1005	1.0000e-08		0.0842		0.90979
1010	1.0000e-08		0.0785		0.91553
1015	1.0000e-08		0.0741		0.92092
1020	1.0000e-08		0.0753		0.91962
1025	1.0000e-08		0.0782		0.91670
1030	1.0000e-08		0.0799		0.91427
1035	1.0000e-08		0.0764		0.91804
1040	1.0000e-08		0.0853		0.90844
1045	1.0000e-08		0.0795		0.91534
1050	1.0000e-08		0.0746		0.92017
1055	1.0000e-08		0.0766		0.91828
1060	1.0000e-08		0.0789		0.91574
1065	1.0000e-08		0.0748		0.92053
1070	1.0000e-08		0.0757		0.91919
1075	1.0000e-08		0.0754		0.91964
1080	1.0000e-08		0.0842		0.91040
1085	1.0000e-08		0.0793		0.91593
1090	1.0000e-08		0.0777		0.91708
1095	1.0000e-08		0.0754		0.91964
1100	1.0000e-08		0.0789		0.91613
1105	1.0000e-08		0.0782		0.91634
1110	1.0000e-08		0.0787		0.91593
1115	1.0000e-08		0.0798		0.91451
1120	1.0000e-08		0.0727		0.92226
1125	1.0000e-08		0.0789		0.91550
1130	1.0000e-08		0.0728		0.92196
1135	1.0000e-08		0.0810		0.91373
1140	1.0000e-08		0.0771		0.91740
1145	1.0000e-08		0.0809		0.91390
1150	1.0000e-08		0.0738		0.92118
1155	1.0000e-08		0.0785		0.91630
1160	1.0000e-08		0.0804		0.91436
1165	1.0000e-08		0.0792		0.91552
1170	1.0000e-08		0.0804		0.91413
1175	1.0000e-08		0.0743		0.92060
1180	1.0000e-08		0.0812		0.91374
1185	1.0000e-08		0.0739		0.92101
1190	1.0000e-08		0.0766		0.91815
1195	1.0000e-08		0.0778		0.91755
1200	1.0000e-08		0.0837		0.91066
1205	1.0000e-08		0.0804		0.91492
1210	1.0000e-08		0.0797		0.91514
1215	1.0000e-08		0.0820		0.91418
1220	1.0000e-08		0.0857		0.90855
1225	1.0000e-08		0.0749		0.91962
1230	1.0000e-08		0.0781		0.91687
1235	1.0000e-08		0.0802		0.91551
1240	1.0000e-08		0.0729		0.92187
1245	1.0000e-08		0.0751		0.91966
1250	1.0000e-08		0.0792		0.91488
1255	1.0000e-08		0.0786		0.91664
1260	1.0000e-08		0.0783		0.91629
1265	1.0000e-08		0.0722		0.92340
1270	1.0000e-08		0.0798		0.91577
1275	1.0000e-08		0.0766		0.91857
1280	1.0000e-08		0.0808		0.91346
1285	1.0000e-08		0.0791		0.91593
1290	1.0000e-08		0.0808		0.91443
1295	1.0000e-08		0.0795		0.91514
1300	1.0000e-08		0.0767		0.91842
1305	1.0000e-08		0.0780		0.91706
1310	1.0000e-08		0.0774		0.91746
1315	1.0000e-08		0.0760		0.91865
1320	1.0000e-08		0.0818		0.91242
1325	1.0000e-08		0.0765		0.91895
1330	1.0000e-08		0.0811		0.91372
1335	1.0000e-08		0.0767		0.91813
1340	1.0000e-08		0.0778		0.91678
1345	1.0000e-08		0.0753		0.91905
1350	1.0000e-08		0.0739		0.92140
1355	1.0000e-08		0.0756		0.91946
1360	1.0000e-08		0.0780		0.91602
1365	1.0000e-08		0.0790		0.91576
1370	1.0000e-08		0.0751		0.91961
1375	1.0000e-08		0.0774		0.91722
1380	1.0000e-08		0.0762		0.91883
1385	1.0000e-08		0.0815		0.91292
1390	1.0000e-08		0.0795		0.91652
1395	1.0000e-08		0.0764		0.91885
1400	1.0000e-08		0.0846		0.90952
1405	1.0000e-08		0.0812		0.91234
1410	1.0000e-08		0.0800		0.91464
1415	1.0000e-08		0.0800		0.91542
1420	1.0000e-08		0.0784		0.91544
1425	1.0000e-08		0.0793		0.91585
1430	1.0000e-08		0.0731		0.92211
1435	1.0000e-08		0.0777		0.91700
1440	1.0000e-08		0.0820		0.91235
1445	1.0000e-08		0.0785		0.91581
1450	1.0000e-08		0.0775		0.91730
1455	1.0000e-08		0.0830		0.91105
1460	1.0000e-08		0.0769		0.91803
1465	1.0000e-08		0.0790		0.91552
1470	1.0000e-08		0.0789		0.91594
1475	1.0000e-08		0.0761		0.91858
1480	1.0000e-08		0.0809		0.91379
1485	1.0000e-08		0.0807		0.91343
1490	1.0000e-08		0.0760		0.91892
1495	1.0000e-08		0.0767		0.91754
1500	1.0000e-08		0.0792		0.91526
1505	1.0000e-08		0.0778		0.91701
1510	1.0000e-08		0.0759		0.91926
1515	1.0000e-08		0.0725		0.92256
1520	1.0000e-08		0.0856		0.90931
1525	1.0000e-08		0.0759		0.91861
