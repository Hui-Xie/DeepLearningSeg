=============training from sratch============
Program ID: 12847

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/training/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/training/labels_npy', '0', '2', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
      Oct 23th, 2019
      1   change to MaxPool with 2*2*2 with stride 2;
      2   Loss use GeneralizedDiceLoss function;
      
      Oct 25th, 2019
      1   add trainAllData switch
      
      Oct 29th, 2019
      1   revise reduce learnging rate at training loss;
      2   reduce min_lr = 1e-9
      
      Nov 1st, 2019
      1   erase batchNorm, for training.
      
      Nov 2nd, 2019
      1   Use InstanceNormalization, instead of BatchNorm
      
      Nov 25th, 2019
      1   Add boundary loss into training.  
      
      Dec 3rd, 2019
      1  cancel Boundary Loss;
      2  add 13 network generated files with accuracy >88% into 3D ground truth, then train;
      
      Dec 7th, 2019
      1  Change latent vector in the V model into 1*1 in feature map size.
      
      Dce 9th, 2019
      1  Instance Norm can not apply for one value per channel, which will make the value =0
      
      Dec 10th, 2019
      1   In the uppooling4 and 5, where inputsize=1, use Deconv instead upsample,
          in order to avoid repeat of interpolation.
        
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-12-10 02:46:07.602783
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191210_024607

Info: this is the 0th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191210_024607.
All files are in one partition.

all dataset: total 35 image files.
Network has total 258,452,978 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		0.7471		0.24273
5	1.0000e-02		0.5715		0.37646
10	1.0000e-02		0.5102		0.43926
15	1.0000e-02		0.5005		0.45301
20	1.0000e-02		0.4955		0.47376
25	1.0000e-02		0.4698		0.48073
30	1.0000e-02		0.4677		0.47228
35	1.0000e-02		0.4531		0.49710
40	1.0000e-02		0.4190		0.54859
45	1.0000e-02		0.3856		0.58003
50	1.0000e-02		0.3801		0.58751
55	1.0000e-02		0.4181		0.54796
60	1.0000e-02		0.4008		0.55842
65	1.0000e-02		0.4226		0.54374
70	1.0000e-02		0.3566		0.61911
75	1.0000e-02		0.3573		0.61369
80	1.0000e-02		0.3179		0.65799
85	1.0000e-02		0.3196		0.65379
90	1.0000e-02		0.3441		0.63203
95	1.0000e-02		0.3168		0.66189
100	1.0000e-02		0.2852		0.69391
105	1.0000e-02		0.2807		0.70461
110	1.0000e-02		0.3137		0.67176
115	1.0000e-02		0.3261		0.65367
120	1.0000e-02		0.3085		0.67425
125	1.0000e-02		0.3114		0.66822
130	1.0000e-02		0.2760		0.70541
135	1.0000e-02		0.3026		0.67497
140	1.0000e-02		0.3400		0.63927
145	1.0000e-02		0.2900		0.69300
150	1.0000e-02		0.2375		0.74803
155	1.0000e-02		0.2868		0.69522
160	1.0000e-02		0.2429		0.74359
165	1.0000e-02		0.2586		0.72237
170	1.0000e-02		0.2727		0.71043
175	1.0000e-02		0.2587		0.72558
180	1.0000e-02		0.2661		0.71651
185	1.0000e-02		0.2194		0.76556
190	1.0000e-02		0.2625		0.71098
195	1.0000e-02		0.2474		0.73468
200	1.0000e-02		0.2367		0.74957
205	1.0000e-02		0.2608		0.72326
210	1.0000e-02		0.2203		0.76678
215	1.0000e-02		0.2225		0.76466
220	1.0000e-02		0.2722		0.71484
225	1.0000e-02		0.1909		0.79431
230	1.0000e-02		0.2054		0.78261
235	1.0000e-02		0.2416		0.74336
240	1.0000e-02		0.2134		0.76700
245	1.0000e-02		0.2009		0.78631
250	1.0000e-02		0.2118		0.77449
255	1.0000e-02		0.1844		0.80231
260	1.0000e-02		0.1794		0.80958
265	1.0000e-02		0.2236		0.76037
270	1.0000e-02		0.1915		0.79452
275	1.0000e-02		0.1898		0.79998
280	1.0000e-02		0.2204		0.76486
285	1.0000e-02		0.1718		0.81597
290	1.0000e-02		0.2006		0.78846
295	1.0000e-02		0.1806		0.80913
300	1.0000e-02		0.1860		0.80344
305	1.0000e-02		0.1629		0.82832
310	1.0000e-02		0.1764		0.81392
315	1.0000e-02		0.2116		0.77591
320	1.0000e-02		0.1867		0.80108
325	1.0000e-02		0.1892		0.80078
330	1.0000e-02		0.1757		0.81372
335	1.0000e-02		0.1582		0.83267
340	1.0000e-02		0.2088		0.77756
345	1.0000e-02		0.1599		0.83129
350	1.0000e-02		0.1367		0.85468
355	1.0000e-02		0.1764		0.81450
360	1.0000e-02		0.1768		0.81351
365	1.0000e-02		0.1506		0.84115
370	1.0000e-02		0.1467		0.84501
375	1.0000e-02		0.1385		0.85372
380	1.0000e-02		0.1562		0.83115
385	1.0000e-02		0.1504		0.84066
390	1.0000e-02		0.1673		0.82337
395	1.0000e-02		0.1721		0.81849
400	1.0000e-02		0.1464		0.84518
405	1.0000e-02		0.1312		0.86048
410	1.0000e-02		0.1594		0.83092
415	1.0000e-02		0.1335		0.85886
420	1.0000e-02		0.1347		0.85699
425	1.0000e-02		0.1451		0.84513
430	1.0000e-02		0.1319		0.85699
435	1.0000e-02		0.1264		0.86676
440	1.0000e-02		0.1198		0.87311
445	1.0000e-02		0.1092		0.88414
450	1.0000e-02		0.1446		0.84543
455	1.0000e-02		0.1301		0.86292
460	1.0000e-02		0.1293		0.86280
465	1.0000e-02		0.1622		0.83021
470	1.0000e-02		0.1218		0.86994
475	1.0000e-02		0.1428		0.84894
480	1.0000e-02		0.1180		0.87438
485	1.0000e-02		0.1191		0.87340
490	1.0000e-02		0.1566		0.83609
495	1.0000e-02		0.1732		0.81848
500	1.0000e-03		0.1352		0.85581
505	1.0000e-03		0.1041		0.88911
510	1.0000e-03		0.1026		0.89136
515	1.0000e-03		0.1079		0.88465
520	1.0000e-03		0.0953		0.89890
525	1.0000e-03		0.0938		0.89981
530	1.0000e-03		0.1011		0.89210
535	1.0000e-03		0.0981		0.89615
540	1.0000e-03		0.0966		0.89828
545	1.0000e-03		0.0898		0.90495
550	1.0000e-03		0.1020		0.89261
555	1.0000e-03		0.0922		0.90198
560	1.0000e-03		0.0929		0.90126
565	1.0000e-03		0.0964		0.89791
570	1.0000e-03		0.0944		0.89953
575	1.0000e-03		0.0945		0.89994
580	1.0000e-03		0.0870		0.90764
585	1.0000e-03		0.0864		0.90858
590	1.0000e-03		0.0858		0.90866
595	1.0000e-03		0.0881		0.90645
600	1.0000e-03		0.0830		0.91145
605	1.0000e-03		0.0893		0.90516
610	1.0000e-03		0.0903		0.90444
615	1.0000e-03		0.0848		0.91024
620	1.0000e-03		0.0833		0.91144
625	1.0000e-03		0.0856		0.90843
630	1.0000e-03		0.0830		0.91230
635	1.0000e-03		0.0862		0.90836
640	1.0000e-03		0.0880		0.90685
645	1.0000e-03		0.0757		0.91911
650	1.0000e-03		0.0887		0.90625
655	1.0000e-03		0.0872		0.90676
660	1.0000e-03		0.0835		0.91175
665	1.0000e-03		0.0796		0.91470
670	1.0000e-03		0.0818		0.91273
675	1.0000e-03		0.0787		0.91633
680	1.0000e-03		0.0811		0.91380
685	1.0000e-03		0.0858		0.90896
690	1.0000e-03		0.0803		0.91517
695	1.0000e-03		0.0737		0.92138
700	1.0000e-03		0.0800		0.91488
705	1.0000e-03		0.0794		0.91539
710	1.0000e-03		0.0822		0.91392
715	1.0000e-03		0.0802		0.91516
720	1.0000e-03		0.0755		0.91993
725	1.0000e-03		0.0798		0.91519
730	1.0000e-03		0.0737		0.92140
735	1.0000e-03		0.0768		0.91820
740	1.0000e-03		0.0737		0.92219
745	1.0000e-03		0.0737		0.92223
750	1.0000e-03		0.0738		0.92159
755	1.0000e-03		0.0835		0.91169
760	1.0000e-03		0.0766		0.91897
765	1.0000e-03		0.0818		0.91364
770	1.0000e-03		0.0773		0.91752
775	1.0000e-03		0.0807		0.91509
780	1.0000e-03		0.0762		0.91934
785	1.0000e-03		0.0805		0.91471
790	1.0000e-03		0.0723		0.92322
795	1.0000e-03		0.0741		0.92222
800	1.0000e-03		0.0752		0.92015
805	1.0000e-03		0.0732		0.92285
810	1.0000e-03		0.0753		0.91976
815	1.0000e-03		0.0760		0.91904
820	1.0000e-03		0.0765		0.91976
825	1.0000e-03		0.0751		0.92082
830	1.0000e-03		0.0729		0.92300
835	1.0000e-03		0.0724		0.92347
840	1.0000e-03		0.0704		0.92494
845	1.0000e-03		0.0689		0.92676
850	1.0000e-03		0.0705		0.92533
855	1.0000e-03		0.0765		0.91919
860	1.0000e-03		0.0681		0.92724
865	1.0000e-03		0.0769		0.91789
870	1.0000e-03		0.0683		0.92742
875	1.0000e-03		0.0680		0.92738
880	1.0000e-03		0.0737		0.92170
885	1.0000e-03		0.0691		0.92648
890	1.0000e-03		0.0648		0.93144
895	1.0000e-03		0.0652		0.93010
900	1.0000e-03		0.0718		0.92425
905	1.0000e-03		0.0667		0.92938
910	1.0000e-03		0.0697		0.92643
915	1.0000e-03		0.0710		0.92512
920	1.0000e-03		0.0682		0.92809
925	1.0000e-03		0.0647		0.93172
930	1.0000e-03		0.0695		0.92661
935	1.0000e-03		0.0656		0.93040
940	1.0000e-03		0.0649		0.93098
945	1.0000e-03		0.0651		0.93135
950	1.0000e-03		0.0671		0.92883
955	1.0000e-03		0.0669		0.92908
960	1.0000e-03		0.0670		0.92878
965	1.0000e-03		0.0617		0.93459
970	1.0000e-03		0.0620		0.93423
975	1.0000e-03		0.0646		0.93141
980	1.0000e-03		0.0651		0.93140
985	1.0000e-03		0.0644		0.93210
990	1.0000e-03		0.0690		0.92694
995	1.0000e-03		0.0623		0.93361
1000	1.0000e-03		0.0628		0.93368
1005	1.0000e-03		0.0655		0.93032
1010	1.0000e-03		0.0680		0.92824
1015	1.0000e-03		0.0598		0.93652
1020	1.0000e-03		0.0615		0.93449
1025	1.0000e-03		0.0622		0.93384
1030	1.0000e-03		0.0592		0.93695
1035	1.0000e-03		0.0650		0.93120
1040	1.0000e-03		0.0612		0.93498
1045	1.0000e-03		0.0630		0.93319
1050	1.0000e-03		0.0609		0.93522
1055	1.0000e-03		0.0626		0.93331
1060	1.0000e-03		0.0650		0.93119
1065	1.0000e-03		0.0646		0.93213
1070	1.0000e-03		0.0612		0.93486
1075	1.0000e-03		0.0581		0.93801
1080	1.0000e-03		0.0572		0.93957
1085	1.0000e-03		0.0634		0.93278
1090	1.0000e-03		0.0630		0.93381
1095	1.0000e-03		0.0695		0.92541
1100	1.0000e-03		0.0653		0.93173
1105	1.0000e-03		0.0674		0.92872
1110	1.0000e-03		0.0613		0.93470
1115	1.0000e-03		0.0643		0.93209
1120	1.0000e-03		0.0576		0.93857
1125	1.0000e-03		0.0604		0.93609
1130	1.0000e-03		0.0664		0.92952
1135	1.0000e-04		0.0577		0.93833
1140	1.0000e-04		0.0577		0.93910
1145	1.0000e-04		0.0575		0.93912
1150	1.0000e-04		0.0634		0.93286
1155	1.0000e-04		0.0603		0.93615
1160	1.0000e-04		0.0572		0.93910
1165	1.0000e-04		0.0560		0.94012
1170	1.0000e-04		0.0617		0.93501
1175	1.0000e-04		0.0590		0.93789
1180	1.0000e-04		0.0580		0.93871
1185	1.0000e-04		0.0622		0.93396
1190	1.0000e-04		0.0582		0.93812
1195	1.0000e-04		0.0619		0.93482
1200	1.0000e-04		0.0577		0.93880
1205	1.0000e-04		0.0608		0.93559
1210	1.0000e-04		0.0617		0.93467
1215	1.0000e-04		0.0583		0.93831
1220	1.0000e-05		0.0578		0.93865
1225	1.0000e-05		0.0580		0.93813
1230	1.0000e-05		0.0611		0.93530
1235	1.0000e-05		0.0587		0.93809
1240	1.0000e-05		0.0597		0.93640
1245	1.0000e-05		0.0540		0.94265
1250	1.0000e-05		0.0596		0.93722
1255	1.0000e-05		0.0529		0.94406
1260	1.0000e-05		0.0593		0.93763
1265	1.0000e-05		0.0576		0.93904
1270	1.0000e-05		0.0598		0.93643
1275	1.0000e-05		0.0627		0.93368
1280	1.0000e-05		0.0591		0.93785
1285	1.0000e-05		0.0571		0.93974
1290	1.0000e-05		0.0581		0.93827
1295	1.0000e-05		0.0550		0.94124
1300	1.0000e-05		0.0587		0.93713
1305	1.0000e-05		0.0583		0.93829
1310	1.0000e-06		0.0587		0.93797
1315	1.0000e-06		0.0558		0.94098
1320	1.0000e-06		0.0579		0.93843
1325	1.0000e-06		0.0650		0.93175
1330	1.0000e-06		0.0608		0.93541
1335	1.0000e-06		0.0534		0.94308
1340	1.0000e-06		0.0605		0.93626
1345	1.0000e-06		0.0581		0.93850
1350	1.0000e-06		0.0547		0.94226
1355	1.0000e-06		0.0576		0.93894
1360	1.0000e-06		0.0597		0.93677
1365	1.0000e-07		0.0564		0.94030
1370	1.0000e-07		0.0589		0.93769
1375	1.0000e-07		0.0562		0.94015
1380	1.0000e-07		0.0614		0.93488
1385	1.0000e-07		0.0588		0.93792
1390	1.0000e-07		0.0620		0.93483
1395	1.0000e-07		0.0574		0.93977
1400	1.0000e-07		0.0570		0.93958
1405	1.0000e-07		0.0596		0.93677
1410	1.0000e-07		0.0582		0.93817
1415	1.0000e-07		0.0584		0.93749
1420	1.0000e-08		0.0565		0.94018
1425	1.0000e-08		0.0605		0.93615
1430	1.0000e-08		0.0593		0.93768
1435	1.0000e-08		0.0534		0.94296
1440	1.0000e-08		0.0589		0.93785
1445	1.0000e-08		0.0605		0.93578
1450	1.0000e-08		0.0584		0.93860
1455	1.0000e-08		0.0571		0.94003
1460	1.0000e-08		0.0566		0.94019
1465	1.0000e-08		0.0560		0.94104
1470	1.0000e-08		0.0599		0.93670
1475	1.0000e-08		0.0564		0.94021
1480	1.0000e-08		0.0641		0.93225
1485	1.0000e-08		0.0611		0.93591
1490	1.0000e-08		0.0597		0.93735
1495	1.0000e-08		0.0590		0.93772
1500	1.0000e-08		0.0563		0.94024
1505	1.0000e-08		0.0563		0.94034
1510	1.0000e-08		0.0562		0.94005
1515	1.0000e-08		0.0611		0.93531
1520	1.0000e-08		0.0585		0.93845
1525	1.0000e-08		0.0606		0.93547
1530	1.0000e-08		0.0556		0.94096
1535	1.0000e-08		0.0570		0.93963
1540	1.0000e-08		0.0620		0.93421
1545	1.0000e-08		0.0618		0.93435
1550	1.0000e-08		0.0584		0.93798
1555	1.0000e-08		0.0569		0.93943
1560	1.0000e-08		0.0583		0.93849
1565	1.0000e-08		0.0566		0.93992
1570	1.0000e-08		0.0593		0.93765
1575	1.0000e-08		0.0618		0.93428
1580	1.0000e-08		0.0598		0.93638
1585	1.0000e-08		0.0624		0.93514
1590	1.0000e-08		0.0567		0.93976
1595	1.0000e-08		0.0595		0.93714
1600	1.0000e-08		0.0588		0.93767
1605	1.0000e-08		0.0606		0.93602
1610	1.0000e-08		0.0612		0.93552
1615	1.0000e-08		0.0570		0.93970
1620	1.0000e-08		0.0597		0.93778
1625	1.0000e-08		0.0600		0.93658
1630	1.0000e-08		0.0582		0.93841
1635	1.0000e-08		0.0644		0.93148
1640	1.0000e-08		0.0547		0.94188
1645	1.0000e-08		0.0655		0.92999
1650	1.0000e-08		0.0592		0.93742
1655	1.0000e-08		0.0582		0.93871
1660	1.0000e-08		0.0585		0.93791
1665	1.0000e-08		0.0599		0.93638
1670	1.0000e-08		0.0635		0.93455
1675	1.0000e-08		0.0579		0.93865
1680	1.0000e-08		0.0597		0.93676
1685	1.0000e-08		0.0557		0.94122
1690	1.0000e-08		0.0581		0.93791
1695	1.0000e-08		0.0582		0.93856
