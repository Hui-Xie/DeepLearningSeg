=============training from sratch============
Program ID: 28644

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/images_npy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '1', '1']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
            8    at July 30th 03:00, 2019:
                    add learning rate print;
                    use convStride =2;
                    add filter number by 2 times along deeper layers.
            9    at July 30th, 10:13, 2019:
                    add MaxPool2d in stage1;
                    add final filters to 2048.
            10   at July 30th, 15:23, 2019
                    final conv layer filter number: 1024
                                                 
                    
            
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   training data has 169 patients 

Training strategy: 

          

Program starting Time: 2019-07-30 15:26:55.117039
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten/20190730_152655

Info: this is the 1th fold leave for test in the 4-fold cross-validation, with 20.0% of data for independent test.

Infor: In all data of 169 files, label 0 has 51 files,
	  and label 1 has 118 files, where positive response rate = 0.6982248520710059 in full data
Infor: the independent test set has 34 files,and Training including validation Set has 135 files which will be divided into 4 folds.
In the independent test set of 34 files, 0 has 10, 1 has 24, the rate of 1s is 0.7058823529411765
In this test_1 dataset, total 34 files, where 1 has 24 with rate of 0.7058823529411765
In this train_1 dataset, total 101 files, where 1 has 70 with rate of 0.693069306930693
In this validation_1 dataset, total 34 files, where 1 has 24 with rate of 0.7058823529411765
=== Network trains from scratch ====
Network has total 90,343,924 parameters.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	LearningRate		TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	1.0000e-02		1.1644	0.5248	0.5571	0.4516		0.4144	0.2941	0.0000	1.0000		0.4143	0.2941	0.0000	1.0000
5	1.0000e-02		2.1694	0.4851	0.4429	0.5806		5.0386	0.6471	0.9167	0.0000		7.0334	0.7059	1.0000	0.0000
10	9.5000e-03		2.3448	0.4257	0.4143	0.4516		2.9793	0.4412	0.2500	0.9000		5.7759	0.2059	0.0833	0.5000
15	9.5000e-03		1.3800	0.4653	0.3857	0.6452		11.9510	0.6176	0.8750	0.0000		28.9490	0.6765	0.9167	0.1000
20	9.0250e-03		2.6112	0.4356	0.3857	0.5484		0.4685	0.3235	0.0417	1.0000		0.7641	0.2647	0.0833	0.7000
25	9.0250e-03		0.9400	0.5248	0.5000	0.5806		0.9059	0.5588	0.7500	0.1000		0.4055	0.6765	0.7917	0.4000
30	8.5737e-03		0.6787	0.4356	0.3286	0.6774		0.8349	0.6176	0.8333	0.1000		0.3692	0.7647	0.9167	0.4000
35	8.5737e-03		0.4331	0.3960	0.3000	0.6129		0.4117	0.4118	0.2500	0.8000		0.4031	0.2941	0.0833	0.8000
40	8.1451e-03		0.4774	0.3960	0.3286	0.5484		0.4022	0.7059	1.0000	0.0000		0.4340	0.7059	0.9583	0.1000
45	8.1451e-03		0.4477	0.3960	0.2000	0.8387		0.4555	0.4412	0.3750	0.6000		0.4022	0.5882	0.5000	0.8000
50	7.7378e-03		0.4179	0.6337	0.8714	0.0968		0.4074	0.6765	0.9583	0.0000		0.4183	0.6765	0.9583	0.0000
55	7.7378e-03		0.4341	0.3861	0.2429	0.7097		0.4136	0.3235	0.0417	1.0000		0.4477	0.2647	0.0000	0.9000
60	7.3509e-03		0.4294	0.5050	0.5286	0.4516		0.4245	0.6765	0.9167	0.1000		0.3736	0.7647	1.0000	0.2000
65	7.3509e-03		0.4396	0.3663	0.1857	0.7742		0.4252	0.4412	0.3333	0.7000		0.3927	0.4412	0.2917	0.8000
70	6.9834e-03		0.4251	0.3168	0.0857	0.8387		0.4047	0.4412	0.2500	0.9000		0.4057	0.4706	0.3333	0.8000
75	6.9834e-03		0.4333	0.6436	0.8000	0.2903		0.4038	0.6471	0.8750	0.1000		0.5305	0.5882	0.8333	0.0000
80	6.6342e-03		0.4311	0.4158	0.3857	0.4839		0.4269	0.5000	0.6667	0.1000		0.3743	0.5588	0.6250	0.4000
85	6.6342e-03		0.4231	0.6238	0.7857	0.2581		0.4107	0.6471	0.8750	0.1000		0.3954	0.7353	0.9167	0.3000
90	6.3025e-03		0.4449	0.3762	0.1571	0.8710		0.4221	0.3235	0.1250	0.8000		0.3825	0.4118	0.1667	1.0000
95	6.3025e-03		0.4231	0.3663	0.1571	0.8387		0.4103	0.4118	0.2500	0.8000		0.3881	0.4706	0.3333	0.8000
100	5.9874e-03		0.4214	0.4554	0.4143	0.5484		0.4108	0.4118	0.2500	0.8000		0.3821	0.4412	0.2083	1.0000
105	5.9874e-03		0.4149	0.6436	0.8429	0.1935		0.4022	0.7353	0.9583	0.2000		0.4185	0.6471	0.8750	0.1000
110	5.6880e-03		0.3998	0.5743	0.6000	0.5161		0.4388	0.6176	0.7917	0.2000		0.3668	0.6765	0.7917	0.4000
115	5.6880e-03		0.4188	0.3663	0.1571	0.8387		0.4106	0.4118	0.1667	1.0000		0.3925	0.3824	0.1667	0.9000
120	5.4036e-03		0.4248	0.3267	0.1143	0.8065		0.4178	0.2941	0.0000	1.0000		0.3839	0.3235	0.0417	1.0000
125	5.4036e-03		0.4307	0.5545	0.6286	0.3871		0.4315	0.5882	0.7917	0.1000		0.3762	0.6471	0.7500	0.4000
130	5.1334e-03		0.4464	0.5743	0.7143	0.2581		0.4283	0.5000	0.5833	0.3000		0.3810	0.5588	0.6250	0.4000
135	5.1334e-03		0.4122	0.4059	0.1857	0.9032		0.4653	0.2941	0.0000	1.0000		0.3809	0.2941	0.0000	1.0000
140	4.8767e-03		0.4176	0.6139	0.7571	0.2903		0.4135	0.6176	0.8333	0.1000		0.3936	0.6176	0.7083	0.4000
145	4.8767e-03		0.4182	0.3465	0.1143	0.8710		0.4322	0.3235	0.0417	1.0000		0.3753	0.3235	0.0417	1.0000
150	4.6329e-03		0.4192	0.6139	0.7571	0.2903		0.4299	0.5588	0.7500	0.1000		0.3756	0.5588	0.6250	0.4000
155	4.6329e-03		0.4194	0.5644	0.5286	0.6452		0.4218	0.5294	0.6667	0.2000		0.3779	0.6176	0.7500	0.3000
160	4.4013e-03		0.4099	0.4950	0.4714	0.5484		0.4482	0.5294	0.6250	0.3000		0.3709	0.5294	0.5833	0.4000
165	4.4013e-03		0.4189	0.6139	0.7857	0.2258		0.4224	0.5294	0.7083	0.1000		0.3783	0.6176	0.7083	0.4000
170	4.1812e-03		0.4078	0.6436	0.7286	0.4516		0.4266	0.5294	0.7083	0.1000		0.3762	0.6176	0.7083	0.4000
175	4.1812e-03		0.4016	0.5941	0.7000	0.3548		0.4773	0.5588	0.7083	0.2000		0.3722	0.6176	0.7083	0.4000
180	3.9721e-03		0.4234	0.4950	0.5286	0.4194		0.4083	0.3824	0.2500	0.7000		0.4173	0.2059	0.0000	0.7000
185	3.9721e-03		0.4142	0.5743	0.6143	0.4839		0.4270	0.5882	0.7083	0.3000		0.3856	0.5000	0.5417	0.4000
190	3.7735e-03		0.4134	0.4356	0.4429	0.4194		0.4537	0.5588	0.7083	0.2000		0.3718	0.5588	0.6250	0.4000
195	3.7735e-03		0.4250	0.3960	0.2286	0.7742		0.4239	0.3235	0.1667	0.7000		0.3803	0.5000	0.3333	0.9000
200	3.5849e-03		0.4055	0.3564	0.0857	0.9677		0.4324	0.3529	0.0833	1.0000		0.3800	0.4118	0.2083	0.9000
205	3.5849e-03		0.4154	0.6535	0.7571	0.4194		0.4186	0.5588	0.7083	0.2000		0.3816	0.6176	0.7083	0.4000
210	3.4056e-03		0.4281	0.5545	0.6286	0.3871		0.4541	0.5294	0.7083	0.1000		0.3742	0.5588	0.6250	0.4000
215	3.4056e-03		0.4356	0.3267	0.1143	0.8065		0.4362	0.2941	0.0833	0.8000		0.3751	0.3529	0.0833	1.0000
220	3.2353e-03		0.4199	0.4356	0.2571	0.8387		0.4279	0.3235	0.0833	0.9000		0.3765	0.3235	0.0417	1.0000
225	3.2353e-03		0.4185	0.5347	0.5714	0.4516		0.4340	0.5000	0.6250	0.2000		0.3734	0.5588	0.6250	0.4000
230	3.0736e-03		0.4155	0.3861	0.1429	0.9355		0.4473	0.3529	0.0833	1.0000		0.3720	0.3235	0.0417	1.0000
235	3.0736e-03		0.4074	0.6634	0.8143	0.3226		0.4307	0.5588	0.7083	0.2000		0.3747	0.6176	0.7083	0.4000
240	2.9199e-03		0.4263	0.5545	0.5857	0.4839		0.4385	0.5882	0.7500	0.2000		0.3726	0.7059	0.8333	0.4000
245	2.9199e-03		0.4037	0.6634	0.8143	0.3226		0.4462	0.5588	0.7083	0.2000		0.3698	0.6471	0.7500	0.4000
250	2.7739e-03		0.4213	0.6238	0.7714	0.2903		0.4505	0.5588	0.7083	0.2000		0.3702	0.6176	0.7083	0.4000
255	2.7739e-03		0.4243	0.6238	0.7714	0.2903		0.4395	0.5588	0.7083	0.2000		0.3704	0.6176	0.7083	0.4000
260	2.6352e-03		0.4155	0.4851	0.4429	0.5806		0.4130	0.3529	0.1667	0.8000		0.3913	0.3824	0.1667	0.9000
265	2.6352e-03		0.4153	0.3861	0.2429	0.7097		0.4211	0.2941	0.0417	0.9000		0.3857	0.3529	0.0833	1.0000
270	2.5034e-03		0.4069	0.6139	0.7286	0.3548		0.4536	0.5294	0.6667	0.2000		0.3714	0.5588	0.6250	0.4000
275	2.5034e-03		0.4181	0.6040	0.7571	0.2581		0.4550	0.5588	0.7083	0.2000		0.3708	0.6765	0.7917	0.4000
280	2.3783e-03		0.4232	0.6931	0.8429	0.3548		0.4425	0.5588	0.7083	0.2000		0.3733	0.6765	0.7917	0.4000
285	2.3783e-03		0.4116	0.6238	0.7571	0.3226		0.4531	0.5588	0.7083	0.2000		0.3708	0.6765	0.7917	0.4000
290	2.2594e-03		0.4028	0.4455	0.2714	0.8387		0.4311	0.3824	0.1250	1.0000		0.3799	0.4118	0.2500	0.8000
295	2.2594e-03		0.4173	0.4356	0.3286	0.6774		0.4380	0.3235	0.1250	0.8000		0.3764	0.4706	0.3333	0.8000
300	2.1464e-03		0.4204	0.4752	0.4429	0.5484		0.4420	0.5294	0.6667	0.2000		0.3757	0.7059	0.8750	0.3000
305	2.1464e-03		0.4153	0.6238	0.7571	0.3226		0.4455	0.5588	0.7500	0.1000		0.3758	0.7059	0.8333	0.4000
310	2.0391e-03		0.3973	0.6634	0.8429	0.2581		0.4544	0.5588	0.7500	0.1000		0.3727	0.7059	0.8333	0.4000
315	2.0391e-03		0.4081	0.6436	0.8286	0.2258		0.4370	0.5882	0.7500	0.2000		0.3720	0.7353	0.8750	0.4000
320	1.9371e-03		0.4118	0.6832	0.8857	0.2258		0.4461	0.5294	0.7083	0.1000		0.3711	0.6765	0.8333	0.3000
325	1.9371e-03		0.4254	0.5446	0.6857	0.2258		0.4400	0.5882	0.7083	0.3000		0.3754	0.6765	0.7917	0.4000
330	1.8403e-03		0.3996	0.4158	0.2571	0.7742		0.4471	0.3529	0.0833	1.0000		0.3787	0.3529	0.0833	1.0000
335	1.8403e-03		0.4231	0.4455	0.3857	0.5806		0.4547	0.5294	0.6250	0.3000		0.3803	0.5588	0.5833	0.5000
340	1.7482e-03		0.4126	0.6139	0.7857	0.2258		0.4321	0.5882	0.7083	0.3000		0.3812	0.6471	0.7500	0.4000
345	1.7482e-03		0.4152	0.5941	0.7286	0.2903		0.4548	0.5588	0.6667	0.3000		0.3859	0.6471	0.7500	0.4000
350	1.6608e-03		0.4177	0.6238	0.7714	0.2903		0.4523	0.5588	0.6667	0.3000		0.3861	0.6471	0.7500	0.4000
355	1.6608e-03		0.4273	0.5743	0.7571	0.1613		0.4431	0.5000	0.6667	0.1000		0.3854	0.6471	0.7500	0.4000
360	1.5778e-03		0.4122	0.6337	0.8143	0.2258		0.4441	0.5588	0.7500	0.1000		0.3837	0.6765	0.7917	0.4000
365	1.5778e-03		0.4115	0.6238	0.7857	0.2581		0.4326	0.5294	0.6667	0.2000		0.3855	0.6765	0.7917	0.4000
370	1.4989e-03		0.4141	0.6337	0.7714	0.3226		0.4296	0.5588	0.7083	0.2000		0.3820	0.6765	0.7917	0.4000
375	1.4989e-03		0.4116	0.6238	0.7571	0.3226		0.4355	0.5882	0.7500	0.2000		0.3827	0.6765	0.7917	0.4000
380	1.4240e-03		0.4099	0.6337	0.7286	0.4194		0.4449	0.5588	0.6667	0.3000		0.3857	0.6765	0.7917	0.4000
385	1.4240e-03		0.4289	0.5941	0.7571	0.2258		0.4339	0.5588	0.7083	0.2000		0.3872	0.6765	0.7917	0.4000
390	1.3528e-03		0.4339	0.6634	0.8429	0.2581		0.4545	0.5294	0.6667	0.2000		0.3858	0.6765	0.7917	0.4000
395	1.3528e-03		0.4162	0.6238	0.7286	0.3871		0.4449	0.5000	0.6250	0.2000		0.3858	0.6176	0.7083	0.4000
400	1.2851e-03		0.4136	0.5545	0.7000	0.2258		0.4433	0.5000	0.6250	0.2000		0.3897	0.5882	0.6667	0.4000
405	1.2851e-03		0.4125	0.6832	0.8429	0.3226		0.4472	0.5294	0.6667	0.2000		0.3855	0.6471	0.7500	0.4000
410	1.2209e-03		0.4066	0.5842	0.7000	0.3226		0.4512	0.5588	0.6667	0.3000		0.3869	0.6471	0.7500	0.4000
415	1.2209e-03		0.4119	0.6535	0.7571	0.4194		0.4570	0.5294	0.6250	0.3000		0.3890	0.6765	0.7917	0.4000
420	1.1598e-03		0.4053	0.5941	0.6857	0.3871		0.4612	0.5000	0.6250	0.2000		0.3874	0.6765	0.7917	0.4000
425	1.1598e-03		0.4124	0.6040	0.7429	0.2903		0.4418	0.5294	0.6667	0.2000		0.3859	0.6765	0.7917	0.4000
430	1.1018e-03		0.4054	0.5842	0.7571	0.1935		0.4616	0.5588	0.6667	0.3000		0.3904	0.6765	0.7917	0.4000
435	1.1018e-03		0.4309	0.6139	0.7857	0.2258		0.4471	0.5588	0.6667	0.3000		0.3874	0.6765	0.7917	0.4000
440	1.0467e-03		0.4080	0.6139	0.7571	0.2903		0.4529	0.5588	0.6667	0.3000		0.3884	0.6765	0.7917	0.4000
445	1.0467e-03		0.4043	0.6337	0.8000	0.2581		0.4534	0.5588	0.6667	0.3000		0.3895	0.6471	0.7500	0.4000
450	9.9440e-04		0.4006	0.6535	0.7571	0.4194		0.4596	0.5588	0.6667	0.3000		0.3914	0.6765	0.7917	0.4000
455	9.9440e-04		0.4146	0.6238	0.7429	0.3548		0.4502	0.5294	0.6667	0.2000		0.3860	0.6765	0.7917	0.4000
460	9.4468e-04		0.4207	0.5941	0.7571	0.2258		0.4538	0.5294	0.6667	0.2000		0.3868	0.6765	0.7917	0.4000
465	9.4468e-04		0.4315	0.6040	0.7714	0.2258		0.4404	0.5588	0.7083	0.2000		0.3896	0.6765	0.7917	0.4000
470	8.9745e-04		0.4127	0.6040	0.7429	0.2903		0.4465	0.5294	0.6667	0.2000		0.3895	0.6765	0.7917	0.4000
475	8.9745e-04		0.4056	0.6139	0.7714	0.2581		0.4580	0.5294	0.6667	0.2000		0.3920	0.6765	0.7917	0.4000
480	8.5258e-04		0.4291	0.5842	0.7286	0.2581		0.4399	0.5588	0.7083	0.2000		0.3887	0.6765	0.7917	0.4000
485	8.5258e-04		0.4123	0.6238	0.8000	0.2258		0.4654	0.5294	0.6667	0.2000		0.3947	0.6765	0.7917	0.4000
490	8.0995e-04		0.4107	0.5743	0.7000	0.2903		0.4707	0.5294	0.6250	0.3000		0.3968	0.6765	0.7917	0.4000
495	8.0995e-04		0.4165	0.6535	0.8000	0.3226		0.4692	0.5294	0.6250	0.3000		0.3953	0.6765	0.7917	0.4000
500	7.6945e-04		0.4158	0.5842	0.7286	0.2581		0.4856	0.5588	0.6667	0.3000		0.3943	0.6765	0.7917	0.4000
505	7.6945e-04		0.4064	0.6436	0.7714	0.3548		0.4676	0.5294	0.6667	0.2000		0.3893	0.6765	0.7917	0.4000
510	7.3098e-04		0.3946	0.6139	0.7286	0.3548		0.4805	0.5588	0.6667	0.3000		0.3947	0.6765	0.7917	0.4000
515	7.3098e-04		0.4090	0.6337	0.7429	0.3871		0.4673	0.5294	0.6667	0.2000		0.3932	0.6765	0.7917	0.4000
520	6.9443e-04		0.4069	0.6436	0.8000	0.2903		0.4508	0.5294	0.6667	0.2000		0.3866	0.6765	0.7917	0.4000
525	6.9443e-04		0.4276	0.6139	0.7571	0.2903		0.4531	0.5294	0.6667	0.2000		0.3872	0.6765	0.7917	0.4000
530	6.5971e-04		0.4154	0.5842	0.7286	0.2581		0.4714	0.5294	0.6250	0.3000		0.3913	0.6765	0.7917	0.4000
