=============training from sratch============
Program ID: 3553

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/images_npy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '1', '1']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
                            
                    
            
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   training data has 169 patients 

Training strategy: 

          

Program starting Time: 2019-07-29 16:41:18.661004
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten/20190729_164118

Info: this is the 1th fold leave for test in the 4-fold cross-validation, with 20.0% of data for independent test.

Infor: In all data of 169 files, label 0 has 51 files,
	  and label 1 has 118 files, where positive response rate = 0.6982248520710059 in full data
Infor: the independent test set has 34 files,and Training including validation Set has 135 files which will be divided into 4 folds.
In the independent test set of 34 files, 0 has 10, 1 has 24, the rate of 1s is 0.7058823529411765
In this test_1 dataset, total 34 files, where 1 has 24 with rate of 0.7058823529411765
In this train_1 dataset, total 101 files, where 1 has 70 with rate of 0.693069306930693
In this validation_1 dataset, total 34 files, where 1 has 24 with rate of 0.7058823529411765
=== Network trains from scratch ====
Network has total 647,012 parameters.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	0.4905	0.4554	0.3857	0.6129		2.8929	0.7059	1.0000	0.0000		2.7257	0.7059	1.0000	0.0000
5	0.4887	0.3564	0.2714	0.5484		1.0960	0.3235	0.0417	1.0000		1.1375	0.2353	0.0000	0.8000
10	0.4042	0.6238	0.7857	0.2581		0.3862	0.6176	0.7917	0.2000		0.3760	0.6765	0.7917	0.4000
15	0.4154	0.6535	0.7714	0.3871		0.3900	0.5588	0.6667	0.3000		0.3753	0.6176	0.6667	0.5000
20	0.4052	0.6634	0.8429	0.2581		0.3910	0.5588	0.7500	0.1000		0.3954	0.6471	0.8750	0.1000
25	0.4116	0.5842	0.6857	0.3548		0.4026	0.5882	0.7500	0.2000		0.3582	0.6471	0.7083	0.5000
30	0.4104	0.6337	0.8286	0.1935		0.3806	0.7059	0.9167	0.2000		0.3506	0.7059	0.8333	0.4000
35	0.4038	0.6436	0.7857	0.3226		0.3864	0.6471	0.8333	0.2000		0.3833	0.7647	0.8750	0.5000
40	0.4094	0.6238	0.7714	0.2903		0.3799	0.7059	0.8750	0.3000		0.3706	0.7353	0.8750	0.4000
45	0.4170	0.6535	0.8143	0.2903		0.3783	0.6471	0.8333	0.2000		0.3609	0.7647	0.9167	0.4000
50	0.4031	0.6535	0.8143	0.2903		0.3847	0.6471	0.7917	0.3000		0.3578	0.7353	0.8333	0.5000
55	0.4114	0.6535	0.7571	0.4194		0.3844	0.6765	0.8333	0.3000		0.3656	0.7647	0.8750	0.5000
60	0.4191	0.6139	0.7286	0.3548		0.4150	0.5294	0.6250	0.3000		0.3824	0.5588	0.5833	0.5000
65	0.4006	0.6733	0.8000	0.3871		0.3941	0.6765	0.9167	0.1000		0.3840	0.7353	0.9167	0.3000
70	0.3962	0.6733	0.8286	0.3226		0.3817	0.7353	1.0000	0.1000		0.3829	0.7059	1.0000	0.0000
75	0.4044	0.6931	0.8143	0.4194		0.4151	0.5882	0.7917	0.1000		0.3851	0.6471	0.8750	0.1000
80	0.3703	0.6832	0.7714	0.4839		0.4660	0.5882	0.7083	0.3000		0.3970	0.5882	0.7083	0.3000
85	0.3654	0.7129	0.8000	0.5161		0.4514	0.6176	0.8750	0.0000		0.3988	0.7059	1.0000	0.0000
90	0.4152	0.6238	0.7143	0.4194		0.3851	0.6471	0.8333	0.2000		0.3832	0.7647	0.9167	0.4000
95	0.3959	0.6238	0.7000	0.4516		0.3904	0.5294	0.6667	0.2000		0.3684	0.5882	0.6250	0.5000
100	0.4100	0.6733	0.7857	0.4194		0.3945	0.5882	0.7500	0.2000		0.3702	0.6471	0.7500	0.4000
105	0.3978	0.6238	0.7286	0.3871		0.4059	0.5000	0.6250	0.2000		0.3886	0.6471	0.7500	0.4000
110	0.4331	0.6139	0.6857	0.4516		0.4474	0.5588	0.7083	0.2000		0.3625	0.7059	0.8333	0.4000
115	0.3716	0.7030	0.7857	0.5161		0.4314	0.6765	0.8333	0.3000		0.3632	0.6471	0.7500	0.4000
120	0.3857	0.6733	0.7571	0.4839		0.4540	0.5882	0.7083	0.3000		0.3749	0.5294	0.5833	0.4000
125	0.3890	0.6832	0.7714	0.4839		0.4253	0.6471	0.7500	0.4000		0.3844	0.5882	0.6667	0.4000
130	0.3525	0.7129	0.7571	0.6129		0.4504	0.5294	0.7500	0.0000		0.3986	0.6471	0.8750	0.1000
135	0.3466	0.7228	0.7714	0.6129		0.6089	0.2647	0.1250	0.6000		0.4805	0.3824	0.2917	0.6000
140	0.3832	0.7921	0.8714	0.6129		0.5220	0.5294	0.6667	0.2000		0.4048	0.5588	0.7500	0.1000
145	0.3676	0.7624	0.8286	0.6129		0.4962	0.5588	0.7500	0.1000		0.3706	0.6765	0.9167	0.1000
150	0.2936	0.7822	0.7857	0.7742		0.6468	0.4118	0.3750	0.5000		0.4558	0.4412	0.5000	0.3000
155	0.3643	0.7426	0.8286	0.5484		0.4596	0.5588	0.6250	0.4000		0.3780	0.6176	0.7500	0.3000
160	0.3196	0.7426	0.7857	0.6452		0.5579	0.5588	0.7083	0.2000		0.4110	0.6176	0.8333	0.1000
165	0.3748	0.7426	0.8000	0.6129		0.4559	0.5588	0.6667	0.3000		0.3403	0.7059	0.9583	0.1000
170	0.3652	0.7228	0.7714	0.6129		0.6741	0.6176	0.8750	0.0000		0.4176	0.7353	1.0000	0.1000
175	0.3004	0.7921	0.8429	0.6774		0.5829	0.6176	0.7500	0.3000		0.3967	0.6471	0.7917	0.3000
180	0.2886	0.7624	0.8143	0.6452		0.7796	0.6471	0.8750	0.1000		0.4251	0.7059	0.9583	0.1000
185	0.3280	0.7228	0.7286	0.7097		0.4407	0.6176	0.8333	0.1000		0.3912	0.7059	0.9167	0.2000
190	0.2749	0.6733	0.6571	0.7097		0.7956	0.4706	0.5833	0.2000		0.5073	0.5588	0.6667	0.3000
195	0.2403	0.8614	0.8714	0.8387		0.8352	0.6471	0.8750	0.1000		0.6839	0.6765	0.9583	0.0000
200	0.3729	0.6337	0.6429	0.6129		0.5181	0.5588	0.7500	0.1000		0.4139	0.6471	0.7917	0.3000
205	0.2114	0.8614	0.8714	0.8387		1.2277	0.5294	0.6250	0.3000		0.5540	0.7059	0.8333	0.4000
210	0.2057	0.7723	0.7000	0.9355		1.1242	0.5882	0.7083	0.3000		0.5951	0.5882	0.7500	0.2000
215	0.1640	0.8911	0.9000	0.8710		1.3686	0.6176	0.8750	0.0000		0.8658	0.7059	0.9583	0.1000
220	0.2354	0.8614	0.9000	0.7742		1.2669	0.6765	0.9167	0.1000		0.9936	0.7059	0.9167	0.2000
225	0.1508	0.8515	0.8000	0.9677		1.3254	0.5294	0.7083	0.1000		0.5598	0.7647	0.8333	0.6000
230	0.1690	0.8317	0.8000	0.9032		1.1966	0.5588	0.7083	0.2000		0.7352	0.5588	0.7083	0.2000
235	0.2389	0.8416	0.8286	0.8710		1.1043	0.5588	0.7917	0.0000		0.7597	0.6471	0.7500	0.4000
240	0.2129	0.8416	0.8143	0.9032		1.1632	0.6176	0.8750	0.0000		0.6610	0.7059	0.8750	0.3000
245	0.1899	0.8416	0.8429	0.8387		1.5826	0.5882	0.8333	0.0000		0.9028	0.6765	0.8750	0.2000
250	0.1147	0.9208	0.9000	0.9677		1.6255	0.6176	0.8750	0.0000		1.0004	0.6765	0.8333	0.3000
255	0.1845	0.8119	0.7429	0.9677		1.0514	0.4706	0.5417	0.3000		0.7687	0.5588	0.5417	0.6000
260	0.1987	0.8911	0.9000	0.8710		1.6694	0.6471	0.9167	0.0000		0.9678	0.7647	1.0000	0.2000
265	0.2633	0.9109	0.8857	0.9677		0.8944	0.5588	0.7083	0.2000		0.7638	0.5588	0.7083	0.2000
270	0.1539	0.8812	0.8571	0.9355		1.0842	0.5882	0.7083	0.3000		0.8219	0.5588	0.6667	0.3000
275	0.0959	0.9010	0.8714	0.9677		1.5263	0.5588	0.6667	0.3000		1.0661	0.5294	0.5833	0.4000
280	0.1282	0.8614	0.8857	0.8065		1.5244	0.6471	0.8333	0.2000		1.0132	0.6765	0.7917	0.4000
285	0.0686	0.9406	0.9286	0.9677		1.7592	0.6176	0.7917	0.2000		0.9587	0.6471	0.7917	0.3000
290	0.1698	0.8713	0.8571	0.9032		1.3463	0.4412	0.5000	0.3000		1.0354	0.5000	0.5833	0.3000
295	0.1556	0.8812	0.8286	1.0000		1.3997	0.5588	0.7917	0.0000		0.9729	0.6471	0.8333	0.2000
300	0.2573	0.8119	0.7857	0.8710		0.9610	0.5588	0.7083	0.2000		0.6670	0.6176	0.7917	0.2000
305	0.2230	0.8515	0.8429	0.8710		0.7116	0.5588	0.6667	0.3000		0.5383	0.6176	0.6250	0.6000
310	0.1471	0.8614	0.8429	0.9032		0.8386	0.5588	0.6667	0.3000		0.6337	0.6176	0.7500	0.3000
315	0.1731	0.8911	0.8714	0.9355		0.9601	0.6176	0.7917	0.2000		0.5936	0.6471	0.7917	0.3000
320	0.1643	0.8317	0.8000	0.9032		0.8916	0.5000	0.6250	0.2000		0.5754	0.6765	0.7083	0.6000
325	0.1559	0.9208	0.9000	0.9677		0.9394	0.5000	0.5000	0.5000		0.8635	0.5882	0.5417	0.7000
330	0.1125	0.9010	0.8714	0.9677		1.0802	0.5294	0.6667	0.2000		0.7133	0.5588	0.6667	0.3000
335	0.1802	0.8812	0.8714	0.9032		1.1531	0.6176	0.7500	0.3000		0.7602	0.7059	0.8333	0.4000
340	0.0766	0.9307	0.9143	0.9677		1.2671	0.5882	0.7083	0.3000		0.8447	0.6471	0.7500	0.4000
345	0.1241	0.8812	0.8571	0.9355		1.5928	0.5294	0.6250	0.3000		0.9421	0.6471	0.7917	0.3000
350	0.1279	0.9307	0.9286	0.9355		1.3906	0.5588	0.6667	0.3000		0.9274	0.5882	0.7083	0.3000
355	0.1068	0.9010	0.8857	0.9355		1.2980	0.5882	0.7083	0.3000		0.9176	0.6471	0.7500	0.4000
360	0.1211	0.9307	0.9286	0.9355		1.4406	0.5588	0.6667	0.3000		0.9427	0.5588	0.6250	0.4000
365	0.0650	0.9307	0.9143	0.9677		1.4444	0.5588	0.6667	0.3000		1.1656	0.5588	0.6250	0.4000
370	0.0846	0.9208	0.9000	0.9677		1.4265	0.5588	0.6667	0.3000		0.9042	0.5588	0.5417	0.6000
375	0.1115	0.9307	0.9143	0.9677		1.2583	0.5588	0.6667	0.3000		0.8135	0.6471	0.7083	0.5000
380	0.0978	0.9406	0.9143	1.0000		1.3107	0.5588	0.6667	0.3000		0.9497	0.5882	0.5833	0.6000
385	0.0526	0.9802	0.9714	1.0000		1.4957	0.5882	0.7083	0.3000		0.9902	0.6765	0.7500	0.5000
390	0.1421	0.8614	0.8286	0.9355		1.4863	0.5882	0.7500	0.2000		1.0394	0.5588	0.6250	0.4000
395	0.1301	0.9109	0.9143	0.9032		1.6871	0.6471	0.7917	0.3000		1.0872	0.6765	0.7917	0.4000
400	0.0831	0.9406	0.9429	0.9355		1.4876	0.5882	0.7083	0.3000		1.0257	0.5588	0.6250	0.4000
405	0.1175	0.9406	0.9571	0.9032		1.6819	0.5882	0.7083	0.3000		1.1563	0.6176	0.7083	0.4000
410	0.1573	0.9406	0.9571	0.9032		1.7600	0.5588	0.6667	0.3000		1.3066	0.5882	0.5833	0.6000
415	0.0922	0.9307	0.9000	1.0000		1.8572	0.5588	0.6667	0.3000		1.1245	0.6176	0.6250	0.6000
420	0.0390	0.9703	0.9571	1.0000		1.6378	0.5294	0.6250	0.3000		1.1988	0.5588	0.5833	0.5000
425	0.0339	0.9901	0.9857	1.0000		1.8414	0.5882	0.7083	0.3000		1.1572	0.6176	0.7083	0.4000
430	0.0548	0.9505	0.9429	0.9677		1.8211	0.5882	0.7083	0.3000		1.1637	0.6471	0.7083	0.5000
435	0.0767	0.9604	0.9571	0.9677		1.9430	0.5588	0.6667	0.3000		1.2430	0.5588	0.5833	0.5000
440	0.0575	0.9604	0.9571	0.9677		1.8951	0.5588	0.6667	0.3000		1.2331	0.5588	0.5833	0.5000
445	0.0609	0.9505	0.9429	0.9677		1.9676	0.5882	0.7083	0.3000		1.3302	0.5882	0.6667	0.4000
450	0.0446	0.9505	0.9286	1.0000		2.0722	0.5588	0.6667	0.3000		1.3231	0.5882	0.6667	0.4000
455	0.0285	0.9901	0.9857	1.0000		2.0083	0.5882	0.7083	0.3000		1.3978	0.5882	0.6667	0.4000
460	0.0423	0.9802	0.9714	1.0000		2.1195	0.5882	0.7083	0.3000		1.4066	0.5882	0.7083	0.3000
465	0.0412	0.9703	0.9857	0.9355		1.9075	0.5588	0.6667	0.3000		1.3801	0.5588	0.6250	0.4000
470	0.0799	0.9406	0.9286	0.9677		2.1034	0.5882	0.7083	0.3000		1.3482	0.5588	0.6667	0.3000
475	0.0530	0.9307	0.9000	1.0000		2.0952	0.5882	0.7083	0.3000		1.3054	0.5882	0.6667	0.4000
480	0.1131	0.9703	0.9714	0.9677		2.0446	0.5882	0.7083	0.3000		1.2516	0.5588	0.5833	0.5000
485	0.1010	0.9802	0.9857	0.9677		1.7585	0.5588	0.6667	0.3000		1.3897	0.5294	0.5833	0.4000
490	0.0737	0.9604	0.9571	0.9677		1.9826	0.5882	0.7083	0.3000		1.3885	0.5294	0.6250	0.3000
495	0.0505	0.9802	0.9714	1.0000		2.0318	0.6176	0.7500	0.3000		1.3027	0.5882	0.6667	0.4000
500	0.0272	0.9802	0.9714	1.0000		1.9340	0.5882	0.7083	0.3000		1.2998	0.5882	0.6667	0.4000
505	0.1460	0.9703	0.9857	0.9355		1.9399	0.5588	0.6667	0.3000		1.3348	0.5588	0.6250	0.4000
510	0.0626	0.9703	0.9571	1.0000		1.7084	0.5882	0.7083	0.3000		1.2123	0.5294	0.5833	0.4000
515	0.0511	0.9505	0.9429	0.9677		1.9743	0.6176	0.7500	0.3000		1.2578	0.5882	0.7083	0.3000
520	0.1135	0.9604	0.9714	0.9355		1.9781	0.6176	0.7500	0.3000		1.2932	0.5588	0.6667	0.3000
525	0.0719	0.9604	0.9714	0.9355		1.9238	0.6176	0.7500	0.3000		1.2564	0.5588	0.6667	0.3000
530	0.1143	0.9604	0.9571	0.9677		2.0032	0.5882	0.7083	0.3000		1.2283	0.6176	0.7083	0.4000
535	0.0310	0.9901	1.0000	0.9677		1.9548	0.6176	0.7500	0.3000		1.2995	0.5882	0.6667	0.4000
540	0.0332	0.9901	0.9857	1.0000		2.0231	0.5882	0.7083	0.3000		1.2380	0.5588	0.6667	0.3000
545	0.0700	0.9901	1.0000	0.9677		2.0532	0.6471	0.7917	0.3000		1.3503	0.6471	0.7500	0.4000
550	0.0394	0.9802	0.9857	0.9677		1.9909	0.5588	0.6667	0.3000		1.3272	0.5882	0.6250	0.5000
555	0.0149	1.0000	1.0000	1.0000		1.9829	0.5882	0.7083	0.3000		1.3532	0.5882	0.6667	0.4000


 training loss less than 0.02, Program exit.


=============END of Training of ResAttentionNet Predict Model =================

Program Ending Time: 2019-07-29 18:06:54.915051
