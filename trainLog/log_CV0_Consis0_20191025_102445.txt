=============training from sratch============
Program ID: 28364

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy', '0', '3', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
      Oct 23th, 2019
      1   change to MaxPool with 2*2*2 with stride 2;
      2   Loss use GeneralizedDiceLoss function;
      
      Oct 25th, 2019
      1   add trainAllData switch
        
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-25 10:24:45.780799
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191025_102445

Info: this is the 0th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191025_102445.
All files are in one partition.

all dataset: total 35 image files.
Network has total 254,652,146 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		0.7933		0.20274
5	1.0000e-02		0.5595		0.40830
10	1.0000e-02		0.4895		0.47208
15	1.0000e-02		0.5165		0.42651
20	1.0000e-02		0.4946		0.46500
25	1.0000e-02		0.4699		0.48704
30	1.0000e-02		0.4192		0.54171
35	1.0000e-02		0.4142		0.54619
40	1.0000e-02		0.4339		0.52983
45	1.0000e-02		0.4083		0.56064
50	1.0000e-02		0.3952		0.57115
55	1.0000e-02		0.3909		0.58149
60	1.0000e-02		0.3655		0.60731
65	1.0000e-02		0.3860		0.59461
70	1.0000e-02		0.3563		0.62126
75	1.0000e-02		0.3577		0.60461
80	1.0000e-02		0.3602		0.60717
85	1.0000e-02		0.3591		0.60455
90	1.0000e-02		0.3401		0.64147
95	1.0000e-02		0.3247		0.65392
100	1.0000e-02		0.3087		0.66771
105	1.0000e-02		0.3194		0.66220
110	1.0000e-02		0.2793		0.70164
115	1.0000e-02		0.3283		0.64454
120	1.0000e-02		0.2738		0.70865
125	1.0000e-02		0.2755		0.70545
130	1.0000e-02		0.2765		0.70717
135	1.0000e-02		0.2648		0.71458
140	1.0000e-02		0.2855		0.69071
145	1.0000e-02		0.2846		0.69681
150	1.0000e-02		0.2743		0.71126
155	1.0000e-02		0.2843		0.70097
160	1.0000e-02		0.2811		0.70168
165	1.0000e-02		0.2602		0.72558
170	1.0000e-02		0.2343		0.74783
175	1.0000e-02		0.2655		0.71802
180	1.0000e-02		0.2376		0.74828
185	1.0000e-02		0.2603		0.72701
190	1.0000e-02		0.2180		0.76694
195	1.0000e-02		0.2291		0.75607
200	1.0000e-02		0.2404		0.74538
205	1.0000e-02		0.2055		0.78280
210	1.0000e-02		0.2403		0.74327
215	1.0000e-02		0.2072		0.77987
220	1.0000e-02		0.2283		0.75866
225	1.0000e-02		0.2282		0.75524
230	1.0000e-02		0.2120		0.77537
235	1.0000e-02		0.2392		0.74592
240	1.0000e-02		0.1870		0.80284
245	1.0000e-02		0.3316		0.64141
250	1.0000e-02		0.2042		0.78164
255	1.0000e-02		0.2067		0.77969
260	1.0000e-02		0.1789		0.81160
265	1.0000e-02		0.1827		0.80572
270	1.0000e-02		0.1998		0.78782
275	1.0000e-02		0.1768		0.81271
280	1.0000e-02		0.1945		0.79528
285	1.0000e-02		0.1707		0.81872
290	1.0000e-02		0.1835		0.80588
295	1.0000e-02		0.1548		0.83572
300	1.0000e-02		0.2314		0.74854
305	1.0000e-02		0.1855		0.80468
310	1.0000e-02		0.1585		0.83099
315	1.0000e-02		0.2450		0.74531
320	1.0000e-02		0.1570		0.83364
325	1.0000e-02		0.1688		0.82127
330	1.0000e-02		0.1868		0.80237
335	1.0000e-02		0.1668		0.82276
340	1.0000e-02		0.1520		0.83949
345	1.0000e-02		0.1505		0.84226
350	1.0000e-02		0.1398		0.85134
355	1.0000e-02		0.1347		0.85648
360	1.0000e-02		0.1302		0.86357
365	1.0000e-02		0.1515		0.83977
370	1.0000e-02		0.1453		0.84595
375	1.0000e-02		0.1186		0.87454
380	1.0000e-02		0.1303		0.86062
385	1.0000e-02		0.1564		0.83434
390	1.0000e-02		0.1628		0.82742
395	1.0000e-02		0.1777		0.81295
400	1.0000e-02		0.1397		0.85195
405	1.0000e-02		0.1349		0.85826
410	1.0000e-02		0.1330		0.85979
415	1.0000e-02		0.1160		0.87768
420	1.0000e-02		0.1107		0.88222
425	1.0000e-02		0.1335		0.85793
430	1.0000e-02		0.1506		0.83960
435	1.0000e-02		0.1281		0.86316
440	1.0000e-02		0.1125		0.88077
445	1.0000e-02		0.1094		0.88376
450	1.0000e-02		0.1330		0.85907
455	1.0000e-02		0.1220		0.87132
460	1.0000e-02		0.1053		0.88843
465	1.0000e-02		0.1046		0.88952
470	1.0000e-02		0.0984		0.89392
475	1.0000e-02		0.1142		0.87976
480	1.0000e-02		0.1212		0.87246
485	1.0000e-02		0.1016		0.89238
490	1.0000e-02		0.0960		0.89759
495	1.0000e-02		0.1284		0.86386
500	1.0000e-02		0.1254		0.86755
505	1.0000e-02		0.0986		0.89561
510	1.0000e-02		0.1055		0.88701
515	1.0000e-02		0.1314		0.86057
520	1.0000e-02		0.0983		0.89492
525	1.0000e-02		0.1054		0.88863
530	1.0000e-02		0.0970		0.89697
535	1.0000e-02		0.0948		0.89999
540	1.0000e-02		0.1198		0.87257
545	1.0000e-02		0.0910		0.90360
550	1.0000e-02		0.0830		0.91225
555	1.0000e-02		0.0844		0.91083
560	1.0000e-02		0.0835		0.91114
565	1.0000e-02		0.0941		0.89950
570	1.0000e-02		0.0925		0.90205
575	1.0000e-02		0.1163		0.87643
580	1.0000e-02		0.0853		0.90996
585	1.0000e-02		0.0836		0.91155
590	1.0000e-02		0.1690		0.82066
595	1.0000e-02		0.0955		0.89905
600	1.0000e-02		0.0858		0.90881
605	1.0000e-03		0.0922		0.90339
610	1.0000e-03		0.0737		0.92185
615	1.0000e-03		0.0848		0.91060
620	1.0000e-03		0.0782		0.91692
625	1.0000e-03		0.0691		0.92675
630	1.0000e-03		0.0798		0.91597
635	1.0000e-03		0.0723		0.92371
640	1.0000e-03		0.0738		0.92086
645	1.0000e-03		0.0680		0.92804
650	1.0000e-03		0.0723		0.92349
655	1.0000e-03		0.0732		0.92239
660	1.0000e-03		0.0721		0.92210
665	1.0000e-03		0.0711		0.92464
670	1.0000e-03		0.0688		0.92680
675	1.0000e-03		0.0655		0.93065
680	1.0000e-03		0.0688		0.92638
685	1.0000e-03		0.0632		0.93281
690	1.0000e-03		0.0684		0.92754
695	1.0000e-03		0.0680		0.92811
700	1.0000e-03		0.0664		0.92959
705	1.0000e-03		0.0690		0.92717
710	1.0000e-03		0.0669		0.92886
715	1.0000e-03		0.0655		0.93036
720	1.0000e-03		0.0658		0.93129
725	1.0000e-03		0.0649		0.93140
730	1.0000e-03		0.0678		0.92784
735	1.0000e-03		0.0608		0.93532
740	1.0000e-03		0.0649		0.93102
745	1.0000e-03		0.0632		0.93280
750	1.0000e-03		0.0647		0.93133
755	1.0000e-03		0.0630		0.93292
760	1.0000e-03		0.0673		0.92973
765	1.0000e-03		0.0641		0.93185
770	1.0000e-03		0.0697		0.92579
775	1.0000e-03		0.0634		0.93294
780	1.0000e-03		0.0642		0.93164
785	1.0000e-03		0.0672		0.92862
790	1.0000e-03		0.0599		0.93661
795	1.0000e-03		0.0633		0.93338
800	1.0000e-03		0.0634		0.93288
805	1.0000e-03		0.0610		0.93534
810	1.0000e-03		0.0666		0.92937
815	1.0000e-03		0.0595		0.93679
820	1.0000e-03		0.0619		0.93450
825	1.0000e-03		0.0658		0.93119
830	1.0000e-03		0.0625		0.93379
835	1.0000e-03		0.0620		0.93435
840	1.0000e-03		0.0631		0.93340
845	1.0000e-03		0.0638		0.93287
850	1.0000e-03		0.0588		0.93745
855	1.0000e-03		0.0605		0.93572
860	1.0000e-03		0.0607		0.93564
865	1.0000e-03		0.0594		0.93704
870	1.0000e-03		0.0566		0.93968
875	1.0000e-03		0.0602		0.93680
880	1.0000e-03		0.0587		0.93779
885	1.0000e-03		0.0605		0.93607
890	1.0000e-03		0.0593		0.93711
895	1.0000e-03		0.0613		0.93515
900	1.0000e-03		0.0571		0.93916
905	1.0000e-03		0.0601		0.93657
910	1.0000e-03		0.0585		0.93837
915	1.0000e-03		0.0620		0.93488
920	1.0000e-03		0.0564		0.94040
925	1.0000e-03		0.0570		0.93957
930	1.0000e-03		0.0577		0.93851
935	1.0000e-03		0.0633		0.93282
940	1.0000e-03		0.0584		0.93786
945	1.0000e-03		0.0557		0.94092
950	1.0000e-03		0.0572		0.93944
955	1.0000e-03		0.0590		0.93830
960	1.0000e-03		0.0600		0.93685
965	1.0000e-03		0.0551		0.94189
970	1.0000e-03		0.0547		0.94193
975	1.0000e-03		0.0579		0.93878
980	1.0000e-03		0.0553		0.94098
985	1.0000e-03		0.0552		0.94123
990	1.0000e-03		0.0563		0.94055
995	1.0000e-03		0.0559		0.94076
1000	1.0000e-03		0.0604		0.93607
1005	1.0000e-03		0.0570		0.93958
1010	1.0000e-03		0.0563		0.94032
1015	1.0000e-03		0.0547		0.94190
1020	1.0000e-03		0.0553		0.94085
1025	1.0000e-04		0.0562		0.94092
1030	1.0000e-04		0.0546		0.94227
1035	1.0000e-04		0.0559		0.94092
1040	1.0000e-04		0.0569		0.93960
1045	1.0000e-04		0.0520		0.94454
1050	1.0000e-04		0.0554		0.94092
1055	1.0000e-04		0.0537		0.94303
1060	1.0000e-04		0.0544		0.94231
1065	1.0000e-04		0.0540		0.94267
1070	1.0000e-04		0.0553		0.94093
1075	1.0000e-04		0.0535		0.94304
1080	1.0000e-04		0.0574		0.93943
1085	1.0000e-04		0.0543		0.94285
1090	1.0000e-04		0.0581		0.93876
1095	1.0000e-04		0.0554		0.94100
1100	1.0000e-05		0.0528		0.94397
1105	1.0000e-05		0.0514		0.94543
1110	1.0000e-05		0.0523		0.94432
1115	1.0000e-05		0.0534		0.94354
1120	1.0000e-05		0.0547		0.94239
1125	1.0000e-05		0.0538		0.94269
1130	1.0000e-05		0.0520		0.94504
1135	1.0000e-05		0.0543		0.94234
1140	1.0000e-05		0.0541		0.94243
1145	1.0000e-05		0.0557		0.94082
1150	1.0000e-05		0.0557		0.94123
1155	1.0000e-05		0.0553		0.94121
1160	1.0000e-05		0.0506		0.94623
1165	1.0000e-05		0.0524		0.94465
1170	1.0000e-05		0.0560		0.94050
1175	1.0000e-05		0.0550		0.94180
1180	1.0000e-05		0.0536		0.94309
1185	1.0000e-05		0.0534		0.94349
1190	1.0000e-05		0.0544		0.94239
1195	1.0000e-05		0.0522		0.94453
1200	1.0000e-05		0.0573		0.93936
1205	1.0000e-05		0.0572		0.93938
1210	1.0000e-05		0.0525		0.94443
1215	1.0000e-06		0.0554		0.94090
1220	1.0000e-06		0.0558		0.94130
1225	1.0000e-06		0.0518		0.94545
1230	1.0000e-06		0.0535		0.94294
1235	1.0000e-06		0.0509		0.94605
1240	1.0000e-06		0.0556		0.94144
1245	1.0000e-06		0.0535		0.94303
1250	1.0000e-06		0.0528		0.94414
1255	1.0000e-06		0.0600		0.93680
