Program ID of SkyWatcher Network training:3764

Program command: 
 ['TrainSkyWatcherPurePrediction.py', '/home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher/PurePrediction', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/Images_ROI_29_140_140', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json']

Major program changes: 
                      merge train and test dataMgr into one.
                      when epoch %5 ==0, do not use mixup.
                      Directly use 3D data for treatment prediction without segmentation. 
                      Number of filters in encoder is 64.
                      Only epoch %5 ==0, print log
                      Use BatchNorm1d in FC layer, instead of InstanceNorm1d.
                      relaunch running at June 15th.  
                       

Experiment setting for Image3d ROI to response:
Input CT data: 29*140*140  3D CT raw image ROI with spacing size(5*2*2)

Predictive Model: 1,  first 3-layer dense conv block with channel size 128.
                  2,  and 3 dense conv DownBB blocks,  each of which includes a stride 2 conv and 3-layers dense conv block; 
                  3,  and 3 fully connected layers  changes the tensor into size 2*1;
                  4,  final a softmax for binary classification;
                  Total network learning parameters are 8 million.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/SkyWatcherModel.py

response Loss Function:   focus loss  with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.

Data:   training data has 113 patients, and valdiation data has 27 patients with training/test rate 80/20.
        We randomize all data, and then assign same distrubtion of treat reponse 0,1 into to training and test data set.


Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.  

                    Learning Scheduler:  Reduce learning rate on  plateau, and learning rate patience is 30 epochs.                                

            

Program starting Time: 2019-06-15 08:24:56.123716
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher/PurePrediction

Now program get 140 input files.
Infor: In all data of 140 files, respone 0 has 41 files,
	  and response 1 has 99 files, where positive response rate = 0.7071428571428572 in full data
==== Regenerate training set and validation set by random with same distribution of 0 and 1 ==== 
Infor: Validation Set has 27 files,and Training Set has 113 files
Infor: In Validataion set, 19 1's, and positive response rate = 0.7037037037037037
Infor: In trainning set, 80 1's,  positive response rate = 0.7079646017699115
Infor: program is in multi samples running model.
TrainTestData Input:  batchSize=4, depth=29, height=140, width=140

Info: the size of bottle neck in the net = (64, 1, 7, 7)

Info: program uses Mixup with alpha=0.4, and mixupProb = 0.5.
Network trains from scratch.
Network has total 2,040,900 parameters.
Infor: Response Cross Entropy Weight: [3.3333333333333335, 1.4285714285714286] for label[0, 1]
Loss Functions List: FocalCELoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	TrLoss	Accura	TPR_r	TNR_r		TsLoss	Accura	TPR_r	TNR_r
0	0.5865	0.6518	0.8987	0.0606		0.3181	0.6250	0.8824	0.0000
5	0.2416	0.6518	0.7468	0.4242		0.1768	0.5417	0.5882	0.4286
10	0.2436	0.5625	0.6456	0.3636		0.2326	0.5417	0.5882	0.4286
15	0.1871	0.5804	0.6582	0.3939		0.2227	0.3750	0.2500	0.6250
20	0.1754	0.5893	0.6076	0.5455		0.3040	0.2500	0.1176	0.5714
25	0.1939	0.5982	0.6250	0.5312		0.2394	0.3750	0.2941	0.5714
30	0.1564	0.6518	0.7089	0.5152		0.2231	0.3333	0.2353	0.5714
35	0.1652	0.6250	0.6962	0.4545		0.1592	0.5833	0.5294	0.7143
40	0.1790	0.6607	0.7215	0.5152		0.1924	0.4583	0.3125	0.7500
45	0.1766	0.5982	0.6582	0.4545		0.2128	0.2500	0.1875	0.3750
50	0.1669	0.6339	0.7468	0.3636		0.2026	0.2917	0.1111	0.8333
55	0.1775	0.5893	0.6835	0.3636		0.2027	0.2083	0.1765	0.2857
60	0.1550	0.7054	0.7375	0.6250		0.2040	0.3333	0.2778	0.5000
65	0.1257	0.7946	0.8228	0.7273		0.2257	0.2917	0.2941	0.2857
70	0.1623	0.6518	0.7089	0.5152		0.2284	0.2500	0.2353	0.2857
75	0.1369	0.7679	0.8354	0.6061		0.2068	0.4583	0.3889	0.6667
80	0.1292	0.7857	0.7722	0.8182		0.2204	0.2917	0.2500	0.3750
85	0.1694	0.6696	0.7215	0.5455		0.2352	0.5000	0.6250	0.2500
90	0.1350	0.7411	0.7468	0.7273		0.2905	0.4583	0.5294	0.2857
95	0.1283	0.7946	0.8375	0.6875		0.2190	0.3333	0.3529	0.2857
100	0.1304	0.7857	0.8228	0.6970		0.2428	0.3750	0.3529	0.4286
105	0.1300	0.8125	0.8125	0.8125		0.2749	0.2500	0.2353	0.2857
110	0.1291	0.7589	0.7595	0.7576		0.3388	0.5417	0.7647	0.0000
115	0.1331	0.7679	0.7848	0.7273		0.3080	0.2917	0.3750	0.1250
120	0.0942	0.8839	0.8500	0.9688		0.2794	0.5000	0.5625	0.3750
125	0.1496	0.7054	0.7250	0.6562		0.4363	0.5417	0.5625	0.5000
130	0.1125	0.7857	0.7875	0.7812		0.2933	0.4583	0.5000	0.3750
135	0.1028	0.8393	0.8500	0.8125		0.2310	0.5833	0.5882	0.5714
140	0.0847	0.8571	0.8500	0.8750		0.2699	0.5000	0.6471	0.1429
145	0.0966	0.8393	0.8500	0.8125		0.2462	0.5833	0.7222	0.1667
150	0.0800	0.9107	0.9494	0.8182		0.2898	0.4583	0.6250	0.1250
155	0.1742	0.6964	0.7342	0.6061		0.3366	0.4167	0.4118	0.4286
160	0.1635	0.7143	0.7875	0.5312		0.2398	0.6667	0.7647	0.4286
165	0.1371	0.7411	0.7625	0.6875		0.2360	0.6250	0.8750	0.1250
170	0.1323	0.7589	0.8228	0.6061		0.2235	0.6250	0.8235	0.1429
175	0.1201	0.8036	0.8228	0.7576		0.3278	0.6250	0.7647	0.2857
180	0.1218	0.7679	0.8125	0.6562		0.2984	0.6667	0.8125	0.3750
185	0.1069	0.8571	0.8734	0.8182		0.2489	0.5833	0.6667	0.3333
190	0.1202	0.8214	0.8750	0.6875		0.3440	0.5000	0.6250	0.2500
195	0.0924	0.8750	0.8875	0.8438		0.3488	0.4583	0.6250	0.1250
200	0.0665	0.9018	0.8734	0.9697		0.2633	0.5417	0.6667	0.1667
205	0.0684	0.9107	0.9000	0.9375		0.3906	0.5833	0.8125	0.1250
210	0.0696	0.8750	0.9241	0.7576		0.3656	0.6667	0.8333	0.1667
215	0.0636	0.9196	0.9250	0.9062		0.4401	0.5417	0.7500	0.1250
220	0.1100	0.8750	0.9114	0.7879		0.2795	0.5833	0.7222	0.1667
225	0.0879	0.8839	0.8987	0.8485		0.4032	0.5417	0.7500	0.1250
230	0.0540	0.9554	0.9625	0.9375		0.3493	0.5833	0.7500	0.2500
235	0.0886	0.8661	0.8861	0.8182		0.3348	0.5000	0.6250	0.2500
240	0.0751	0.8929	0.9114	0.8485		0.2995	0.5833	0.7222	0.1667
245	0.0554	0.9554	0.9620	0.9394		0.3303	0.7083	0.8235	0.4286
250	0.0625	0.9018	0.9125	0.8750		0.3272	0.7500	0.9412	0.2857
255	0.0486	0.9732	0.9747	0.9697		0.4685	0.7083	0.9412	0.1429
260	0.0695	0.9107	0.9241	0.8788		0.4233	0.4583	0.6250	0.1250
265	0.0749	0.8839	0.9114	0.8182		0.3130	0.5417	0.8125	0.0000
270	0.0693	0.9196	0.9500	0.8438		0.3169	0.7083	0.8824	0.2857
275	0.0523	0.9196	0.9114	0.9394		0.4871	0.6250	0.8235	0.1429
280	0.0470	0.9554	0.9494	0.9697		0.4481	0.4583	0.6875	0.0000
285	0.0385	0.9554	0.9747	0.9091		0.4330	0.5000	0.5882	0.2857
290	0.0488	0.9732	0.9747	0.9697		0.3990	0.5833	0.7059	0.2857
295	0.0687	0.8929	0.8875	0.9062		0.3441	0.5417	0.7059	0.1429
300	0.0489	0.9286	0.9250	0.9375		0.2766	0.7083	0.8824	0.2857
305	0.0424	0.9554	0.9367	1.0000		0.2834	0.5417	0.6471	0.2857
310	0.1079	0.8929	0.9241	0.8182		0.2444	0.7083	0.8824	0.2857
315	0.0648	0.9107	0.9114	0.9091		0.3115	0.6250	0.8750	0.1250
320	0.0608	0.9286	0.9367	0.9091		0.2963	0.6667	0.7647	0.4286
325	0.0818	0.9107	0.9250	0.8750		0.2845	0.6250	0.8824	0.0000
330	0.0519	0.9554	0.9375	1.0000		0.3673	0.5000	0.6875	0.1250
335	0.0388	0.9643	0.9494	1.0000		0.2599	0.7083	0.8824	0.2857
340	0.0580	0.9196	0.9250	0.9062		0.2728	0.4583	0.6875	0.0000
345	0.0661	0.8839	0.8861	0.8788		0.3368	0.6667	0.8824	0.1429
350	0.0427	0.9643	0.9620	0.9697		0.3115	0.6667	0.8824	0.1429
355	0.0307	0.9732	0.9750	0.9688		0.2114	0.6667	0.8235	0.2857
360	0.0421	0.9554	0.9620	0.9394		0.2677	0.7083	0.8824	0.2857
365	0.0397	0.9821	0.9750	1.0000		0.2330	0.6667	0.8824	0.1429
370	0.0461	0.9375	0.9494	0.9091		0.2937	0.5833	0.7059	0.2857
375	0.0446	0.9554	0.9494	0.9697		0.2694	0.5833	0.8125	0.1250
380	0.0357	0.9732	0.9620	1.0000		0.3523	0.5000	0.7059	0.0000
385	0.0363	0.9911	0.9873	1.0000		0.3859	0.4583	0.6250	0.1250
390	0.0320	0.9732	0.9625	1.0000		0.3770	0.5833	0.7647	0.1429
395	0.0522	0.9554	0.9873	0.8788		0.3874	0.5833	0.7647	0.1429
400	0.0420	0.9732	0.9750	0.9688		0.4319	0.6250	0.8750	0.1250
405	0.0348	0.9821	0.9873	0.9697		0.2916	0.6250	0.7778	0.1667
410	0.0353	0.9732	0.9750	0.9688		0.3189	0.5833	0.8125	0.1250
415	0.0355	0.9732	0.9620	1.0000		0.4077	0.5000	0.6875	0.1250
420	0.0281	0.9821	0.9750	1.0000		0.2357	0.6250	0.8235	0.1429
425	0.0440	0.9643	0.9875	0.9062		0.3203	0.6250	0.8235	0.1429
430	0.0517	0.9464	0.9500	0.9375		0.3116	0.6250	0.8235	0.1429
435	0.0312	0.9911	0.9873	1.0000		0.3776	0.6667	0.8824	0.1429
440	0.0267	1.0000	1.0000	1.0000		0.3052	0.6250	0.8235	0.1429
445	0.0278	0.9821	0.9873	0.9697		0.3509	0.5833	0.7222	0.1667
450	0.0340	0.9821	0.9747	1.0000		0.3473	0.5833	0.7778	0.0000
455	0.0636	0.9554	0.9875	0.8750		0.3073	0.5417	0.6667	0.1667
460	0.0574	0.9464	0.9625	0.9062		0.3645	0.5000	0.6875	0.1250
465	0.0283	0.9821	0.9873	0.9697		0.4317	0.5000	0.6875	0.1250
470	0.0346	0.9732	0.9747	0.9697		0.3932	0.5000	0.6875	0.1250
475	0.0306	0.9732	0.9747	0.9697		0.3469	0.5000	0.6875	0.1250
480	0.0364	0.9821	0.9747	1.0000		0.3988	0.6250	0.8235	0.1429
485	0.0336	0.9732	0.9620	1.0000		0.4332	0.6667	0.8750	0.2500
490	0.0252	0.9821	0.9747	1.0000		0.2366	0.7083	0.8333	0.3333
495	0.0404	0.9643	0.9747	0.9394		0.4923	0.5833	0.7647	0.1429
500	0.0665	0.9464	0.9747	0.8788		0.2711	0.5417	0.7059	0.1429
505	0.0221	0.9911	0.9873	1.0000		0.3619	0.5000	0.6471	0.1429
510	0.0306	0.9821	0.9747	1.0000		0.3739	0.4167	0.5625	0.1250
515	0.0282	0.9821	1.0000	0.9394		0.3087	0.5417	0.6667	0.1667
520	0.0248	0.9821	0.9747	1.0000		0.3398	0.5417	0.7059	0.1429
525	0.0503	0.9286	0.9494	0.8788		0.3374	0.4167	0.5625	0.1250
530	0.0262	0.9911	0.9873	1.0000		0.3522	0.5833	0.7647	0.1429
535	0.0276	0.9732	0.9747	0.9697		0.3749	0.4583	0.6250	0.1250
540	0.0259	0.9732	0.9747	0.9697		0.3483	0.5417	0.6875	0.2500
545	0.0222	0.9911	0.9873	1.0000		0.3216	0.6250	0.7368	0.2000
550	0.0218	1.0000	1.0000	1.0000		0.2612	0.5417	0.7059	0.1429
555	0.0242	0.9911	0.9875	1.0000		0.3756	0.5000	0.6875	0.1250
560	0.0494	0.9643	0.9875	0.9062		0.4152	0.5833	0.8125	0.1250
565	0.0220	0.9911	0.9873	1.0000		0.2976	0.4167	0.5625	0.1250
570	0.0261	0.9821	0.9875	0.9688		0.3815	0.5417	0.7500	0.1250
575	0.0282	0.9911	0.9873	1.0000		0.4273	0.5833	0.7778	0.0000
580	0.0211	1.0000	1.0000	1.0000		0.2501	0.6250	0.7778	0.1667
585	0.0216	1.0000	1.0000	1.0000		0.2902	0.5417	0.6667	0.1667
590	0.0234	0.9911	0.9873	1.0000		0.2962	0.5833	0.7222	0.1667
595	0.0261	0.9643	0.9620	0.9697		0.3668	0.4583	0.6250	0.1250
600	0.0260	0.9821	0.9750	1.0000		0.3182	0.5000	0.6111	0.1667
605	0.0269	0.9732	0.9747	0.9697		0.3881	0.4167	0.5625	0.1250
610	0.0295	0.9732	0.9747	0.9697		0.3812	0.5833	0.8125	0.1250
615	0.0505	0.9554	0.9873	0.8788		0.4390	0.6250	0.8750	0.1250
620	0.0196	1.0000	1.0000	1.0000		0.4222	0.4583	0.5882	0.1429
625	0.0233	0.9821	0.9873	0.9697		0.2981	0.5417	0.7059	0.1429
630	0.0252	0.9821	0.9873	0.9697		0.3886	0.6250	0.8333	0.0000
635	0.0191	0.9821	0.9873	0.9697		0.4282	0.6250	0.8235	0.1429
640	0.0193	1.0000	1.0000	1.0000		0.2315	0.5417	0.6667	0.1667
645	0.0158	0.9911	1.0000	0.9697		0.3627	0.5000	0.6875	0.1250
