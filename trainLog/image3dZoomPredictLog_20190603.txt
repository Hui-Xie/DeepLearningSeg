Program ID of Predictive Network training:5103

Program command: 
 ['TrainPredictNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/Predict', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainImages_zoom_147_281_281', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/testImages_zoom_147_281_281', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/patientResponseDict.json', 'image3dZoom']

Major program changes: 
                      the nunmber of filters in 1st layer in V model = 96
                      latent Vector size: 1536*51*49 (featureMap* slices * axisPlaneLatentVector)
                      PredictModel is convsDenseModule+FC network.
                      there total 162 patient data, in which, 130 with smaller patientId as training data, 
                                                          and 32 with bigger patientID as test data

Experiment setting for Latent to response:
Input: 1536*51*49 Tensor as latent vector,
       where 1536 is the  number of filter at the bottleneck of V model, 
             51 is the number of slices of ROI CT image with size 51*281*281 for input to V model, 
             49 =7*7 is the flatted feature map for each filter.

Predictive Model: 1,  first 4-layer dense conv block reducing feature space into 768 with tensor size 768*51*49 
                  2,  and 4 dense conv blocks each of which includes a stride 2 conv and 4-layers dense conv block; now the the tensor is with size 48*2*2
                  3,  and a simple conv-batchNorm-Relu layer with filter size(2,2) change the tensor with size of  48*1;
                  4,  and 2 fully connected layers  changes the tensor into size 2*1;
                  5  final a softmax for binary classification;
                  Total network learning parameters are 29 millions.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/LatentPredictModel.py

Loss Function:   Cross Entropy with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.

Data:            training data has 130 patients, and test data has 32 patients with training/test rate 80/20.

Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.
                                                          
                                                         
Experiment setting for Image3d Zoom to response:
Input: 147*281*281 scaled 3D CT raw image as numpy array 
       
others same with Image3d ROI model.


Experiment setting for Image3d ROI to response:
Input: 147*281*281  3D CT raw image ROI as numpy array 
       
Predictive Model: 1,  first 3-layer dense conv block with channel size 24.
                  2,  and 6 dense conv DownBB blocks,  each of which includes a stride 2 conv and 3-layers dense conv block; 
                  3,  and 3 fully connected layers  changes the tensor into size 2*1;
                  4,  final a softmax for binary classification;
                  Total network learning parameters are 236K.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/Image3dPredictModel.py

Loss Function:   Cross Entropy with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.

Data:            training data has 130 patients, and test data has 32 patients with training/test rate 80/20.

Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.  
                    
                    change patience of learningRate scheduler to 30.                                         

            

Program starting Time: 2019-06-03 16:06:29.243880
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/Predict

Now program get 130 input files.
Now program get 32 input files.
TrainData Input:  batchSize=4, depth=147, height=281, width=281, NumClassfication=2

TestData Input:  batchSize=4, depth=147, height=281, width=281, NumClassfication=2

Info: the size of bottle neck in the net = 24* (1, 3, 3)

Info: program uses Mixup with alpha=0.4, and mixupProb = 0.5.
Network trains from scratch.
Network has total 236,648 parameters.
Infor: Cross Entropy Weight: [3.3333333333333335, 1.4285714285714286] for label[0, 1]
Info: program will use 4 GPUs.
Loss Functions List: FocalCELoss with weight of 1; 
Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 


Epoch		TrLoss	TrainAccuracy	TsLoss	TestAccuracy
0		0.1724	0.72368		0.1689	0.81250
1		0.1735	0.66071		0.1683	0.81250
2		0.1744	0.59615		0.1703	0.81250
3		0.1729	0.60417		0.1706	0.81250
4		0.1733	0.62500		0.1707	0.81250
5		0.1732	0.71154		0.1699	0.81250
6		0.1726	0.68333		0.1695	0.81250
7		0.1733	0.69737		0.1696	0.81250
8		0.1731	0.62500		0.1702	0.81250
9		0.1727	0.69643		0.1690	0.81250
10		0.1725	0.68750		0.1681	0.81250
11		0.1728	0.72222		0.1683	0.81250
12		0.1732	0.64706		0.1688	0.81250
13		0.1716	0.63333		0.1678	0.81250
14		0.1732	0.69118		0.1669	0.81250
15		0.1722	0.65789		0.1673	0.81250
16		0.1725	0.73529		0.1673	0.81250
17		0.1735	0.61842		0.1653	0.81250
18		0.1739	0.67857		0.1674	0.81250
19		0.1733	0.70833		0.1688	0.81250
20		0.1724	0.77941		0.1686	0.81250
21		0.1728	0.65000		0.1674	0.81250
22		0.1730	0.71591		0.1691	0.81250
23		0.1718	0.58929		0.1665	0.81250
24		0.1736	0.59091		0.1669	0.81250
25		0.1710	0.77941		0.1669	0.81250
26		0.1742	0.64706		0.1683	0.81250
27		0.1725	0.69048		0.1684	0.81250
28		0.1740	0.71667		0.1684	0.81250
29		0.1728	0.70312		0.1677	0.81250
30		0.1730	0.71154		0.1675	0.81250
31		0.1729	0.68333		0.1671	0.81250
32		0.1724	0.72727		0.1671	0.81250
33		0.1729	0.69118		0.1688	0.81250
34		0.1736	0.73214		0.1677	0.81250
35		0.1718	0.76923		0.1679	0.81250
36		0.1716	0.68750		0.1673	0.81250
37		0.1742	0.71667		0.1679	0.81250
38		0.1722	0.70312		0.1658	0.81250
39		0.1738	0.70000		0.1683	0.81250
40		0.1727	0.68750		0.1672	0.81250
41		0.1736	0.66071		0.1691	0.81250
42		0.1741	0.60938		0.1697	0.81250
43		0.1727	0.71667		0.1683	0.81250
44		0.1725	0.69643		0.1686	0.81250
45		0.1729	0.64583		0.1688	0.81250
46		0.1724	0.65625		0.1671	0.81250
47		0.1728	0.64286		0.1679	0.81250
48		0.1729	0.64062		0.1681	0.81250
49		0.1736	0.68750		0.1680	0.81250
50		0.1734	0.68333		0.1691	0.81250
51		0.1720	0.68478		0.1689	0.81250
52		0.1729	0.69118		0.1684	0.81250
53		0.1727	0.73077		0.1684	0.81250
54		0.1732	0.71875		0.1680	0.81250
55		0.1735	0.68333		0.1684	0.81250
56		0.1727	0.71154		0.1677	0.81250
57		0.1742	0.67308		0.1686	0.81250
58		0.1740	0.65000		0.1683	0.81250
59		0.1735	0.62500		0.1700	0.81250
60		0.1730	0.69118		0.1696	0.81250
61		0.1725	0.70238		0.1694	0.81250
62		0.1730	0.68750		0.1694	0.81250
63		0.1727	0.65789		0.1686	0.81250
64		0.1733	0.61538		0.1692	0.81250
65		0.1728	0.65278		0.1691	0.81250
66		0.1729	0.63889		0.1687	0.81250
67		0.1733	0.67647		0.1693	0.81250
68		0.1730	0.67857		0.1683	0.81250
69		0.1726	0.60000		0.1685	0.81250
70		0.1736	0.62500		0.1685	0.81250
71		0.1728	0.68421		0.1688	0.81250
72		0.1730	0.65625		0.1694	0.81250
73		0.1723	0.64062		0.1686	0.81250
74		0.1729	0.66667		0.1688	0.81250
75		0.1728	0.70000		0.1682	0.81250
76		0.1729	0.64474		0.1678	0.81250
77		0.1725	0.63158		0.1668	0.81250
78		0.1733	0.63636		0.1685	0.81250
79		0.1735	0.66071		0.1689	0.81250
80		0.1732	0.68056		0.1683	0.81250
81		0.1727	0.65000		0.1689	0.81250
82		0.1723	0.71875		0.1687	0.81250
83		0.1730	0.70000		0.1682	0.81250
84		0.1718	0.72222		0.1677	0.81250
85		0.1736	0.61538		0.1677	0.81250
86		0.1734	0.66667		0.1687	0.81250
87		0.1725	0.53571		0.1685	0.81250
88		0.1726	0.68333		0.1690	0.81250
89		0.1729	0.64706		0.1685	0.81250
90		0.1731	0.56667		0.1678	0.81250
91		0.1719	0.66667		0.1685	0.81250
92		0.1726	0.67308		0.1682	0.81250
93		0.1734	0.67647		0.1669	0.81250
94		0.1729	0.68333		0.1684	0.81250
95		0.1726	0.64474		0.1677	0.81250
96		0.1722	0.67857		0.1684	0.81250
97		0.1741	0.69737		0.1678	0.81250
98		0.1733	0.67188		0.1679	0.81250
99		0.1733	0.62500		0.1678	0.81250
100		0.1714	0.73214		0.1677	0.81250
101		0.1727	0.67857		0.1682	0.81250
102		0.1732	0.66250		0.1683	0.81250
103		0.1737	0.62500		0.1685	0.81250
104		0.1727	0.69643		0.1678	0.81250
105		0.1716	0.71053		0.1688	0.81250
106		0.1716	0.68750		0.1680	0.81250
107		0.1730	0.58824		0.1671	0.81250
108		0.1708	0.82143		0.1670	0.81250
109		0.1727	0.64773		0.1677	0.81250
110		0.1736	0.67308		0.1684	0.81250
111		0.1741	0.65000		0.1678	0.81250
112		0.1743	0.70588		0.1686	0.81250
113		0.1717	0.71667		0.1686	0.81250
114		0.1721	0.73438		0.1678	0.81250
115		0.1727	0.73438		0.1678	0.81250
116		0.1737	0.61364		0.1679	0.81250
117		0.1712	0.73077		0.1678	0.81250
118		0.1730	0.59211		0.1685	0.81250
119		0.1729	0.66667		0.1668	0.81250
120		0.1720	0.69231		0.1678	0.81250
121		0.1734	0.70833		0.1671	0.81250
122		0.1738	0.64062		0.1668	0.81250
123		0.1733	0.75000		0.1666	0.81250
124		0.1728	0.71667		0.1663	0.81250
125		0.1723	0.63889		0.1669	0.81250
126		0.1722	0.70000		0.1665	0.81250
127		0.1732	0.65789		0.1678	0.81250
128		0.1722	0.69231		0.1678	0.81250
129		0.1720	0.59615		0.1667	0.81250
130		0.1706	0.76667		0.1665	0.81250
131		0.1729	0.63889		0.1683	0.81250
132		0.1733	0.69643		0.1676	0.81250
133		0.1728	0.61538		0.1683	0.81250
134		0.1730	0.67045		0.1684	0.81250
135		0.1734	0.66667		0.1670	0.81250
136		0.1732	0.66667		0.1678	0.81250
137		0.1724	0.71667		0.1677	0.81250
138		0.1741	0.59375		0.1671	0.81250
139		0.1739	0.61765		0.1679	0.81250
140		0.1731	0.67105		0.1680	0.81250
141		0.1732	0.64286		0.1687	0.81250
142		0.1736	0.71667		0.1674	0.81250
143		0.1726	0.68333		0.1687	0.81250
144		0.1723	0.65625		0.1680	0.81250
145		0.1719	0.70312		0.1663	0.81250
146		0.1732	0.69643		0.1674	0.81250
147		0.1729	0.63235		0.1687	0.81250
148		0.1730	0.70000		0.1687	0.81250
149		0.1739	0.61842		0.1680	0.81250
150		0.1731	0.68182		0.1674	0.81250
151		0.1727	0.75000		0.1674	0.81250
152		0.1730	0.57353		0.1671	0.81250
153		0.1719	0.82353		0.1687	0.81250
154		0.1728	0.70833		0.1674	0.81250
155		0.1731	0.60294		0.1680	0.81250
156		0.1733	0.56944		0.1681	0.81250
157		0.1729	0.75000		0.1675	0.81250
158		0.1728	0.70833		0.1681	0.81250
159		0.1732	0.68750		0.1681	0.81250
160		0.1726	0.66071		0.1675	0.81250
161		0.1732	0.60714		0.1681	0.81250
162		0.1724	0.70000		0.1681	0.81250
163		0.1732	0.63235		0.1665	0.81250
164		0.1729	0.62500		0.1675	0.81250
165		0.1731	0.71875		0.1675	0.81250
166		0.1722	0.73214		0.1681	0.81250
167		0.1720	0.70455		0.1674	0.81250
168		0.1720	0.68750		0.1680	0.81250
169		0.1744	0.63333		0.1674	0.81250
170		0.1722	0.66667		0.1674	0.81250
171		0.1720	0.67500		0.1680	0.81250
172		0.1730	0.71429		0.1687	0.81250
173		0.1719	0.66667		0.1670	0.81250
174		0.1738	0.62500		0.1687	0.81250
175		0.1734	0.72059		0.1671	0.81250
176		0.1736	0.66071		0.1680	0.81250
177		0.1726	0.71154		0.1681	0.81250
178		0.1733	0.70833		0.1681	0.81250
179		0.1736	0.64706		0.1664	0.81250
180		0.1737	0.65625		0.1687	0.81250
181		0.1729	0.63462		0.1668	0.81250
182		0.1730	0.71875		0.1675	0.81250
183		0.1738	0.65000		0.1675	0.81250
184		0.1722	0.66071		0.1681	0.81250
185		0.1716	0.71667		0.1671	0.81250
186		0.1734	0.72222		0.1688	0.81250
187		0.1723	0.63333		0.1681	0.81250
188		0.1730	0.73214		0.1665	0.81250
189		0.1725	0.68182		0.1675	0.81250
190		0.1715	0.67647		0.1680	0.81250
191		0.1740	0.69737		0.1671	0.81250
192		0.1724	0.66667		0.1674	0.81250
193		0.1729	0.64583		0.1674	0.81250
194		0.1714	0.75000		0.1681	0.81250
195		0.1729	0.68333		0.1681	0.81250
196		0.1743	0.58333		0.1681	0.81250
197		0.1731	0.63636		0.1671	0.81250
198		0.1722	0.72222		0.1665	0.81250
199		0.1723	0.72727		0.1674	0.81250
200		0.1723	0.75000		0.1674	0.81250
201		0.1712	0.68056		0.1680	0.81250
202		0.1732	0.67308		0.1674	0.81250
203		0.1733	0.69231		0.1687	0.81250
204		0.1734	0.69643		0.1680	0.81250
205		0.1720	0.70588		0.1687	0.81250
206		0.1730	0.65000		0.1674	0.81250
207		0.1725	0.72222		0.1687	0.81250
208		0.1717	0.68333		0.1680	0.81250
209		0.1718	0.66667		0.1674	0.81250
210		0.1738	0.57353		0.1687	0.81250
211		0.1724	0.71875		0.1674	0.81250
212		0.1727	0.70833		0.1680	0.81250
213		0.1737	0.69643		0.1674	0.81250
214		0.1735	0.66667		0.1674	0.81250
215		0.1746	0.69231		0.1680	0.81250
216		0.1728	0.67188		0.1681	0.81250
217		0.1719	0.72727		0.1674	0.81250
218		0.1704	0.73077		0.1680	0.81250
219		0.1736	0.59375		0.1659	0.81250
220		0.1732	0.62500		0.1674	0.81250
221		0.1739	0.66667		0.1671	0.81250
222		0.1739	0.72500		0.1674	0.81250
223		0.1754	0.60714		0.1681	0.81250
224		0.1739	0.64062		0.1675	0.81250
225		0.1722	0.67308		0.1687	0.81250
226		0.1740	0.64706		0.1681	0.81250
227		0.1734	0.69444		0.1681	0.81250
228		0.1718	0.69643		0.1681	0.81250
229		0.1721	0.75000		0.1681	0.81250
230		0.1732	0.65000		0.1687	0.81250
231		0.1734	0.66071		0.1681	0.81250
232		0.1735	0.67188		0.1681	0.81250
233		0.1732	0.63636		0.1681	0.81250
234		0.1725	0.68750		0.1675	0.81250
235		0.1731	0.62500		0.1675	0.81250
236		0.1733	0.63333		0.1681	0.81250
237		0.1741	0.60000		0.1681	0.81250
238		0.1722	0.73438		0.1672	0.81250
239		0.1738	0.66667		0.1672	0.81250
240		0.1737	0.61364		0.1681	0.81250
241		0.1732	0.62500		0.1675	0.81250
242		0.1727	0.71875		0.1675	0.81250
243		0.1737	0.68421		0.1681	0.81250
244		0.1732	0.69231		0.1682	0.81250
245		0.1743	0.61765		0.1688	0.81250
246		0.1733	0.65278		0.1682	0.81250
247		0.1734	0.62500		0.1672	0.81250
248		0.1721	0.70000		0.1682	0.81250
249		0.1712	0.69118		0.1681	0.81250
250		0.1723	0.70833		0.1672	0.81250
251		0.1733	0.64286		0.1681	0.81250
252		0.1725	0.72059		0.1681	0.81250
253		0.1733	0.67647		0.1675	0.81250
254		0.1725	0.71429		0.1681	0.81250
255		0.1728	0.70312		0.1672	0.81250
256		0.1735	0.65278		0.1682	0.81250
257		0.1720	0.68750		0.1688	0.81250
258		0.1732	0.63235		0.1672	0.81250
259		0.1723	0.67188		0.1672	0.81250
260		0.1730	0.67857		0.1661	0.81250
261		0.1718	0.70000		0.1681	0.81250
262		0.1736	0.66250		0.1675	0.81250
263		0.1737	0.58333		0.1672	0.81250
264		0.1725	0.69444		0.1681	0.81250
265		0.1727	0.66667		0.1682	0.81250
266		0.1729	0.65000		0.1682	0.81250
267		0.1732	0.68056		0.1682	0.81250
268		0.1738	0.73077		0.1666	0.81250
269		0.1732	0.67105		0.1682	0.81250
270		0.1724	0.72222		0.1672	0.81250
271		0.1731	0.68333		0.1682	0.81250
272		0.1734	0.66071		0.1675	0.81250
273		0.1722	0.75000		0.1682	0.81250
274		0.1719	0.68333		0.1682	0.81250
275		0.1729	0.69231		0.1682	0.81250
276		0.1720	0.68750		0.1675	0.81250
277		0.1727	0.73529		0.1688	0.81250
278		0.1727	0.69118		0.1681	0.81250
279		0.1731	0.68750		0.1681	0.81250
280		0.1733	0.69048		0.1688	0.81250
281		0.1724	0.73750		0.1675	0.81250
282		0.1725	0.72222		0.1675	0.81250
283		0.1728	0.72917		0.1672	0.81250
284		0.1722	0.70833		0.1675	0.81250
285		0.1721	0.73438		0.1681	0.81250
286		0.1734	0.71667		0.1681	0.81250
287		0.1742	0.63235		0.1681	0.81250
288		0.1723	0.67857		0.1681	0.81250
289		0.1725	0.70000		0.1688	0.81250
290		0.1715	0.66667		0.1681	0.81250
291		0.1726	0.67105		0.1688	0.81250
292		0.1742	0.69231		0.1688	0.81250
293		0.1739	0.63462		0.1681	0.81250
294		0.1742	0.63235		0.1675	0.81250
295		0.1719	0.68750		0.1682	0.81250
296		0.1726	0.70312		0.1675	0.81250
297		0.1722	0.68750		0.1675	0.81250
298		0.1731	0.71053		0.1675	0.81250
299		0.1746	0.67188		0.1682	0.81250
300		0.1736	0.69231		0.1682	0.81250
301		0.1731	0.69643		0.1675	0.81250
302		0.1726	0.73438		0.1675	0.81250
303		0.1724	0.70455		0.1675	0.81250
304		0.1740	0.65000		0.1682	0.81250
305		0.1728	0.60000		0.1675	0.81250
306		0.1730	0.71429		0.1682	0.81250
307		0.1738	0.68333		0.1675	0.81250
308		0.1736	0.70455		0.1682	0.81250
309		0.1728	0.65625		0.1682	0.81250
310		0.1721	0.68750		0.1682	0.81250
311		0.1739	0.63889		0.1682	0.81250
312		0.1728	0.68750		0.1675	0.81250
313		0.1717	0.65789		0.1688	0.81250
314		0.1720	0.70312		0.1666	0.81250
315		0.1726	0.67857		0.1682	0.81250
316		0.1727	0.70000		0.1682	0.81250
317		0.1730	0.63158		0.1682	0.81250
318		0.1720	0.75000		0.1675	0.81250
319		0.1719	0.66071		0.1675	0.81250
320		0.1733	0.67500		0.1682	0.81250
321		0.1719	0.65000		0.1675	0.81250
322		0.1726	0.69643		0.1669	0.81250
323		0.1728	0.61842		0.1675	0.81250
324		0.1729	0.65385		0.1682	0.81250
325		0.1733	0.62500		0.1682	0.81250
326		0.1729	0.67857		0.1675	0.81250
327		0.1725	0.69643		0.1675	0.81250
328		0.1723	0.68750		0.1675	0.81250
329		0.1721	0.75000		0.1682	0.81250
330		0.1716	0.73333		0.1682	0.81250
331		0.1719	0.64286		0.1688	0.81250
332		0.1723	0.67188		0.1675	0.81250
333		0.1726	0.68333		0.1682	0.81250
334		0.1729	0.61765		0.1682	0.81250
335		0.1726	0.64062		0.1666	0.81250
336		0.1725	0.70588		0.1688	0.81250
337		0.1711	0.70833		0.1675	0.81250
338		0.1728	0.72059		0.1682	0.81250
339		0.1730	0.65909		0.1682	0.81250
340		0.1727	0.73611		0.1672	0.81250
341		0.1723	0.64474		0.1666	0.81250
342		0.1734	0.69444		0.1682	0.81250
343		0.1727	0.70312		0.1682	0.81250
344		0.1722	0.70833		0.1682	0.81250
345		0.1735	0.70312		0.1666	0.81250
346		0.1730	0.69048		0.1682	0.81250
347		0.1733	0.63095		0.1682	0.81250
348		0.1707	0.75000		0.1682	0.81250
349		0.1727	0.75000		0.1681	0.81250
350		0.1733	0.68750		0.1672	0.81250
351		0.1724	0.71667		0.1675	0.81250
352		0.1720	0.71429		0.1681	0.81250
353		0.1720	0.77500		0.1675	0.81250
354		0.1728	0.66250		0.1675	0.81250
355		0.1709	0.67188		0.1672	0.81250
356		0.1720	0.73333		0.1675	0.81250
357		0.1735	0.64062		0.1681	0.81250
358		0.1708	0.75000		0.1681	0.81250
359		0.1725	0.71667		0.1681	0.81250
360		0.1733	0.70000		0.1675	0.81250
361		0.1729	0.66667		0.1681	0.81250
362		0.1732	0.67308		0.1681	0.81250
363		0.1727	0.72059		0.1681	0.81250
364		0.1730	0.68333		0.1681	0.81250
365		0.1730	0.73214		0.1681	0.81250
366		0.1731	0.70000		0.1681	0.81250
367		0.1725	0.64474		0.1688	0.81250
368		0.1736	0.65385		0.1681	0.81250
369		0.1722	0.75000		0.1681	0.81250
370		0.1736	0.69737		0.1681	0.81250
371		0.1730	0.66176		0.1681	0.81250
372		0.1724	0.69444		0.1675	0.81250
373		0.1712	0.73438		0.1681	0.81250
374		0.1713	0.59615		0.1675	0.81250
375		0.1713	0.72222		0.1666	0.81250
376		0.1718	0.70000		0.1675	0.81250
377		0.1727	0.61765		0.1675	0.81250
378		0.1733	0.58824		0.1681	0.81250
379		0.1726	0.68750		0.1688	0.81250
380		0.1732	0.67188		0.1681	0.81250
381		0.1720	0.71429		0.1675	0.81250
382		0.1723	0.70833		0.1681	0.81250
383		0.1747	0.62500		0.1672	0.81250
384		0.1719	0.73529		0.1681	0.81250
385		0.1735	0.64706		0.1688	0.81250
386		0.1708	0.71429		0.1675	0.81250
387		0.1717	0.70312		0.1672	0.81250
388		0.1723	0.72059		0.1675	0.81250
389		0.1735	0.70833		0.1675	0.81250
390		0.1732	0.63158		0.1681	0.81250
391		0.1729	0.71667		0.1681	0.81250
392		0.1728	0.71429		0.1688	0.81250
393		0.1730	0.75000		0.1666	0.81250
394		0.1731	0.71250		0.1681	0.81250
395		0.1726	0.72917		0.1675	0.81250
396		0.1718	0.67188		0.1681	0.81250
397		0.1734	0.67857		0.1688	0.81250
398		0.1719	0.76923		0.1675	0.81250
399		0.1738	0.57143		0.1688	0.81250
400		0.1729	0.65000		0.1681	0.81250
401		0.1730	0.65385		0.1681	0.81250
402		0.1731	0.71875		0.1681	0.81250
403		0.1723	0.69118		0.1688	0.81250
404		0.1729	0.62500		0.1681	0.81250
405		0.1728	0.73077		0.1688	0.81250
406		0.1728	0.69118		0.1681	0.81250
407		0.1737	0.64583		0.1681	0.81250
408		0.1739	0.66250		0.1681	0.81250
409		0.1740	0.62500		0.1672	0.81250
410		0.1716	0.68056		0.1681	0.81250
411		0.1729	0.70000		0.1675	0.81250
412		0.1722	0.63333		0.1672	0.81250
413		0.1743	0.67105		0.1675	0.81250
414		0.1730	0.67308		0.1681	0.81250
415		0.1721	0.73333		0.1688	0.81250
416		0.1714	0.80263		0.1675	0.81250
417		0.1729	0.68750		0.1666	0.81250
418		0.1717	0.71429		0.1675	0.81250
419		0.1734	0.67647		0.1681	0.81250
420		0.1714	0.71875		0.1681	0.81250
421		0.1731	0.66667		0.1681	0.81250
422		0.1719	0.66667		0.1681	0.81250
423		0.1744	0.53571		0.1666	0.81250
424		0.1726	0.65909		0.1681	0.81250
425		0.1740	0.70833		0.1681	0.81250
426		0.1732	0.67188		0.1688	0.81250
427		0.1736	0.66667		0.1675	0.81250
428		0.1721	0.70833		0.1681	0.81250
429		0.1723	0.56250		0.1681	0.81250
430		0.1730	0.70833		0.1675	0.81250
431		0.1734	0.69118		0.1681	0.81250
432		0.1722	0.67308		0.1688	0.81250
433		0.1723	0.69444		0.1675	0.81250
434		0.1712	0.63333		0.1675	0.81250
435		0.1734	0.67308		0.1681	0.81250
436		0.1735	0.61905		0.1675	0.81250
437		0.1729	0.70000		0.1681	0.81250
438		0.1736	0.65789		0.1675	0.81250
439		0.1724	0.68750		0.1681	0.81250
440		0.1729	0.67500		0.1675	0.81250
441		0.1736	0.67857		0.1688	0.81250
442		0.1730	0.66176		0.1681	0.81250
443		0.1736	0.67105		0.1681	0.81250
444		0.1730	0.71875		0.1681	0.81250
445		0.1725	0.67188		0.1666	0.81250
446		0.1732	0.68333		0.1681	0.81250
447		0.1729	0.70588		0.1672	0.81250
448		0.1717	0.67857		0.1675	0.81250
449		0.1729	0.71429		0.1675	0.81250
450		0.1734	0.69737		0.1688	0.81250
451		0.1732	0.68750		0.1681	0.81250
452		0.1738	0.71667		0.1681	0.81250
453		0.1717	0.71053		0.1675	0.81250
454		0.1727	0.65000		0.1675	0.81250
455		0.1727	0.61765		0.1688	0.81250
456		0.1727	0.67647		0.1675	0.81250
457		0.1708	0.66071		0.1688	0.81250
458		0.1738	0.69048		0.1675	0.81250
459		0.1718	0.56250		0.1681	0.81250
460		0.1733	0.71875		0.1681	0.81250
461		0.1739	0.67105		0.1681	0.81250
462		0.1722	0.69444		0.1681	0.81250
463		0.1727	0.70588		0.1681	0.81250
464		0.1719	0.76667		0.1681	0.81250
465		0.1721	0.70588		0.1688	0.81250
466		0.1736	0.68056		0.1675	0.81250
467		0.1737	0.65278		0.1681	0.81250
468		0.1722	0.71875		0.1675	0.81250
469		0.1734	0.64773		0.1675	0.81250
470		0.1741	0.69444		0.1675	0.81250
471		0.1730	0.67188		0.1681	0.81250
472		0.1727	0.63158		0.1681	0.81250
473		0.1739	0.65789		0.1681	0.81250
474		0.1731	0.69737		0.1681	0.81250
475		0.1710	0.69444		0.1675	0.81250
476		0.1730	0.70312		0.1675	0.81250
477		0.1710	0.71875		0.1675	0.81250
478		0.1726	0.71667		0.1666	0.81250
479		0.1735	0.65278		0.1681	0.81250
480		0.1729	0.76923		0.1681	0.81250
481		0.1729	0.73077		0.1675	0.81250
482		0.1727	0.63158		0.1675	0.81250
483		0.1710	0.78846		0.1672	0.81250
484		0.1708	0.70833		0.1681	0.81250
485		0.1729	0.67647		0.1681	0.81250
486		0.1733	0.72727		0.1688	0.81250
487		0.1723	0.70588		0.1675	0.81250
488		0.1730	0.70312		0.1675	0.81250
489		0.1739	0.64062		0.1688	0.81250
490		0.1735	0.60417		0.1688	0.81250
491		0.1734	0.69643		0.1666	0.81250
492		0.1721	0.71875		0.1672	0.81250
493		0.1725	0.69643		0.1672	0.81250
494		0.1734	0.72222		0.1681	0.81250
495		0.1736	0.67500		0.1688	0.81250
496		0.1730	0.71875		0.1681	0.81250
497		0.1720	0.63333		0.1681	0.81250
498		0.1721	0.70455		0.1675	0.81250
499		0.1739	0.67308		0.1666	0.81250
500		0.1727	0.62500		0.1681	0.81250
501		0.1730	0.73611		0.1675	0.81250
502		0.1722	0.73611		0.1688	0.81250
503		0.1734	0.68750		0.1681	0.81250
504		0.1725	0.71667		0.1675	0.81250
505		0.1740	0.65385		0.1672	0.81250
506		0.1722	0.72727		0.1681	0.81250
507		0.1708	0.71429		0.1681	0.81250
508		0.1723	0.65385		0.1675	0.81250
509		0.1724	0.72917		0.1675	0.81250
510		0.1712	0.72222		0.1681	0.81250
511		0.1736	0.71667		0.1675	0.81250
512		0.1726	0.75000		0.1681	0.81250
513		0.1727	0.69231		0.1688	0.81250
514		0.1743	0.58824		0.1681	0.81250
515		0.1740	0.62500		0.1675	0.81250
516		0.1726	0.68750		0.1675	0.81250
