=============training from sratch============
Program ID: 31596

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy', '2', '0', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
       
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-20 08:25:22.517511
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191020_082522

Info: this is the 2th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191020_082522.
6-fold cross validation: the 2th fold is for test, the 3th fold is for validation, remaining folds are for training.

training dataset: total 23 image files.

validation dataset: total 6 image files.

test dataset: total 6 image files.
Total 23 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy
0 has 19046734 elements, with a rate of  0.7820993610610256 
1 has 5306609 elements, with a rate of  0.2179006389389744 
loss weight = tensor([1.0000, 3.5892])
Network has total 254,664,050 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		23.0565		0.30714		53.0171		0.33577		11.9072		0.47967
5	1.0000e-02		4.3255		0.45290		9.4479		0.58001		5.0465		0.38530
10	1.0000e-02		2.1079		0.65443		3.5114		0.74441		2.5537		0.59293
15	1.0000e-02		2.3788		0.61645		3.2534		0.70334		2.3527		0.66992
20	1.0000e-02		2.4164		0.61013		2.9124		0.73770		2.2985		0.63949
25	1.0000e-02		1.8013		0.69465		3.1087		0.72003		2.6392		0.60793
30	1.0000e-02		1.9787		0.64021		2.9126		0.68216		2.6607		0.57233
35	1.0000e-02		1.8516		0.69530		2.6340		0.76174		4.3480		0.49359
40	1.0000e-02		3.9455		0.63404		568.2638		0.44512		45.6536		0.29200
45	1.0000e-02		3.0704		0.60209		12.6882		0.78609		1.9891		0.70330
50	1.0000e-02		2.3271		0.61278		4.1291		0.72130		3.3029		0.65315
55	1.0000e-02		1.9980		0.65335		6.9310		0.72681		4.8175		0.42035
60	1.0000e-02		2.3320		0.61461		8.0210		0.70400		2.3295		0.65981
65	1.0000e-02		1.9592		0.65075		3.4543		0.68547		1.8460		0.65356
70	1.0000e-02		1.4962		0.68318		14.3178		0.65009		5.3746		0.60370
75	1.0000e-02		1.7092		0.68716		2.2676		0.73945		1.6344		0.71945
80	1.0000e-02		1.4807		0.70313		3.9435		0.64114		2.7207		0.57802
85	1.0000e-02		1.7857		0.70853		2.9617		0.62378		2.6764		0.55899
90	1.0000e-02		1.3901		0.72662		1.6966		0.76315		1.9834		0.68053
95	1.0000e-02		1.3352		0.73863		1.8793		0.73843		1.6506		0.69074
100	1.0000e-02		1.6419		0.70850		1.9916		0.77877		1.8431		0.67956
105	1.0000e-02		1.3621		0.72619		1.8953		0.75829		1.3938		0.71107
110	1.0000e-02		1.3690		0.72997		2.7522		0.67449		2.2641		0.66988
115	1.0000e-02		2.1191		0.70649		3.8718		0.59374		3.9001		0.52636
120	1.0000e-02		1.3837		0.72881		2.5641		0.72673		1.5216		0.69331
125	1.0000e-02		1.4449		0.74270		2.6386		0.73066		1.7334		0.68258
130	1.0000e-02		1.5030		0.71249		5.0395		0.61155		4.4666		0.59524
135	1.0000e-02		1.3135		0.71609		1.8889		0.73170		1.5671		0.72147
140	1.0000e-02		1.4981		0.69676		2.2641		0.69789		2.4011		0.62835
145	1.0000e-03		1.3978		0.76180		2.0956		0.71892		1.6935		0.68759
150	1.0000e-03		0.9750		0.76174		2.4337		0.71081		1.6740		0.70860
155	1.0000e-03		0.9948		0.77576		2.4337		0.73542		1.2657		0.74519
160	1.0000e-03		1.0689		0.77694		2.6698		0.74520		1.5128		0.72407
165	1.0000e-03		0.9396		0.77194		2.3370		0.72856		1.3781		0.72151
170	1.0000e-03		0.9537		0.77426		2.0766		0.75212		1.1436		0.74387
175	1.0000e-03		0.8889		0.78310		2.5478		0.73637		1.1375		0.75159
180	1.0000e-03		1.0019		0.78263		3.3621		0.68358		1.3316		0.72263
185	1.0000e-03		0.8992		0.80651		4.4166		0.69578		1.6051		0.73967
190	1.0000e-03		1.0140		0.79455		5.2586		0.64212		2.3500		0.63454
195	1.0000e-03		1.0269		0.78402		3.3743		0.68243		1.4042		0.73835
200	1.0000e-04		0.9510		0.79603		2.7851		0.73647		1.3045		0.74449
205	1.0000e-04		0.8340		0.78596		2.4500		0.72484		1.3087		0.73093
210	1.0000e-04		0.7496		0.79477		2.8116		0.71137		1.2391		0.74651
215	1.0000e-04		1.0239		0.79114		3.1330		0.72890		1.1277		0.77598
220	1.0000e-04		0.8047		0.79466		2.5611		0.71963		1.2506		0.73839
225	1.0000e-04		0.8127		0.79796		3.3483		0.72899		1.2241		0.77217
230	1.0000e-04		0.9586		0.79878		3.2739		0.72083		1.2022		0.76141
235	1.0000e-04		0.7425		0.79937		2.7974		0.71966		1.1312		0.76142
240	1.0000e-04		0.7949		0.80538		3.9506		0.70867		1.2988		0.77308
245	1.0000e-04		0.8148		0.80031		3.0265		0.71282		1.2095		0.75954
250	1.0000e-04		0.9207		0.79893		3.6406		0.71034		1.2200		0.76765
255	1.0000e-05		0.7505		0.80594		3.3490		0.71011		1.2225		0.75929
260	1.0000e-05		0.8978		0.79971		4.1528		0.69862		1.3240		0.76731
265	1.0000e-05		0.9018		0.79345		3.6501		0.69760		1.2219		0.76275
270	1.0000e-05		0.8764		0.81640		3.3687		0.70916		1.1886		0.76470
275	1.0000e-05		0.7531		0.81442		3.0707		0.70464		1.2860		0.74176
280	1.0000e-05		0.8337		0.80040		3.8645		0.69615		1.2389		0.76913
285	1.0000e-05		0.8455		0.78486		3.2694		0.70596		1.1592		0.76693
290	1.0000e-05		0.9551		0.78826		3.0860		0.70241		1.2568		0.74463
295	1.0000e-05		0.8747		0.79779		3.2292		0.70717		1.1446		0.76149
300	1.0000e-05		0.7856		0.80013		3.6386		0.71748		1.2440		0.76512
305	1.0000e-05		0.7572		0.80829		3.1059		0.70648		1.1283		0.76094
310	1.0000e-06		0.8748		0.80638		3.7818		0.70447		1.1432		0.77850
315	1.0000e-06		0.8443		0.80659		4.5391		0.70285		1.5405		0.75918
320	1.0000e-06		0.7545		0.80235		3.0955		0.70345		1.1805		0.75718
325	1.0000e-06		0.9419		0.78907		2.9225		0.71111		1.2029		0.74943
330	1.0000e-06		0.8861		0.80625		3.6033		0.71108		1.1591		0.77650
335	1.0000e-06		0.7276		0.79336		3.3123		0.70150		1.2149		0.75706
340	1.0000e-06		0.8537		0.81247		5.1957		0.69783		1.7265		0.75481
345	1.0000e-06		0.7067		0.81552		3.7049		0.69630		1.2461		0.75583
350	1.0000e-06		0.7491		0.79113		2.9543		0.69693		1.3273		0.73048
355	1.0000e-06		0.7940		0.79938		3.3390		0.70538		1.2412		0.75361
360	1.0000e-06		0.7289		0.80278		3.4768		0.70338		1.1080		0.77999
365	1.0000e-07		0.7709		0.80303		2.9580		0.71139		1.2022		0.75244
370	1.0000e-07		0.8943		0.80085		3.6567		0.71256		1.1909		0.77144
375	1.0000e-07		1.0498		0.78679		3.5060		0.70552		1.2525		0.76161
380	1.0000e-07		0.7797		0.79698		3.8411		0.70414		1.2117		0.76953
385	1.0000e-07		1.0383		0.80005		3.6323		0.71394		1.1820		0.77664
390	1.0000e-07		0.7895		0.80245		3.1393		0.70514		1.1608		0.75665
395	1.0000e-07		0.7854		0.79059		3.6224		0.70396		1.2408		0.76216
400	1.0000e-07		0.7715		0.81176		4.0827		0.70989		1.2715		0.77558
405	1.0000e-07		0.8874		0.80515		4.5126		0.69831		1.4061		0.76734
410	1.0000e-07		1.0089		0.80767		5.0449		0.71152		1.5104		0.77813
415	1.0000e-07		0.9238		0.79570		3.7848		0.70231		1.3124		0.75595
420	1.0000e-08		1.0336		0.78511		4.6445		0.70836		1.4294		0.76885
425	1.0000e-08		0.7299		0.80535		3.3110		0.71173		1.1429		0.77127
430	1.0000e-08		0.9144		0.78481		2.9773		0.70138		1.3624		0.72898
435	1.0000e-08		0.7798		0.79709		2.7720		0.70900		1.3960		0.72019
440	1.0000e-08		0.7601		0.79357		3.5719		0.70183		1.2151		0.76186
445	1.0000e-08		0.8462		0.79049		3.4063		0.70285		1.1821		0.77082
450	1.0000e-08		0.8771		0.79137		3.4702		0.70334		1.1762		0.76604
455	1.0000e-08		0.8568		0.78928		3.5660		0.69997		1.2499		0.75930
460	1.0000e-08		0.8134		0.81350		3.5039		0.70159		1.1763		0.76310
465	1.0000e-08		0.8002		0.81743		3.5781		0.70981		1.1988		0.76560
470	1.0000e-08		0.9037		0.80243		3.2814		0.70037		1.2021		0.75800
475	1.0000e-08		0.8082		0.80551		3.4952		0.70507		1.2913		0.75238
480	1.0000e-08		0.7113		0.80541		3.2782		0.70230		1.2054		0.75849
485	1.0000e-08		1.1522		0.78834		4.3798		0.69988		1.3319		0.77849
490	1.0000e-08		0.8405		0.80452		3.4191		0.70401		1.1918		0.75801
495	1.0000e-08		0.8345		0.79425		2.8858		0.71056		1.2158		0.74902
500	1.0000e-08		0.8684		0.79996		3.6654		0.70099		1.3659		0.74996
505	1.0000e-08		0.7121		0.81190		3.3285		0.71035		1.1646		0.76997
510	1.0000e-08		0.6886		0.80112		3.2055		0.70007		1.2311		0.74839
515	1.0000e-08		0.8477		0.79697		3.8807		0.70579		1.2171		0.76948
520	1.0000e-08		0.7007		0.81634		4.5885		0.70656		1.4324		0.77115
525	1.0000e-08		0.8750		0.80262		3.5196		0.70379		1.2735		0.75558
530	1.0000e-08		0.7808		0.79485		3.1099		0.70651		1.1911		0.75498
535	1.0000e-08		0.7005		0.79652		3.1061		0.70493		1.1732		0.75797
540	1.0000e-08		0.8788		0.81887		4.7288		0.69548		1.3725		0.77524
545	1.0000e-08		0.8515		0.79668		3.0862		0.70010		1.3302		0.73584
550	1.0000e-08		0.7543		0.80739		3.4912		0.70856		1.2247		0.76089
555	1.0000e-08		0.8103		0.80199		4.7628		0.69983		1.4422		0.77105
560	1.0000e-08		0.7393		0.81120		3.8113		0.71242		1.2738		0.76956
565	1.0000e-08		0.9669		0.80239		3.7275		0.70599		1.1535		0.77647
570	1.0000e-08		0.8246		0.79421		3.4781		0.70969		1.2334		0.75947
575	1.0000e-08		0.8877		0.80589		3.8183		0.70560		1.2242		0.77519
580	1.0000e-08		0.8531		0.79786		3.3381		0.70276		1.2222		0.75550
585	1.0000e-08		0.9534		0.79110		3.1150		0.70842		1.1570		0.76225
590	1.0000e-08		0.7796		0.78968		3.1019		0.70334		1.3060		0.73904
595	1.0000e-08		0.8249		0.78182		3.3906		0.70035		1.2234		0.75782
600	1.0000e-08		0.8981		0.80109		3.8188		0.69569		1.3248		0.75214
605	1.0000e-08		0.7486		0.79299		3.6161		0.70321		1.2645		0.75955
610	1.0000e-08		0.7981		0.80504		3.5123		0.69924		1.1493		0.77614
615	1.0000e-08		0.8713		0.80769		4.3493		0.69654		1.3518		0.77096
620	1.0000e-08		0.7628		0.80978		3.1104		0.70808		1.2322		0.75027
625	1.0000e-08		0.8957		0.81359		3.2235		0.70903		1.1556		0.76540
630	1.0000e-08		0.7763		0.81100		3.3347		0.70709		1.1728		0.76290
635	1.0000e-08		0.8319		0.81008		4.2229		0.70668		1.2954		0.77737
640	1.0000e-08		0.8124		0.79712		3.8699		0.70387		1.2627		0.76923
