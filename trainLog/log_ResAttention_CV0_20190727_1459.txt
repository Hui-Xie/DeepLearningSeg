Program ID: 485

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/images_npy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '0']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters.
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   training data has 169 patients 

Training strategy: 

          

Program starting Time: 2019-07-27 14:59:46.945077
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/ResNeXtAtten

Info: this is the 0th fold leave for test in the 4-fold cross-validation, with 20.0% of data for independent test.

Infor: In all data of 169 files, label 0 has 51 files,
	  and label 1 has 118 files, where positive response rate = 0.6982248520710059 in full data
Infor: the independent test set has 34 files,and Training including validation Set has 135 files which will be divided into 4 folds.
In the independent test set of 34 files, 0 has 10, 1 has 24, the rate of 1s is 0.7058823529411765
In this test_0 dataset, total 34 files, where 1 has 24 with rate of 0.7058823529411765
In this train_0 dataset, total 100 files, where 1 has 70 with rate of 0.7
In this validation_0 dataset, total 35 files, where 1 has 24 with rate of 0.6857142857142857
=== Network trains from scratch ====
Network has total 1,235,220 parameters.
Info: program will use 2 GPUs.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	0.7051	0.5000	0.6000	0.2667		0.3994	0.6857	1.0000	0.0000		0.3899	0.7059	1.0000	0.0000
5	0.5813	0.5000	0.3714	0.8000		0.5597	0.3714	0.0833	1.0000		0.5922	0.3235	0.0833	0.9000
10	0.4619	0.5200	0.3857	0.8333		0.7838	0.4286	0.2917	0.7273		0.9415	0.3529	0.2500	0.6000
15	0.3464	0.6700	0.5857	0.8667		0.5663	0.6286	0.7500	0.3636		0.7474	0.5000	0.5833	0.3000
20	0.3283	0.8200	0.8714	0.7000		0.8938	0.4286	0.2917	0.7273		1.1347	0.4118	0.2083	0.9000
25	0.2531	0.8100	0.8857	0.6333		0.8159	0.5143	0.5000	0.5455		0.9522	0.4706	0.4583	0.5000
30	0.2322	0.7800	0.7571	0.8333		0.6792	0.5143	0.4583	0.6364		0.8585	0.5294	0.5833	0.4000
35	0.3040	0.7600	0.7857	0.7000		0.7646	0.7143	0.8333	0.4545		1.0453	0.5882	0.7083	0.3000
40	0.3404	0.8000	0.7714	0.8667		0.8372	0.5143	0.5000	0.5455		0.7517	0.5000	0.3750	0.8000
45	0.2172	0.8300	0.7571	1.0000		1.0580	0.6286	0.6250	0.6364		1.0843	0.5882	0.5833	0.6000
50	0.3128	0.9000	0.9429	0.8000		1.2212	0.7143	0.8333	0.4545		1.4026	0.7059	0.8333	0.4000
55	0.0971	0.9200	0.9714	0.8000		1.1759	0.6857	0.7917	0.4545		1.1585	0.6765	0.7500	0.5000
60	0.2466	0.8700	0.8857	0.8333		1.1060	0.6571	0.7917	0.3636		1.2463	0.6471	0.7500	0.4000
65	0.2046	0.8300	0.7714	0.9667		0.9638	0.6571	0.6667	0.6364		1.0379	0.6176	0.5833	0.7000
70	0.2062	0.8700	0.8286	0.9667		1.1915	0.5429	0.5000	0.6364		1.4327	0.4412	0.4583	0.4000
75	0.1568	0.8800	0.8429	0.9667		1.1995	0.6000	0.5833	0.6364		1.2446	0.4706	0.4583	0.5000
80	0.0679	0.9600	0.9571	0.9667		1.1067	0.6000	0.6250	0.5455		1.2349	0.5588	0.6667	0.3000
85	0.0817	0.9500	0.9429	0.9667		1.1741	0.6286	0.7083	0.4545		1.2021	0.5882	0.7083	0.3000
90	0.1044	0.9500	0.9286	1.0000		1.1897	0.6286	0.7917	0.2727		1.2928	0.6471	0.7917	0.3000
95	0.1256	0.9300	0.9000	1.0000		1.0898	0.7143	0.8750	0.3636		1.2856	0.6176	0.7500	0.3000
100	0.0630	0.9600	0.9714	0.9333		1.1656	0.6857	0.7917	0.4545		1.4148	0.6471	0.7500	0.4000
105	0.1368	0.9200	0.9286	0.9000		1.2087	0.5714	0.5833	0.5455		1.3035	0.5882	0.6250	0.5000
110	0.0860	0.9600	0.9714	0.9333		1.5178	0.7143	0.8750	0.3636		1.6484	0.6176	0.7500	0.3000
115	0.1000	0.9500	0.9429	0.9667		1.3313	0.6571	0.7917	0.3636		1.3611	0.6471	0.7917	0.3000
120	0.0550	0.9800	0.9857	0.9667		1.3849	0.7143	0.8333	0.4545		1.4860	0.6176	0.7500	0.3000
125	0.0900	0.9500	0.9571	0.9333		1.1532	0.5429	0.5833	0.4545		1.2714	0.5588	0.6667	0.3000
130	0.0917	0.9600	0.9857	0.9000		1.3244	0.6000	0.6250	0.5455		1.4306	0.6176	0.7500	0.3000
135	0.1434	0.9800	0.9857	0.9667		1.2981	0.6000	0.6250	0.5455		1.3700	0.5588	0.6667	0.3000
140	0.0649	0.9700	0.9714	0.9667		1.3771	0.6000	0.6667	0.4545		1.3862	0.6471	0.7500	0.4000
145	0.0805	0.9700	0.9857	0.9333		1.1296	0.6286	0.7500	0.3636		1.1710	0.6176	0.7500	0.3000
150	0.0362	0.9900	0.9857	1.0000		1.3275	0.5714	0.6250	0.4545		1.3829	0.5882	0.6667	0.4000
155	0.0956	0.9500	0.9571	0.9333		1.2598	0.6000	0.6667	0.4545		1.3490	0.6176	0.7500	0.3000
160	0.0609	0.9600	0.9571	0.9667		1.3758	0.5714	0.6250	0.4545		1.4509	0.6471	0.7500	0.4000
165	0.0471	0.9900	0.9857	1.0000		1.4688	0.6000	0.6250	0.5455		1.5773	0.5588	0.6667	0.3000
170	0.0772	0.9600	0.9571	0.9667		1.3017	0.6000	0.6250	0.5455		1.4155	0.6176	0.7083	0.4000
175	0.0910	0.9600	0.9429	1.0000		1.3101	0.6286	0.7083	0.4545		1.3786	0.6176	0.7500	0.3000
180	0.1245	0.9600	0.9571	0.9667		1.4194	0.6000	0.6250	0.5455		1.4757	0.5882	0.6667	0.4000
185	0.0568	0.9600	0.9571	0.9667		1.4093	0.5714	0.5833	0.5455		1.4300	0.5882	0.6667	0.4000
190	0.0378	0.9900	0.9857	1.0000		1.4387	0.6000	0.6250	0.5455		1.4747	0.5588	0.6667	0.3000
195	0.1633	0.9600	0.9857	0.9000		1.5677	0.6286	0.7083	0.4545		1.6152	0.6471	0.7500	0.4000
200	0.0684	0.9400	0.9143	1.0000		1.3987	0.5714	0.6250	0.4545		1.4365	0.5882	0.6667	0.4000
205	0.1111	0.9800	0.9857	0.9667		1.3823	0.5714	0.6250	0.4545		1.3926	0.6176	0.7500	0.3000
210	0.0922	0.9800	0.9857	0.9667		1.4201	0.5714	0.6250	0.4545		1.5257	0.5588	0.6667	0.3000
215	0.0376	0.9800	0.9714	1.0000		1.3006	0.5714	0.6250	0.4545		1.3455	0.6176	0.7083	0.4000
220	0.0569	0.9800	0.9857	0.9667		1.3541	0.5714	0.6250	0.4545		1.3983	0.6176	0.7500	0.3000
225	0.1011	0.9500	0.9714	0.9000		1.3220	0.5714	0.6250	0.4545		1.4264	0.6471	0.7500	0.4000
230	0.0812	0.9700	0.9571	1.0000		1.3824	0.6000	0.6667	0.4545		1.4622	0.6176	0.7500	0.3000
235	0.0427	0.9900	0.9857	1.0000		1.3700	0.5714	0.6250	0.4545		1.4414	0.5882	0.7083	0.3000
240	0.0780	0.9600	0.9571	0.9667		1.4100	0.6571	0.7500	0.4545		1.4964	0.6176	0.7500	0.3000
245	0.1342	0.9600	0.9857	0.9000		1.3622	0.5714	0.6250	0.4545		1.4824	0.6176	0.7500	0.3000
250	0.1066	0.9600	0.9571	0.9667		1.4839	0.6286	0.7083	0.4545		1.5463	0.6176	0.7500	0.3000
255	0.0761	0.9900	1.0000	0.9667		1.1932	0.5714	0.6250	0.4545		1.3082	0.6176	0.7500	0.3000
