=============training from sratch============
Program ID: 31446

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy', '3', '1', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
       
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-20 08:19:21.561085
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191020_081921

Info: this is the 3th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191020_081921.
6-fold cross validation: the 3th fold is for test, the 4th fold is for validation, remaining folds are for training.

training dataset: total 23 image files.

validation dataset: total 6 image files.

test dataset: total 6 image files.
Total 23 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy
0 has 19147801 elements, with a rate of  0.7862493867884996 
1 has 5205542 elements, with a rate of  0.21375061321150038 
loss weight = tensor([1.0000, 3.6783])
Network has total 254,664,050 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		70.4057		0.17563		102.7691		0.10136		183.9033		0.05689
5	1.0000e-02		8.7466		0.49673		4.5372		0.64771		10.0057		0.74799
10	1.0000e-02		2.4104		0.63899		2.6909		0.62042		3.6790		0.70570
15	1.0000e-02		2.0789		0.66733		2.0932		0.65793		2.3813		0.76111
20	1.0000e-02		1.7864		0.68183		3.0104		0.58347		2.8400		0.70692
25	1.0000e-02		2.0872		0.64391		1.9471		0.66712		2.0897		0.76315
30	1.0000e-02		2.6744		0.64736		2.4935		0.61902		3.0308		0.71676
35	1.0000e-02		1.7215		0.72265		3.5170		0.63027		5.8513		0.73605
40	1.0000e-02		1.7773		0.68177		4.8515		0.50622		4.7867		0.51972
45	1.0000e-02		2.0771		0.67450		5.0138		0.67093		12.6251		0.74323
50	1.0000e-02		1.7809		0.67650		3.5220		0.61410		4.0163		0.72706
55	1.0000e-02		1.5621		0.69339		4.0215		0.57498		5.2837		0.66281
60	1.0000e-02		1.2985		0.71317		2.6025		0.60957		2.9004		0.70786
65	1.0000e-02		1.5004		0.73329		4.1589		0.53212		5.2412		0.47957
70	1.0000e-02		1.9753		0.65323		2.9420		0.62233		2.7698		0.74461
75	1.0000e-02		1.8772		0.70272		2.6391		0.64171		2.7701		0.74429
80	1.0000e-03		1.3557		0.73856		4.3040		0.56901		5.1507		0.61955
85	1.0000e-03		1.4370		0.74898		3.1702		0.56163		4.4586		0.68136
90	1.0000e-03		1.1946		0.74864		4.5439		0.42764		3.9039		0.72061
95	1.0000e-03		1.3642		0.73836		5.2029		0.46173		4.1303		0.72867
100	1.0000e-03		1.0125		0.77082		6.0350		0.42966		4.1182		0.66505
105	1.0000e-03		0.9932		0.78407		6.8208		0.41245		4.0557		0.64682
110	1.0000e-03		1.0959		0.76897		5.0540		0.45954		3.9139		0.68487
115	1.0000e-03		1.2366		0.75762		5.3585		0.49532		4.7275		0.62878
120	1.0000e-03		1.0000		0.78657		5.6582		0.46408		5.0975		0.57679
125	1.0000e-03		0.9596		0.78885		5.3655		0.48972		3.6780		0.63783
130	1.0000e-03		1.1306		0.79722		4.1981		0.49739		2.4862		0.73068
135	1.0000e-04		0.8891		0.78819		5.9521		0.48643		3.7864		0.62074
140	1.0000e-04		1.0423		0.79041		4.3424		0.54317		3.4240		0.67334
145	1.0000e-04		0.9260		0.78597		5.1405		0.51577		3.5176		0.66109
150	1.0000e-04		0.9515		0.79448		4.6150		0.52133		2.9434		0.68713
155	1.0000e-04		0.9181		0.79956		5.3227		0.44703		2.9577		0.69973
160	1.0000e-04		1.0976		0.79180		5.8417		0.45065		3.1984		0.68702
165	1.0000e-04		0.9670		0.80215		6.1081		0.44617		2.7954		0.74309
170	1.0000e-04		0.8430		0.80535		5.0285		0.51924		3.2830		0.68821
175	1.0000e-04		0.8424		0.80311		5.5138		0.49118		3.0429		0.69871
180	1.0000e-04		0.8525		0.79714		5.6981		0.49284		3.0292		0.70723
185	1.0000e-04		0.9972		0.79713		5.9682		0.46768		2.9348		0.71674
190	1.0000e-05		0.8792		0.80174		6.1749		0.45006		3.0379		0.69585
195	1.0000e-05		0.9736		0.78000		5.3271		0.50612		3.3969		0.67225
200	1.0000e-05		0.9437		0.80007		5.5542		0.48021		3.5167		0.64188
205	1.0000e-05		0.7640		0.80950		5.6726		0.42661		2.7392		0.71126
210	1.0000e-05		0.8992		0.80190		5.0856		0.47615		2.7264		0.71187
215	1.0000e-05		0.8984		0.80478		5.4249		0.48568		2.7753		0.71408
220	1.0000e-05		0.8500		0.79308		5.6507		0.46003		3.0161		0.69484
225	1.0000e-05		0.9178		0.78119		5.4890		0.48549		3.0352		0.70727
230	1.0000e-05		1.1888		0.78350		5.3799		0.51163		3.0599		0.69814
235	1.0000e-05		0.9660		0.78864		5.3147		0.51772		3.7090		0.68694
240	1.0000e-05		0.8995		0.79450		5.5967		0.45608		2.7712		0.71453
245	1.0000e-06		0.9124		0.79763		5.9501		0.46600		3.1757		0.68229
250	1.0000e-06		0.8068		0.79812		5.5462		0.48267		3.0961		0.68384
255	1.0000e-06		1.1050		0.79313		4.5800		0.54126		3.0282		0.70610
260	1.0000e-06		0.8212		0.78725		5.0887		0.50898		3.7712		0.64627
265	1.0000e-06		0.7355		0.79990		5.1721		0.52278		3.5880		0.67055
270	1.0000e-06		0.8560		0.80542		5.1687		0.52531		3.1235		0.68671
275	1.0000e-06		0.7885		0.80356		5.5550		0.49692		3.3541		0.68869
280	1.0000e-06		1.1316		0.80368		6.0538		0.44436		2.7978		0.70019
285	1.0000e-06		0.9073		0.80293		4.2522		0.53603		2.8658		0.70248
290	1.0000e-06		0.8652		0.78194		4.4170		0.53232		2.7908		0.70470
295	1.0000e-06		0.9513		0.79557		6.2668		0.45318		3.1625		0.69657
300	1.0000e-07		0.8067		0.79825		5.2055		0.50856		3.0451		0.70679
305	1.0000e-07		0.9267		0.79754		5.3233		0.48766		2.9582		0.70319
310	1.0000e-07		0.9036		0.79790		4.9321		0.49436		3.0209		0.71505
315	1.0000e-07		0.9904		0.79210		4.3033		0.53915		2.7277		0.70847
320	1.0000e-07		0.8927		0.80804		4.1889		0.54733		2.8857		0.70659
325	1.0000e-07		0.8481		0.80541		5.6554		0.47655		2.9517		0.69011
330	1.0000e-07		0.8983		0.80038		5.4573		0.47969		2.9828		0.69968
335	1.0000e-07		0.9394		0.80200		5.3042		0.53805		4.6859		0.60178
340	1.0000e-07		0.8547		0.80933		5.2838		0.53804		3.9966		0.64976
345	1.0000e-07		0.9869		0.80116		5.3121		0.50653		3.0440		0.70951
350	1.0000e-07		0.9649		0.78839		4.9074		0.51890		3.2952		0.69555
355	1.0000e-08		0.8239		0.80519		5.2012		0.50746		3.0615		0.69414
360	1.0000e-08		0.7757		0.79883		5.4555		0.48074		3.0203		0.69340
365	1.0000e-08		1.0493		0.80011		6.1298		0.44013		3.0152		0.70423
370	1.0000e-08		0.7859		0.78961		4.5315		0.55746		4.7020		0.60386
375	1.0000e-08		0.9652		0.78254		4.9991		0.48915		2.9055		0.68872
380	1.0000e-08		0.9802		0.79508		7.0259		0.43673		3.0542		0.71967
385	1.0000e-08		0.9145		0.79238		4.6054		0.54821		2.9741		0.68938
390	1.0000e-08		0.9047		0.80992		6.4207		0.45864		4.7043		0.58154
395	1.0000e-08		1.1085		0.78935		6.1331		0.44381		2.7171		0.72536
400	1.0000e-08		0.8429		0.79911		5.0048		0.49633		2.8994		0.71499
405	1.0000e-08		0.8413		0.80741		5.2878		0.51515		3.3222		0.67887
410	1.0000e-08		0.8108		0.81108		4.6226		0.53039		2.8548		0.70260
415	1.0000e-08		0.8467		0.79841		5.6275		0.47181		2.8089		0.71242
420	1.0000e-08		0.8679		0.78562		4.6060		0.53673		2.8802		0.70286
425	1.0000e-08		0.8678		0.80310		4.6957		0.52569		2.8672		0.70803
430	1.0000e-08		0.8221		0.79922		5.1942		0.51334		4.0940		0.61522
435	1.0000e-08		0.9278		0.79446		4.9430		0.52491		2.9182		0.70502
440	1.0000e-08		0.8064		0.79707		5.0193		0.52283		3.1420		0.68637
445	1.0000e-08		0.8150		0.78801		5.3373		0.49000		2.8679		0.70951
450	1.0000e-08		0.9780		0.79857		5.5209		0.51131		3.3632		0.67977
455	1.0000e-08		0.8767		0.79996		5.0244		0.52950		3.4370		0.66404
460	1.0000e-08		0.8243		0.79322		5.3605		0.50553		3.2953		0.69401
465	1.0000e-08		0.8354		0.79563		5.3868		0.50553		3.2986		0.67587
470	1.0000e-08		0.9068		0.78309		5.0406		0.53346		3.8198		0.67068
475	1.0000e-08		1.0018		0.79463		5.6375		0.46279		3.1488		0.67733
480	1.0000e-08		0.9573		0.79591		4.7047		0.49614		2.6085		0.72816
485	1.0000e-08		1.0340		0.79754		5.6732		0.45740		2.8218		0.69427
490	1.0000e-08		0.9464		0.79912		5.1409		0.52043		3.1606		0.68768
495	1.0000e-08		0.8154		0.79863		5.5061		0.46427		2.8180		0.70290
500	1.0000e-08		0.9786		0.81073		5.7805		0.47709		2.9592		0.71565
505	1.0000e-08		1.0411		0.78239		5.8959		0.48743		3.6964		0.66273
510	1.0000e-08		0.9523		0.78741		4.6926		0.53614		3.0436		0.69869
515	1.0000e-08		1.0251		0.79863		5.6884		0.43062		2.6463		0.72640
520	1.0000e-08		1.0772		0.77713		5.5450		0.47255		2.9723		0.69487
525	1.0000e-08		0.9144		0.80161		4.4650		0.51313		2.7130		0.71899
530	1.0000e-08		0.9687		0.79318		4.5134		0.54640		2.9360		0.70023
535	1.0000e-08		0.9090		0.80366		6.2272		0.42916		3.2335		0.67030
540	1.0000e-08		0.8651		0.80535		5.0761		0.52247		3.1195		0.69317
545	1.0000e-08		0.9277		0.79328		5.4115		0.50986		3.3258		0.68020
550	1.0000e-08		0.8347		0.80516		5.0587		0.51592		3.1753		0.68921
555	1.0000e-08		0.8522		0.79317		4.8823		0.53568		3.5904		0.68508
560	1.0000e-08		0.8250		0.78822		5.1879		0.49467		3.2647		0.70158
565	1.0000e-08		0.9070		0.80286		5.6130		0.47132		3.1115		0.68942
570	1.0000e-08		0.8924		0.79789		5.2281		0.50284		2.9940		0.69133
575	1.0000e-08		0.8590		0.79526		5.6993		0.49037		3.0896		0.69240
580	1.0000e-08		0.8581		0.79895		5.1261		0.50637		2.9590		0.69277
585	1.0000e-08		0.8403		0.81562		4.9667		0.48476		2.8254		0.70351
590	1.0000e-08		1.0088		0.77804		4.8194		0.53340		3.1551		0.69792
595	1.0000e-08		1.2087		0.79295		6.6439		0.41973		2.9433		0.71031
600	1.0000e-08		0.8683		0.80189		5.5788		0.48688		5.8306		0.55829
605	1.0000e-08		1.0138		0.79684		5.0659		0.54025		3.6017		0.66525
610	1.0000e-08		0.7672		0.79922		5.4210		0.47571		2.9363		0.70716
615	1.0000e-08		0.7835		0.80384		5.7664		0.45885		2.9191		0.69514
620	1.0000e-08		0.7766		0.80354		5.1842		0.51590		3.4980		0.65737
625	1.0000e-08		0.8008		0.79410		5.0628		0.52974		3.2404		0.68780
630	1.0000e-08		0.8242		0.78605		4.5108		0.52336		2.7274		0.70814
635	1.0000e-08		0.9649		0.81080		5.5166		0.50082		3.4325		0.68625
640	1.0000e-08		0.9210		0.80155		5.1981		0.51447		3.0210		0.70854
