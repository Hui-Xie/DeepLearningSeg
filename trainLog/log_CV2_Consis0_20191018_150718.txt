=============training from sratch============
Program ID: 17729

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy', '2', '3', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
       
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-18 15:07:18.862210
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191018_150718

Info: this is the 2th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191018_150718.
6-fold cross validation: the 2th fold is for test, the 3th fold is for validation, remaining folds are for training.

training dataset: total 23 image files.

validation dataset: total 6 image files.

test dataset: total 6 image files.
Total 23 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy
0 has 19046734 elements, with a rate of  0.7820993610610256 
1 has 5306609 elements, with a rate of  0.2179006389389744 
loss weight = tensor([1.0000, 3.5892])
Network has total 254,652,146 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		5.6367		0.32649		3.7783		0.59537		3.9186		0.37263
5	1.0000e-02		2.3232		0.62851		1.9563		0.75590		1.9160		0.68672
10	1.0000e-02		2.2289		0.65619		1.6699		0.75575		1.8326		0.65957
15	1.0000e-02		2.1253		0.63882		1.7363		0.75727		1.7318		0.70058
20	1.0000e-02		2.2089		0.64460		1.6940		0.72517		2.6738		0.61125
25	1.0000e-02		2.0672		0.67770		2.4311		0.69893		4.1560		0.55331
30	1.0000e-02		1.6801		0.68200		1.5052		0.78865		1.8365		0.67702
35	1.0000e-02		1.6517		0.67292		1.7282		0.76176		1.3683		0.73805
40	1.0000e-02		1.4768		0.70811		2.2598		0.74725		1.8241		0.68824
45	1.0000e-02		1.7348		0.70552		1.6823		0.72015		2.0199		0.65842
50	1.0000e-02		1.6574		0.71084		1.4848		0.78854		1.2553		0.76538
55	1.0000e-02		1.3820		0.72913		2.5235		0.73754		1.7131		0.70503
60	1.0000e-02		1.5430		0.71459		1.9338		0.73325		1.6101		0.69557
65	1.0000e-02		1.6185		0.71484		2.3917		0.70280		1.5886		0.69356
70	1.0000e-02		1.3079		0.74428		1.8627		0.74468		1.2246		0.77408
75	1.0000e-02		1.2956		0.76696		2.5964		0.71320		1.2020		0.75617
80	1.0000e-02		1.1470		0.77605		1.8684		0.74745		1.4510		0.73630
85	1.0000e-02		1.1425		0.75107		1.9642		0.73349		1.4537		0.70939
90	1.0000e-02		1.2096		0.76029		1.4276		0.75568		1.4521		0.72471
95	1.0000e-02		0.7867		0.79327		1.8298		0.72185		1.9209		0.69212
100	1.0000e-02		1.0354		0.76823		2.7674		0.64993		2.2630		0.65093
105	1.0000e-02		1.2687		0.75719		1.9366		0.72248		1.6471		0.69482
110	1.0000e-02		1.1037		0.77345		1.6768		0.74765		1.3223		0.74479
115	1.0000e-02		1.3370		0.76270		2.5332		0.68486		1.0725		0.76698
120	1.0000e-02		1.1968		0.75530		2.3527		0.69063		1.7526		0.68529
125	1.0000e-02		1.0431		0.77194		1.9794		0.74926		0.9159		0.80716
130	1.0000e-02		0.8439		0.78300		1.6414		0.75148		1.0694		0.79522
135	1.0000e-02		1.0090		0.79898		2.5924		0.70942		1.0622		0.76089
140	1.0000e-02		1.6958		0.70766		2.0176		0.70892		1.8348		0.64003
145	1.0000e-03		0.9748		0.78759		1.8118		0.76320		0.9765		0.80636
150	1.0000e-03		0.8622		0.80347		1.8248		0.74273		0.9592		0.79181
155	1.0000e-03		0.6934		0.82898		2.0050		0.75649		1.0728		0.80270
160	1.0000e-03		0.6751		0.82487		1.7885		0.75286		1.0273		0.79118
165	1.0000e-03		0.8060		0.82789		1.9642		0.74497		0.9487		0.79230
170	1.0000e-03		0.6827		0.83199		2.0242		0.74542		0.9484		0.79501
175	1.0000e-03		0.7166		0.82397		1.8847		0.74739		0.9422		0.78772
180	1.0000e-03		0.8223		0.81745		1.9659		0.75002		0.9116		0.80086
185	1.0000e-03		0.5380		0.84787		2.2950		0.73598		0.9718		0.80064
190	1.0000e-03		0.5433		0.83879		2.2335		0.73742		0.9826		0.78542
195	1.0000e-03		0.6508		0.83638		1.9695		0.74779		1.0967		0.78164
200	1.0000e-04		0.5618		0.85084		2.1905		0.74166		1.0215		0.78918
205	1.0000e-04		0.5462		0.85049		2.2158		0.73543		0.9621		0.78501
210	1.0000e-04		0.5337		0.84781		2.1046		0.74640		0.9480		0.79557
215	1.0000e-04		0.5710		0.83947		2.4723		0.72118		0.9978		0.77986
220	1.0000e-04		0.5144		0.84072		2.2381		0.73671		0.9311		0.79830
225	1.0000e-04		0.5377		0.84943		2.0149		0.76027		1.0504		0.78478
230	1.0000e-04		0.5456		0.85308		2.0941		0.75176		0.9861		0.79503
235	1.0000e-04		0.5539		0.84263		2.2177		0.73179		1.0046		0.77819
240	1.0000e-04		0.5818		0.85217		2.1645		0.74824		0.9834		0.79867
245	1.0000e-04		0.5095		0.86459		2.1117		0.75162		0.9431		0.79859
250	1.0000e-04		0.5293		0.83801		2.5172		0.71772		1.0940		0.76542
255	1.0000e-05		0.5146		0.85334		2.0663		0.74382		0.9816		0.78762
260	1.0000e-05		0.5175		0.84657		2.1586		0.74263		0.9683		0.80067
265	1.0000e-05		0.5483		0.84239		2.1937		0.73858		0.9775		0.78608
270	1.0000e-05		0.4607		0.83672		2.1406		0.74232		0.9429		0.80066
275	1.0000e-05		0.6478		0.83433		2.0077		0.75328		1.0018		0.78495
280	1.0000e-05		0.4798		0.84407		2.1904		0.73493		0.9877		0.78525
285	1.0000e-05		0.5740		0.85171		2.0795		0.74855		0.9948		0.79069
290	1.0000e-05		0.5213		0.84694		2.1192		0.74633		0.9929		0.78502
295	1.0000e-05		0.4293		0.85679		2.3086		0.73071		0.9996		0.78108
300	1.0000e-05		0.5991		0.84968		2.1916		0.74375		1.0134		0.80214
305	1.0000e-05		0.4971		0.85792		2.0702		0.75726		0.9959		0.79414
310	1.0000e-06		0.5480		0.85592		2.3088		0.73348		0.9983		0.78309
315	1.0000e-06		0.4756		0.84553		2.1492		0.74093		0.9497		0.79580
320	1.0000e-06		0.6552		0.85347		2.0951		0.75662		1.0270		0.78347
325	1.0000e-06		0.5540		0.84467		2.0712		0.74489		1.0186		0.77963
330	1.0000e-06		0.6242		0.84822		2.1718		0.74218		0.9835		0.78605
335	1.0000e-06		0.5001		0.85025		2.2485		0.73792		0.9933		0.78580
340	1.0000e-06		0.4715		0.86023		2.0781		0.75273		0.9798		0.79129
345	1.0000e-06		0.6221		0.84580		2.2285		0.74080		0.9688		0.79046
350	1.0000e-06		0.5964		0.85617		2.4595		0.72783		1.0005		0.78241
355	1.0000e-06		0.5340		0.84513		2.2521		0.73585		1.0011		0.78435
360	1.0000e-06		0.6165		0.83705		2.2536		0.73341		0.9915		0.78319
365	1.0000e-07		0.5570		0.85610		2.0570		0.75527		0.9952		0.79042
370	1.0000e-07		0.6050		0.85719		2.1913		0.74693		1.0540		0.79374
375	1.0000e-07		0.6186		0.84020		2.0843		0.74998		0.9627		0.79009
380	1.0000e-07		0.5651		0.85735		2.1496		0.75796		1.0987		0.78880
385	1.0000e-07		0.5079		0.85570		2.0894		0.75755		1.0091		0.79595
390	1.0000e-07		0.6348		0.84486		2.2415		0.73531		0.9840		0.78684
395	1.0000e-07		0.6001		0.84578		1.9793		0.76355		1.0247		0.78749
400	1.0000e-07		0.5214		0.85734		2.1904		0.74672		0.9442		0.79888
405	1.0000e-07		0.6767		0.84325		1.9476		0.76163		1.0446		0.78354
410	1.0000e-07		0.4679		0.85898		2.4607		0.72434		1.0011		0.78308
415	1.0000e-07		0.5711		0.84659		2.1366		0.75030		1.0211		0.80177
420	1.0000e-08		0.5070		0.85260		2.2298		0.73726		0.9435		0.79518
425	1.0000e-08		0.6467		0.85194		2.1081		0.75245		0.9638		0.79077
430	1.0000e-08		0.6008		0.85889		2.1635		0.74403		0.9556		0.79373
435	1.0000e-08		0.5356		0.85610		2.1648		0.74348		0.9547		0.79954
440	1.0000e-08		0.4804		0.85705		2.2299		0.73999		0.9423		0.79897
445	1.0000e-08		0.5984		0.84945		2.1577		0.74487		0.9742		0.78995
450	1.0000e-08		0.5026		0.85035		2.1588		0.74169		0.9584		0.79542
455	1.0000e-08		0.5543		0.85479		2.1694		0.74719		1.0196		0.77966
460	1.0000e-08		0.5944		0.84294		2.2460		0.73995		0.9367		0.80320
465	1.0000e-08		0.5252		0.84360		2.0711		0.75210		1.0229		0.79100
470	1.0000e-08		0.6881		0.84372		2.2633		0.74276		0.9799		0.79936
475	1.0000e-08		0.5608		0.83094		2.2055		0.74048		0.9684		0.79139
480	1.0000e-08		0.5822		0.84171		2.1459		0.74513		0.9709		0.78687
485	1.0000e-08		0.5041		0.85868		2.0731		0.75077		1.0055		0.78443
490	1.0000e-08		0.4532		0.85336		2.0524		0.75143		0.9524		0.79586
495	1.0000e-08		0.6068		0.84777		2.1002		0.74340		0.9464		0.79565
500	1.0000e-08		0.5192		0.85725		2.1475		0.75420		1.0123		0.79066
505	1.0000e-08		0.4639		0.84915		2.1883		0.73825		0.9397		0.79400
510	1.0000e-08		0.4949		0.85631		2.1537		0.74656		0.9418		0.79695
515	1.0000e-08		0.5905		0.84038		2.1822		0.74336		1.0186		0.77912
520	1.0000e-08		0.4865		0.85571		2.0643		0.75454		0.9732		0.78969
525	1.0000e-08		0.5281		0.85053		2.1992		0.74260		0.9350		0.79627
530	1.0000e-08		0.5304		0.85432		2.1514		0.74657		0.9252		0.79741
535	1.0000e-08		0.5181		0.85738		2.2549		0.73955		0.9513		0.80135
540	1.0000e-08		0.5064		0.85536		2.2279		0.74009		0.9539		0.79029
545	1.0000e-08		0.6758		0.83596		2.1520		0.74663		0.9966		0.78459
550	1.0000e-08		0.5249		0.84770		2.1628		0.74050		0.9713		0.78885
555	1.0000e-08		0.4864		0.86260		2.2597		0.73612		0.9460		0.79256
560	1.0000e-08		0.4842		0.85357		2.2737		0.73179		0.9850		0.78397
565	1.0000e-08		0.4889		0.86265		2.0874		0.75781		0.9847		0.79632
570	1.0000e-08		0.5080		0.84985		2.0657		0.75027		0.9655		0.79215
575	1.0000e-08		0.5660		0.84325		2.0720		0.74893		0.9975		0.78841
580	1.0000e-08		0.5092		0.83724		2.0983		0.74702		0.9716		0.78928
585	1.0000e-08		0.4852		0.85558		2.2479		0.74417		0.9900		0.79745
590	1.0000e-08		0.5770		0.84597		2.0257		0.75946		0.9955		0.78824
595	1.0000e-08		0.4715		0.85750		1.9933		0.76160		1.0915		0.79393
600	1.0000e-08		0.5222		0.85829		2.0600		0.75652		0.9929		0.78702
605	1.0000e-08		0.6265		0.85178		2.1177		0.74961		0.9861		0.78713
610	1.0000e-08		0.5649		0.84930		2.1002		0.75401		1.0546		0.79819
615	1.0000e-08		0.4434		0.84652		2.2248		0.73626		0.9462		0.79263
620	1.0000e-08		0.4882		0.85656		2.0771		0.74633		0.9644		0.79387
625	1.0000e-08		0.6337		0.86152		2.1294		0.75795		1.0981		0.78666
630	1.0000e-08		0.5655		0.84883		2.1903		0.74054		1.0023		0.78254
635	1.0000e-08		0.4534		0.84799		2.1743		0.73698		0.9277		0.79442
640	1.0000e-08		0.5077		0.85325		2.3847		0.73062		1.0241		0.78306
645	1.0000e-08		0.4700		0.85405		2.1906		0.73963		0.9789		0.78409
650	1.0000e-08		0.6381		0.84434		2.1071		0.75239		1.0001		0.78644
655	1.0000e-08		0.5655		0.83697		2.3894		0.72741		1.0067		0.78084
660	1.0000e-08		0.6084		0.85043		2.1282		0.74851		0.9676		0.79195
665	1.0000e-08		0.6647		0.84473		2.0432		0.75782		1.0360		0.78425
670	1.0000e-08		0.5833		0.86173		2.1791		0.75792		1.1006		0.78422
675	1.0000e-08		0.5701		0.83662		2.1201		0.74305		0.9586		0.78997
680	1.0000e-08		0.5114		0.84055		2.3233		0.72725		1.0310		0.77463
685	1.0000e-08		0.5250		0.84737		2.1987		0.73981		1.0610		0.77488
690	1.0000e-08		0.5192		0.87720		2.1741		0.75282		1.0265		0.78518
695	1.0000e-08		0.4733		0.85833		2.2533		0.73610		0.9639		0.78808
700	1.0000e-08		0.4826		0.84654		2.0663		0.75125		0.9606		0.79974
705	1.0000e-08		0.5654		0.83958		2.2473		0.73412		0.9711		0.78658
710	1.0000e-08		0.5291		0.85755		2.1454		0.75039		0.9399		0.79544
715	1.0000e-08		0.6289		0.84407		2.1578		0.74021		0.9880		0.78342
720	1.0000e-08		0.5048		0.86437		2.0929		0.74695		0.9598		0.78908
725	1.0000e-08		0.5712		0.84978		2.1021		0.75640		1.0574		0.78486
730	1.0000e-08		0.4948		0.83870		2.3052		0.72938		1.0695		0.76891
735	1.0000e-08		0.6388		0.85113		2.0681		0.75891		1.0013		0.78758
740	1.0000e-08		0.5991		0.84803		2.2300		0.73645		0.9743		0.78896
745	1.0000e-08		0.4961		0.85502		2.1599		0.74312		0.9730		0.78567
750	1.0000e-08		0.5648		0.84452		2.2519		0.73354		0.9934		0.78178
755	1.0000e-08		0.6283		0.84486		2.1092		0.74960		0.9343		0.79658
760	1.0000e-08		0.4694		0.85004		2.1040		0.75107		0.9509		0.79485
765	1.0000e-08		0.6069		0.85439		2.2790		0.73347		0.9570		0.79035
770	1.0000e-08		0.5274		0.84101		2.1864		0.73658		0.9866		0.78473
775	1.0000e-08		0.5468		0.85910		2.0698		0.75269		0.9592		0.79790
780	1.0000e-08		0.5890		0.84294		2.0742		0.75279		1.0034		0.79343
785	1.0000e-08		0.5498		0.85219		2.0922		0.75547		0.9569		0.79328
790	1.0000e-08		0.5424		0.84120		2.0597		0.75920		1.0358		0.78491
795	1.0000e-08		0.5675		0.85755		2.2136		0.73852		0.9915		0.78611
800	1.0000e-08		0.6552		0.84892		2.2166		0.74039		0.9366		0.79620
805	1.0000e-08		0.4683		0.85958		2.1691		0.74636		0.9434		0.80024
810	1.0000e-08		0.5433		0.86317		2.3448		0.73654		0.9943		0.78711
815	1.0000e-08		0.4858		0.85665		2.1708		0.73929		0.9410		0.79571
820	1.0000e-08		0.5948		0.84908		2.1490		0.74681		0.9699		0.79597
825	1.0000e-08		0.5171		0.85411		2.0578		0.74972		0.9658		0.78909
830	1.0000e-08		0.4709		0.83363		2.3003		0.73309		0.9785		0.78700
835	1.0000e-08		0.6327		0.84020		2.1540		0.75425		1.0362		0.79533
840	1.0000e-08		0.4773		0.87178		2.1744		0.75673		1.0988		0.78924
845	1.0000e-08		0.5762		0.84720		2.0407		0.75309		0.9558		0.79248
850	1.0000e-08		0.4695		0.85773		2.0952		0.75714		1.0189		0.79309
855	1.0000e-08		0.5660		0.84792		2.1273		0.75390		1.0507		0.79773
860	1.0000e-08		0.6262		0.84863		2.0400		0.75398		1.0057		0.79390
865	1.0000e-08		0.5583		0.85158		2.0320		0.75392		0.9502		0.80061
870	1.0000e-08		0.4821		0.84077		2.1090		0.74306		0.9612		0.78929
875	1.0000e-08		0.5634		0.85543		2.1437		0.74920		0.9869		0.78923
880	1.0000e-08		0.4955		0.85015		2.0081		0.75469		0.9552		0.79828
885	1.0000e-08		0.4862		0.83287		2.2103		0.73703		0.9408		0.79420
890	1.0000e-08		0.5289		0.85438		2.1190		0.74900		0.9480		0.79733
895	1.0000e-08		0.5820		0.84157		2.1468		0.74095		0.9509		0.79326
900	1.0000e-08		0.5774		0.85440		2.2060		0.75101		1.0666		0.79628
905	1.0000e-08		0.5868		0.84353		2.2717		0.73440		0.9832		0.78694
910	1.0000e-08		0.5408		0.85930		1.9523		0.76498		0.9838		0.79029
915	1.0000e-08		0.5488		0.84808		2.3570		0.72599		1.0467		0.77330
920	1.0000e-08		0.5545		0.86351		2.2338		0.74040		0.9656		0.79353
925	1.0000e-08		0.5189		0.85330		2.1485		0.74483		0.9679		0.79074
930	1.0000e-08		0.4569		0.84864		2.1750		0.73989		0.9480		0.79299
935	1.0000e-08		0.5075		0.85799		2.0764		0.75014		1.0484		0.77840
940	1.0000e-08		0.5058		0.84447		2.0966		0.74737		0.9549		0.80154
945	1.0000e-08		0.5202		0.86358		2.0544		0.75826		1.0200		0.79482
950	1.0000e-08		0.6830		0.83146		2.2576		0.73858		0.9504		0.79532
955	1.0000e-08		0.5134		0.85429		2.1721		0.74773		0.9925		0.79400
960	1.0000e-08		0.5296		0.85703		2.0331		0.75653		0.9729		0.79704
965	1.0000e-08		0.4535		0.84705		2.4129		0.72659		1.0367		0.77727
970	1.0000e-08		0.5384		0.85580		2.0126		0.75867		0.9931		0.78982
975	1.0000e-08		0.5973		0.84509		2.2221		0.73466		1.0017		0.78304
980	1.0000e-08		0.6465		0.83558		2.1967		0.73817		0.9994		0.78382
985	1.0000e-08		0.5126		0.86325		2.0723		0.75983		1.0624		0.79533
990	1.0000e-08		0.6556		0.84741		2.0368		0.76055		1.0241		0.78215
995	1.0000e-08		0.5050		0.86092		2.0615		0.75424		0.9762		0.79281
1000	1.0000e-08		0.6027		0.84319		2.4488		0.72180		1.1008		0.76609
1005	1.0000e-08		0.6120		0.83973		2.1882		0.74327		0.9530		0.80235
1010	1.0000e-08		0.5791		0.85096		2.0594		0.74883		0.9661		0.79141
1015	1.0000e-08		0.6217		0.84690		2.1050		0.74678		0.9731		0.78696
1020	1.0000e-08		0.5030		0.83851		2.1104		0.74396		0.9525		0.79137
1025	1.0000e-08		0.5490		0.84029		2.1575		0.74233		0.9653		0.78985
1030	1.0000e-08		0.5568		0.84966		2.0956		0.75193		0.9805		0.79325
1035	1.0000e-08		0.4238		0.86416		2.1961		0.74171		0.9858		0.78415
1040	1.0000e-08		0.4961		0.84254		2.1563		0.74997		0.9855		0.79961
1045	1.0000e-08		0.5335		0.84412		2.2485		0.73915		0.9300		0.80182
1050	1.0000e-08		0.5046		0.85651		2.0501		0.75794		1.0083		0.78881
1055	1.0000e-08		0.5181		0.84733		2.1501		0.74132		0.9900		0.78808
1060	1.0000e-08		0.5933		0.83858		2.2122		0.74542		0.9611		0.79550
1065	1.0000e-08		0.4805		0.85501		2.1819		0.74250		0.9259		0.79682
1070	1.0000e-08		0.5568		0.85245		2.1453		0.75202		0.9940		0.80101
1075	1.0000e-08		0.4929		0.86479		2.0905		0.74765		0.9637		0.78978
1080	1.0000e-08		0.6127		0.85183		2.0431		0.75498		0.9659		0.79486
1085	1.0000e-08		0.4535		0.85909		2.1786		0.74329		0.9392		0.80293
1090	1.0000e-08		0.4663		0.86305		2.1125		0.75043		0.9555		0.79054
1095	1.0000e-08		0.6134		0.84830		2.2114		0.74417		0.9574		0.79098
1100	1.0000e-08		0.5940		0.84765		2.0437		0.74681		0.9864		0.78448
1105	1.0000e-08		0.4375		0.87006		2.3286		0.73881		0.9956		0.79896
1110	1.0000e-08		0.5585		0.84626		2.2128		0.73997		1.0133		0.78205
1115	1.0000e-08		0.5070		0.84704		2.2577		0.73436		0.9565		0.78905
1120	1.0000e-08		0.5715		0.85070		2.0957		0.74978		0.9659		0.79849
