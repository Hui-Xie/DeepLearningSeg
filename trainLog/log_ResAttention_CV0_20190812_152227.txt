=============training from sratch============
Program ID: 5042

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt', '/home/hxie1/data/OvarianCancerCT/pixelSize223/numpy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '0', '0']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
            8    at July 30th 03:00, 2019:
                    add learning rate print;
                    use convStride =2;
                    add filter number by 2 times along deeper layers.
            9    at July 30th, 10:13, 2019:
                    add MaxPool2d in stage1;
                    add final filters to 2048.
            10   at July 30th, 15:23, 2019
                    final conv layer filter number: 1024
            11   at Aug 10th, 2019:
                    A. Add new patient data; and exclude non-standard patient data;
                    B. test the k-th fold,  validation on the (k+1)th fold;
                    C. new inputsize: 231*251*251 with pixels size 3*2*2 mm
                    D. window level shresthold [0,300]
                    E. put data padding in to converting from nrrd to numpy;
                    F. Add STN network as first laye of network
                    G. change input data into gaussian normalization in slice by slice; (x-mean)/std;
            12   Aug 12th, 2019
                    A. fix the bug that SITk reads int imape, and normalization into interger error;
                    B  test without SPN.           
                    
                    
                                                        
                    
            
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   total 220 patients, 5-fold cross validation, test 45, validation 45, and training 130.  

Training strategy: 

          

Program starting Time: 2019-08-12 15:22:27.073902
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt/20190812_152227

Info: this is the 0th fold leave for test in the 5-fold cross-validation.

Infor: In all data of 220 files, label 0 has 77 files,
	  and label 1 has 143 files, where positive response rate = 0.65 in full data
5-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.
training dataset:	 total 130 files, where 1 has 85.0 with rate of 0.6538461538461539
validation dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
test dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
=== Network trains from scratch ====
Network has total 77,852,006 parameters.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	LearningRate		TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	1.0000e-05		0.5030	0.5615	0.6941	0.3111		0.4676	0.3556	0.0000	1.0000		0.4676	0.3556	0.0000	1.0000
5	1.0000e-05		0.5171	0.5154	0.6824	0.2000		0.5116	0.4222	0.1724	0.8750		0.5269	0.3778	0.1379	0.8125
10	9.5000e-06		0.4876	0.4769	0.4000	0.6222		0.5078	0.3556	0.0690	0.8750		0.5870	0.3556	0.1379	0.7500
15	9.5000e-06		0.4881	0.6077	0.7294	0.3778		0.4756	0.6000	0.6207	0.5625		0.5697	0.3778	0.4138	0.3125
20	9.0250e-06		0.5082	0.4615	0.4000	0.5778		0.4592	0.6000	0.8966	0.0625		0.4921	0.6667	1.0000	0.0625
25	9.0250e-06		0.4872	0.5154	0.5529	0.4444		0.4838	0.5778	0.7931	0.1875		0.4984	0.5778	0.8621	0.0625
30	8.5737e-06		0.5181	0.4385	0.4000	0.5111		0.4918	0.5111	0.5862	0.3750		0.4712	0.6222	0.6897	0.5000
35	8.5737e-06		0.4713	0.5769	0.5882	0.5556		0.4715	0.4222	0.1724	0.8750		0.5233	0.4667	0.2759	0.8125
40	8.1451e-06		0.4792	0.6000	0.7294	0.3556		0.4718	0.5556	0.4828	0.6875		0.5036	0.4889	0.5172	0.4375
45	8.1451e-06		0.5065	0.4077	0.3529	0.5111		0.4758	0.4222	0.2759	0.6875		0.5535	0.5333	0.4483	0.6875
50	7.7378e-06		0.4932	0.4538	0.3412	0.6667		0.4699	0.4889	0.3793	0.6875		0.5467	0.6222	0.5862	0.6875
55	7.7378e-06		0.5022	0.5538	0.6941	0.2889		0.5101	0.4444	0.5172	0.3125		0.6060	0.5333	0.5862	0.4375
60	7.3509e-06		0.4718	0.6308	0.8235	0.2667		0.4285	0.5778	0.6207	0.5000		0.5439	0.6000	0.6897	0.4375
65	7.3509e-06		0.5057	0.5923	0.8235	0.1556		0.4814	0.4889	0.3448	0.7500		0.5847	0.4667	0.5517	0.3125
70	6.9834e-06		0.4753	0.5846	0.6706	0.4222		0.4422	0.5556	0.5517	0.5625		0.5915	0.6222	0.7241	0.4375
75	6.9834e-06		0.4792	0.5692	0.6471	0.4222		0.4942	0.5556	0.3448	0.9375		0.6556	0.4667	0.4483	0.5000
80	6.6342e-06		0.4906	0.6154	0.7882	0.2889		0.4719	0.5556	0.6207	0.4375		0.6028	0.5333	0.6897	0.2500
85	6.6342e-06		0.4968	0.4846	0.5176	0.4222		0.4644	0.5333	0.6207	0.3750		0.5952	0.5556	0.7586	0.1875
90	6.3025e-06		0.5024	0.4538	0.4471	0.4667		0.4439	0.5778	0.5172	0.6875		0.5608	0.4889	0.5517	0.3750
95	6.3025e-06		0.4664	0.6077	0.6824	0.4667		0.4888	0.5556	0.6552	0.3750		0.5610	0.6000	0.7586	0.3125
100	5.9874e-06		0.4700	0.5308	0.5294	0.5333		0.4979	0.5111	0.3103	0.8750		0.7341	0.4222	0.2759	0.6875
105	5.9874e-06		0.4570	0.6077	0.5294	0.7556		0.4931	0.5556	0.6207	0.4375		0.5308	0.6667	0.8276	0.3750
110	5.6880e-06		0.4931	0.4923	0.4118	0.6444		0.4841	0.5333	0.6552	0.3125		0.5447	0.6222	0.7241	0.4375
115	5.6880e-06		0.4999	0.5462	0.7529	0.1556		0.4794	0.5333	0.6552	0.3125		0.5318	0.6667	0.8966	0.2500
120	5.4036e-06		0.4863	0.5692	0.6706	0.3778		0.4870	0.5333	0.5517	0.5000		0.5655	0.5111	0.6207	0.3125
125	5.4036e-06		0.4873	0.4846	0.3647	0.7111		0.4854	0.4444	0.3448	0.6250		0.6147	0.4667	0.3103	0.7500
130	5.1334e-06		0.4887	0.5308	0.5882	0.4222		0.4621	0.4667	0.3793	0.6250		0.5794	0.4667	0.5517	0.3125
135	5.1334e-06		0.4578	0.6154	0.6118	0.6222		0.4950	0.5333	0.5172	0.5625		0.5778	0.4889	0.5862	0.3125
140	4.8767e-06		0.4934	0.5077	0.5294	0.4667		0.4998	0.5111	0.4483	0.6250		0.6138	0.5111	0.5517	0.4375
145	4.8767e-06		0.4942	0.4846	0.4941	0.4667		0.4702	0.4444	0.2414	0.8125		0.6781	0.4222	0.3448	0.5625
150	4.6329e-06		0.4740	0.5846	0.6706	0.4222		0.4545	0.6000	0.7241	0.3750		0.6320	0.6000	0.7931	0.2500
155	4.6329e-06		0.4765	0.5462	0.5765	0.4889		0.4833	0.5778	0.5172	0.6875		0.6734	0.5111	0.5517	0.4375
160	4.4013e-06		0.4759	0.5923	0.6235	0.5333		0.4857	0.5556	0.4828	0.6875		0.6475	0.4000	0.3448	0.5000
165	4.4013e-06		0.4918	0.5077	0.5529	0.4222		0.4493	0.6444	0.6552	0.6250		0.6406	0.4667	0.5172	0.3750
170	4.1812e-06		0.4848	0.5769	0.6353	0.4667		0.4517	0.5333	0.6207	0.3750		0.6196	0.4889	0.6207	0.2500
175	4.1812e-06		0.4678	0.5154	0.4941	0.5556		0.4560	0.6000	0.6897	0.4375		0.6545	0.5111	0.5862	0.3750
180	3.9721e-06		0.4807	0.5923	0.6824	0.4222		0.4731	0.5556	0.6897	0.3125		0.6409	0.5556	0.6552	0.3750
185	3.9721e-06		0.4574	0.6692	0.7765	0.4667		0.4635	0.5333	0.6552	0.3125		0.6196	0.6000	0.7586	0.3125
190	3.7735e-06		0.4762	0.5769	0.6941	0.3556		0.4569	0.5333	0.5517	0.5000		0.6608	0.5111	0.6207	0.3125
195	3.7735e-06		0.4590	0.6538	0.7294	0.5111		0.4565	0.5333	0.4828	0.6250		0.6736	0.4889	0.5172	0.4375
200	3.5849e-06		0.4765	0.5769	0.6353	0.4667		0.4889	0.5333	0.5517	0.5000		0.6493	0.5111	0.6207	0.3125
205	3.5849e-06		0.4939	0.5692	0.5882	0.5333		0.4643	0.5556	0.5862	0.5000		0.6050	0.5556	0.6552	0.3750
210	3.4056e-06		0.4796	0.6077	0.7294	0.3778		0.4647	0.5333	0.5517	0.5000		0.6053	0.5111	0.6552	0.2500
215	3.4056e-06		0.4695	0.5846	0.6706	0.4222		0.4688	0.5111	0.5172	0.5000		0.5905	0.5111	0.6552	0.2500
220	3.2353e-06		0.4624	0.5769	0.5412	0.6444		0.4667	0.4667	0.3793	0.6250		0.6963	0.5111	0.4828	0.5625
225	3.2353e-06		0.4615	0.6000	0.6588	0.4889		0.4768	0.6000	0.5517	0.6875		0.7151	0.4444	0.4828	0.3750
230	3.0736e-06		0.4580	0.6615	0.7647	0.4667		0.4935	0.5333	0.5862	0.4375		0.6156	0.5333	0.6552	0.3125
235	3.0736e-06		0.4837	0.5769	0.5765	0.5778		0.4904	0.4667	0.3448	0.6875		0.6250	0.4667	0.4483	0.5000
240	2.9199e-06		0.4735	0.5615	0.6235	0.4444		0.4432	0.6444	0.7586	0.4375		0.6009	0.5778	0.7586	0.2500
245	2.9199e-06		0.4913	0.5154	0.5765	0.4000		0.4738	0.5111	0.5862	0.3750		0.6083	0.5778	0.7586	0.2500
250	2.7739e-06		0.4897	0.5154	0.6000	0.3556		0.4768	0.6222	0.6552	0.5625		0.6476	0.5778	0.7586	0.2500
255	2.7739e-06		0.4613	0.6231	0.6706	0.5333		0.4430	0.5333	0.5862	0.4375		0.6448	0.5778	0.6897	0.3750
260	2.6352e-06		0.4686	0.6231	0.7294	0.4222		0.4510	0.5111	0.5517	0.4375		0.6973	0.5556	0.6552	0.3750
265	2.6352e-06		0.4682	0.6154	0.6824	0.4889		0.4456	0.5111	0.5517	0.4375		0.5921	0.4889	0.6207	0.2500
270	2.5034e-06		0.4637	0.6154	0.7059	0.4444		0.4348	0.6889	0.7931	0.5000		0.6084	0.6222	0.8621	0.1875
275	2.5034e-06		0.4908	0.5000	0.5176	0.4667		0.4800	0.5333	0.4828	0.6250		0.6650	0.4889	0.5517	0.3750
280	2.3783e-06		0.4694	0.5846	0.6353	0.4889		0.4383	0.6000	0.6207	0.5625		0.6039	0.5778	0.6897	0.3750
285	2.3783e-06		0.4667	0.5846	0.6353	0.4889		0.4508	0.5333	0.5862	0.4375		0.6375	0.5111	0.6552	0.2500
290	2.2594e-06		0.4671	0.5462	0.6000	0.4444		0.4656	0.5111	0.5172	0.5000		0.5905	0.4667	0.5172	0.3750
295	2.2594e-06		0.4654	0.5769	0.6353	0.4667		0.4617	0.4444	0.4483	0.4375		0.6070	0.5111	0.5862	0.3750
300	2.1464e-06		0.4821	0.6308	0.7412	0.4222		0.4707	0.5333	0.5517	0.5000		0.5750	0.4889	0.6207	0.2500
305	2.1464e-06		0.4647	0.6462	0.7059	0.5333		0.4408	0.5111	0.5862	0.3750		0.6566	0.4667	0.5517	0.3125
310	2.0391e-06		0.4545	0.6154	0.5647	0.7111		0.4689	0.5556	0.5862	0.5000		0.5661	0.4444	0.4483	0.4375
315	2.0391e-06		0.4719	0.5923	0.7176	0.3556		0.4456	0.5778	0.6552	0.4375		0.6250	0.5333	0.6207	0.3750
320	1.9371e-06		0.4728	0.6308	0.6941	0.5111		0.4600	0.5333	0.5862	0.4375		0.6166	0.4444	0.4483	0.4375
325	1.9371e-06		0.4873	0.5615	0.6824	0.3333		0.4590	0.5778	0.6207	0.5000		0.6249	0.5556	0.5517	0.5625
330	1.8403e-06		0.4487	0.6154	0.6588	0.5333		0.4560	0.5778	0.6552	0.4375		0.6864	0.4667	0.4828	0.4375
335	1.8403e-06		0.4660	0.6077	0.6588	0.5111		0.4585	0.5111	0.6207	0.3125		0.6386	0.4889	0.5517	0.3750
340	1.7482e-06		0.4650	0.6154	0.6471	0.5556		0.4416	0.5333	0.6207	0.3750		0.6719	0.5556	0.6552	0.3750
345	1.7482e-06		0.4608	0.6308	0.7294	0.4444		0.4672	0.4889	0.5517	0.3750		0.6041	0.4889	0.5862	0.3125
350	1.6608e-06		0.4591	0.6385	0.6824	0.5556		0.4667	0.5333	0.6207	0.3750		0.6648	0.5111	0.6552	0.2500
355	1.6608e-06		0.4488	0.6923	0.8000	0.4889		0.4556	0.5333	0.6897	0.2500		0.6297	0.5556	0.6897	0.3125
360	1.5778e-06		0.4673	0.6462	0.6941	0.5556		0.4551	0.5778	0.6207	0.5000		0.6266	0.5556	0.6552	0.3750
365	1.5778e-06		0.4614	0.6154	0.7059	0.4444		0.4403	0.5778	0.6552	0.4375		0.6350	0.5778	0.7241	0.3125
370	1.4989e-06		0.4771	0.6077	0.6941	0.4444		0.4509	0.5333	0.5862	0.4375		0.6312	0.5111	0.6552	0.2500
