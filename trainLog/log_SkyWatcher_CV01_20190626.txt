Program ID of SkyWatcher Network training:21371

Program command: 
 ['TrainSkyWatcher.py', '/home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/images_npy', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/labels_npy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json']

Major program changes: 
                      along deeper layer, increase filter number.
                      10 fold cross validation, 0 fold for test.
                      data partition with patient ID, instead of VOI.
                      in image3dResponseDataMgr, random Crop ROI in the fly.
                      erase normalization layers  in the fully connected layers.
                      Crop ROI around the mass center in each labeled slice. 
                      use bootStrapping in both trainSet and testSet
                      

Experiment setting for Image3d ROI to response:
Input CT data: 29*140*140  3D CT raw image ROI with spacing size(5*2*2)
segmentation label: 23*127*127 with spacing size(5*2*2) segmentation label with value (0,1,2) which erases lymph node label

This is a multi-task learning. 

Predictive Model: 1,  first 3-layer dense conv block with channel size 128.
                  2,  and 3 dense conv DownBB blocks,  each of which includes a stride 2 conv and 3-layers dense conv block; 
                  3,  and 3 fully connected layers  changes the tensor into size 2*1;
                  4,  final a softmax for binary classification;
                  Total network learning parameters are 8 million.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/SkyWatcherModel.py

response Loss Function:   focus loss  with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.
segmentation loss function: focus loss  with weight [1.0416883685076772, 39.37007874015748, 68.39945280437757] for label (0, 1, 2)

Data:   training data has 113 patients, and valdiation data has 27 patients with training/test rate 80/20.
        We randomize all data, and then assign same distrubtion of treat reponse 0,1 into to training and test data set.
        

Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.  

                    Learning Scheduler:  Reduce learning rate on  plateau, and learning rate patience is 30 epochs.                                

            

Program starting Time: 2019-06-26 16:25:17.942479
Info: this is the 0th fold leave for test in the 10-fold cross-validation.

Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher

Now program get 169 input files.
Infor: In all data of 169 files, respone 0 has 51 files,
	  and response 1 has 118 files, where positive response rate = 0.6982248520710059 in full data
Infor: Validation Set has 16 files,and Training Set has 153 files
Infor: Validataion set has 11 1's, and positive response rate = 0.6875
Infor: trainning set has 107 1's, and positive response rate = 0.6993464052287581
Infor: the drop_last data in the dataMgr may lead the number of validation set and training set less than above number.
Infor: program is in multi samples running model.
TrainTestData Input:  batchSize=6, depth=29, height=140, width=140

Info: the size of bottle neck in the net = (512, 1, 7, 7)

Info: program uses Mixup with alpha=0.4, and mixupProb = 0.5.
Network trains from scratch.
Network has total 21,463,612 parameters.
Infor: Segmentation Cross Entropy Weight: [1.0416883685076772, 39.37007874015748, 68.39945280437757] for label (0, 1, 2)
Info: program will use 3 GPUs.
Loss Functions List: FocalCELoss with weight of 1; 	FocalCELoss with weight of 1; 
Hints: Test Dice_0 is the dice coeff for all non-zero labels
Hints: Test Dice_1 is for primary cancer(green), 	
 test Dice_2 is for metastasis(yellow), 	
 and test Dice_3 is for invaded lymph node(brown).
Hints: Test TPR_0 is the TPR for all non-zero labels
Hints: Test TPR_1 is for primary cancer(green), 	
 TPR_2 is for metastasis(yellow), 	
 and TPR_3 is for invaded lymph node(brown).

Dice is based on all 2D segmented slices in the volume from weak annotation, not real 3D dice.

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	TrLoss	Dice0	Dice1	Dice2	TPR_0	TPR_1	TPR_2	Accura	TPR_r	TNR_r		TsLoss	Dice0	Dice1	Dice2	TPR_0	TPR_1	TPR_2	Accura	TPR_r	TNR_r
0	0.6944	0.074	0.033	0.046	0.834	0.083	0.822	0.4766	0.4299	0.5234		0.6801	0.118	0.099	0.075	0.672	0.298	0.560	0.7273	0.6364	0.8182
5	0.5084	0.180	0.267	0.108	0.900	0.482	0.783	0.4720	0.3738	0.5701		0.5386	0.217	0.452	0.064	0.952	0.707	0.595	0.5000	0.8182	0.1818
10	0.4834	0.170	0.254	0.118	0.903	0.589	0.782	0.4860	0.4299	0.5421		0.5369	0.296	0.276	0.196	0.915	0.270	0.803	0.4091	0.2727	0.5455
15	0.4587	0.186	0.336	0.114	0.909	0.690	0.719	0.5748	0.5607	0.5888		0.5017	0.217	0.483	0.111	0.991	0.921	0.684	0.3636	0.3636	0.3636
20	0.4436	0.186	0.323	0.125	0.913	0.743	0.765	0.6168	0.7196	0.5140		0.5331	0.274	0.388	0.163	0.960	0.563	0.812	0.3636	0.5455	0.1818
25	0.4306	0.188	0.321	0.125	0.888	0.764	0.730	0.5981	0.5327	0.6636		0.4759	0.287	0.487	0.205	0.872	0.639	0.731	0.3636	0.7273	0.0000
30	0.4496	0.196	0.277	0.132	0.885	0.623	0.720	0.6262	0.6168	0.6355		0.4472	0.263	0.401	0.152	0.985	0.539	0.931	0.4091	0.5455	0.2727
35	0.4334	0.197	0.317	0.137	0.901	0.676	0.800	0.6355	0.6636	0.6075		0.3909	0.222	0.472	0.169	0.973	0.720	0.911	0.6364	0.4545	0.8182
40	0.3838	0.210	0.345	0.151	0.932	0.790	0.818	0.5981	0.5794	0.6168		0.4452	0.294	0.416	0.196	0.933	0.700	0.898	0.4091	0.4545	0.3636
45	0.3822	0.212	0.309	0.157	0.895	0.749	0.772	0.6121	0.7009	0.5234		0.3732	0.279	0.451	0.195	0.925	0.754	0.824	0.5455	0.6364	0.4545
50	0.3746	0.215	0.347	0.154	0.893	0.756	0.782	0.7009	0.6822	0.7196		0.4636	0.296	0.480	0.218	0.968	0.766	0.814	0.2727	0.4545	0.0909
55	0.3575	0.229	0.370	0.172	0.893	0.830	0.768	0.6776	0.6542	0.7009		0.4573	0.349	0.443	0.251	0.882	0.621	0.803	0.4091	0.5455	0.2727
60	0.3483	0.224	0.373	0.154	0.913	0.834	0.781	0.6729	0.6729	0.6729		0.5031	0.292	0.294	0.205	0.961	0.647	0.849	0.2273	0.4545	0.0000
65	0.3605	0.216	0.350	0.164	0.902	0.888	0.789	0.6636	0.6542	0.6729		0.4803	0.258	0.424	0.193	0.951	0.731	0.772	0.5000	0.4545	0.5455
70	0.3413	0.235	0.383	0.171	0.908	0.787	0.777	0.7523	0.7850	0.7196		0.4963	0.340	0.508	0.205	0.921	0.927	0.659	0.3636	0.6364	0.0909
75	0.3470	0.225	0.364	0.154	0.895	0.837	0.747	0.7336	0.7570	0.7103		0.4270	0.324	0.462	0.207	0.898	0.892	0.704	0.4545	0.7273	0.1818
80	0.3253	0.249	0.391	0.173	0.895	0.822	0.796	0.6682	0.6822	0.6542		0.3784	0.316	0.527	0.241	0.916	0.870	0.825	0.5000	0.7273	0.2727
85	0.3067	0.234	0.390	0.168	0.904	0.834	0.810	0.7150	0.7850	0.6449		0.3799	0.305	0.443	0.208	0.916	0.899	0.768	0.6364	0.9091	0.3636
90	0.3115	0.244	0.391	0.184	0.908	0.834	0.826	0.7243	0.7477	0.7009		0.4413	0.316	0.415	0.260	0.885	0.721	0.935	0.3636	0.5455	0.1818
95	0.3188	0.234	0.394	0.166	0.923	0.850	0.786	0.6963	0.6729	0.7196		0.4251	0.283	0.385	0.217	0.943	0.944	0.657	0.4091	0.6364	0.1818
100	0.3010	0.246	0.371	0.173	0.906	0.837	0.792	0.7617	0.7850	0.7383		0.3788	0.320	0.421	0.257	0.903	0.835	0.801	0.5455	0.5455	0.5455
105	0.2733	0.245	0.413	0.161	0.894	0.860	0.804	0.7570	0.8037	0.7103		0.4050	0.312	0.414	0.256	0.897	0.812	0.872	0.4545	0.6364	0.2727
110	0.3041	0.248	0.374	0.176	0.891	0.813	0.792	0.6916	0.6916	0.6916		0.4091	0.348	0.493	0.251	0.895	0.837	0.796	0.3636	0.6364	0.0909
115	0.2577	0.252	0.378	0.192	0.901	0.876	0.818	0.7430	0.7757	0.7103		0.4119	0.327	0.442	0.196	0.875	0.748	0.767	0.5455	0.7273	0.3636
120	0.2653	0.234	0.352	0.188	0.907	0.842	0.847	0.7804	0.7570	0.8037		0.4949	0.317	0.407	0.280	0.946	0.834	0.866	0.2727	0.5455	0.0000
125	0.2611	0.252	0.389	0.182	0.888	0.875	0.814	0.7430	0.7944	0.6916		0.5555	0.357	0.496	0.250	0.931	0.708	0.902	0.4545	0.6364	0.2727
130	0.2724	0.236	0.347	0.176	0.917	0.847	0.831	0.7477	0.7757	0.7196		0.4838	0.313	0.541	0.256	0.901	0.813	0.824	0.3182	0.2727	0.3636
135	0.2499	0.237	0.382	0.173	0.912	0.887	0.826	0.7617	0.8505	0.6729		0.4784	0.302	0.491	0.217	0.948	0.748	0.908	0.3636	0.5455	0.1818
140	0.2731	0.241	0.394	0.178	0.907	0.849	0.839	0.7523	0.7383	0.7664		0.3863	0.332	0.413	0.295	0.926	0.869	0.845	0.3636	0.5455	0.1818
145	0.2520	0.267	0.410	0.201	0.871	0.872	0.790	0.7757	0.8224	0.7290		0.5293	0.339	0.423	0.247	0.887	0.786	0.717	0.3636	0.3636	0.3636
150	0.2342	0.263	0.406	0.177	0.921	0.855	0.830	0.8224	0.8131	0.8318		0.4844	0.354	0.458	0.294	0.957	0.967	0.878	0.3182	0.3636	0.2727
155	0.2288	0.266	0.402	0.186	0.904	0.877	0.805	0.7710	0.7196	0.8224		0.5729	0.369	0.423	0.243	0.913	0.759	0.744	0.5455	1.0000	0.0909
160	0.1930	0.248	0.392	0.182	0.898	0.872	0.858	0.8879	0.8879	0.8879		0.5631	0.298	0.374	0.210	0.901	0.773	0.904	0.4545	0.4545	0.4545
165	0.1971	0.267	0.418	0.199	0.903	0.881	0.850	0.8738	0.9065	0.8411		0.8698	0.366	0.402	0.250	0.932	0.798	0.850	0.5909	1.0000	0.1818
170	0.1964	0.273	0.403	0.200	0.902	0.882	0.838	0.8832	0.8972	0.8692		0.4046	0.345	0.449	0.224	0.872	0.779	0.804	0.3636	0.5455	0.1818
175	0.1958	0.250	0.395	0.188	0.882	0.858	0.826	0.8785	0.8692	0.8879		0.6376	0.321	0.390	0.256	0.918	0.893	0.813	0.4545	0.7273	0.1818
180	0.2142	0.263	0.381	0.199	0.884	0.818	0.839	0.8598	0.8598	0.8598		0.5060	0.343	0.517	0.253	0.905	0.789	0.843	0.4545	0.5455	0.3636
185	0.1981	0.279	0.424	0.208	0.887	0.821	0.850	0.9019	0.9065	0.8972		0.7029	0.344	0.342	0.250	0.953	0.695	0.905	0.4091	0.5455	0.2727
190	0.2009	0.269	0.404	0.196	0.896	0.867	0.815	0.8411	0.8598	0.8224		0.4778	0.363	0.431	0.261	0.923	0.870	0.877	0.4091	0.4545	0.3636
195	0.2119	0.258	0.401	0.193	0.918	0.919	0.829	0.8318	0.8411	0.8224		0.5428	0.385	0.489	0.259	0.836	0.795	0.791	0.3182	0.4545	0.1818
200	0.1949	0.278	0.424	0.202	0.885	0.858	0.835	0.8598	0.8598	0.8598		0.5507	0.346	0.510	0.206	0.971	0.859	0.949	0.4091	0.5455	0.2727
205	0.1609	0.274	0.411	0.195	0.897	0.871	0.834	0.9252	0.8972	0.9533		0.6297	0.330	0.394	0.253	0.853	0.739	0.765	0.3182	0.3636	0.2727
210	0.1372	0.282	0.425	0.218	0.935	0.923	0.906	0.9439	0.9439	0.9439		0.5069	0.341	0.460	0.260	0.826	0.761	0.795	0.5000	0.6364	0.3636
215	0.1370	0.297	0.412	0.227	0.906	0.866	0.865	0.9393	0.9159	0.9626		0.5856	0.342	0.449	0.265	0.877	0.784	0.793	0.4545	0.7273	0.1818
220	0.1703	0.272	0.402	0.202	0.892	0.842	0.836	0.9206	0.9159	0.9252		0.5739	0.352	0.350	0.267	0.851	0.724	0.734	0.4545	0.4545	0.4545
225	0.1360	0.264	0.383	0.211	0.908	0.871	0.888	0.9346	0.9346	0.9346		0.4693	0.374	0.524	0.281	0.889	0.804	0.860	0.4091	0.4545	0.3636
230	0.1306	0.282	0.423	0.207	0.922	0.924	0.852	0.9579	0.9533	0.9626		0.4972	0.368	0.526	0.255	0.907	0.741	0.845	0.5455	0.8182	0.2727
235	0.1192	0.273	0.409	0.216	0.919	0.908	0.887	0.9626	0.9533	0.9720		0.6986	0.371	0.420	0.289	0.936	0.872	0.942	0.2727	0.5455	0.0000
240	0.1285	0.278	0.383	0.215	0.939	0.908	0.892	0.9346	0.9065	0.9626		0.4612	0.346	0.492	0.264	0.881	0.851	0.894	0.5000	0.6364	0.3636
245	0.1197	0.283	0.439	0.200	0.923	0.892	0.872	0.9579	0.9346	0.9813		0.8964	0.377	0.404	0.277	0.915	0.732	0.832	0.3182	0.6364	0.0000
250	0.1296	0.283	0.412	0.220	0.918	0.901	0.870	0.9533	0.9720	0.9346		0.6934	0.371	0.447	0.242	0.868	0.786	0.814	0.3636	0.5455	0.1818
255	0.1180	0.278	0.415	0.211	0.929	0.899	0.914	0.9579	0.9626	0.9533		0.5945	0.364	0.420	0.238	0.930	0.796	0.903	0.3182	0.4545	0.1818
260	0.1315	0.279	0.421	0.207	0.939	0.886	0.908	0.9393	0.9159	0.9626		1.2220	0.370	0.207	0.261	0.816	0.311	0.804	0.3636	0.5455	0.1818
265	0.1141	0.279	0.416	0.205	0.924	0.886	0.888	0.9486	0.9720	0.9252		0.4484	0.376	0.507	0.291	0.932	0.867	0.834	0.3636	0.5455	0.1818
270	0.1197	0.290	0.411	0.219	0.932	0.882	0.885	0.9860	0.9907	0.9813		0.5459	0.372	0.521	0.260	0.941	0.825	0.890	0.4545	0.6364	0.2727
275	0.0986	0.285	0.397	0.219	0.907	0.877	0.866	0.9860	0.9907	0.9813		0.5857	0.395	0.446	0.302	0.886	0.749	0.831	0.5455	0.5455	0.5455
280	0.1144	0.292	0.440	0.225	0.911	0.906	0.885	0.9486	0.9439	0.9533		0.6283	0.362	0.461	0.253	0.924	0.801	0.854	0.3182	0.6364	0.0000
285	0.0977	0.287	0.407	0.211	0.932	0.909	0.880	0.9907	1.0000	0.9813		0.9242	0.370	0.443	0.292	0.885	0.778	0.849	0.2727	0.5455	0.0000
290	0.1146	0.291	0.417	0.212	0.927	0.917	0.883	0.9299	0.9346	0.9252		0.5964	0.347	0.442	0.221	0.893	0.806	0.864	0.4545	0.5455	0.3636
295	0.1096	0.288	0.436	0.219	0.933	0.894	0.903	0.9626	0.9720	0.9533		0.6805	0.345	0.376	0.269	0.870	0.762	0.817	0.4545	0.5455	0.3636
300	0.0982	0.293	0.445	0.222	0.916	0.873	0.894	0.9533	0.9626	0.9439		0.8438	0.389	0.477	0.290	0.856	0.751	0.847	0.3636	0.4545	0.2727
305	0.1070	0.284	0.394	0.222	0.929	0.892	0.892	0.9533	0.9439	0.9626		0.6756	0.337	0.436	0.244	0.774	0.850	0.590	0.5455	0.7273	0.3636
310	0.0894	0.300	0.403	0.251	0.913	0.891	0.878	0.9860	0.9813	0.9907		0.7679	0.384	0.507	0.271	0.913	0.815	0.800	0.4545	0.6364	0.2727
315	0.0938	0.297	0.450	0.221	0.919	0.935	0.879	0.9626	0.9626	0.9626		0.7882	0.376	0.495	0.269	0.779	0.771	0.734	0.3636	0.5455	0.1818
320	0.0984	0.288	0.413	0.229	0.935	0.934	0.913	0.9720	0.9813	0.9626		0.7398	0.383	0.459	0.301	0.879	0.807	0.894	0.3636	0.5455	0.1818
325	0.1012	0.290	0.414	0.221	0.914	0.915	0.869	0.9860	1.0000	0.9720		0.8099	0.364	0.429	0.337	0.898	0.836	0.868	0.3182	0.5455	0.0909
330	0.0947	0.287	0.401	0.222	0.928	0.920	0.873	0.9860	0.9907	0.9813		0.8424	0.352	0.490	0.320	0.894	0.799	0.865	0.3182	0.5455	0.0909
