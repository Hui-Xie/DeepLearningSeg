=============training from sratch============
Program ID: 18621

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/training/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/training/labels_npy', '0', '3', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
      Oct 23th, 2019
      1   change to MaxPool with 2*2*2 with stride 2;
      2   Loss use GeneralizedDiceLoss function;
      
      Oct 25th, 2019
      1   add trainAllData switch
      
      Oct 29th, 2019
      1   revise reduce learnging rate at training loss;
      2   reduce min_lr = 1e-9
      
      Nov 1st, 2019
      1   erase batchNorm, for training.
      
      Nov 2nd, 2019
      1   Use InstanceNormalization, instead of BatchNorm
      
      Nov 25th, 2019
      1   Add boundary loss into training.  
      
      Dec 3rd, 2019
      1  cancel Boundary Loss;
      2  add 13 network generated files with accuracy >88% into 3D ground truth, then train;
      
      Dec 7th, 2019
      1  Change latent vector in the V model into 1*1 in feature map size.
      
      Dce 9th, 2019
      1  Instance Norm can not apply for one value per channel, which will make the value =0
      
      Dec 10th, 2019
      1   In the uppooling4 and 5, where inputsize=1, use Deconv instead upsample,
          in order to avoid repeat of interpolation.
      2   In the DownPoooling5, the final 3*3 conv becomes 3*3 maxpool, which stresses translation invariance.   
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-12-10 16:44:45.288168
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191210_164445

Info: this is the 0th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191210_164445.
All files are in one partition.

all dataset: total 35 image files.
Network has total 237,217,778 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		0.7288		0.26881
5	1.0000e-02		0.5219		0.43399
10	1.0000e-02		0.4784		0.48808
15	1.0000e-02		0.4966		0.45896
20	1.0000e-02		0.5107		0.44218
25	1.0000e-02		0.4488		0.51547
30	1.0000e-02		0.4499		0.51853
35	1.0000e-02		0.4169		0.54862
40	1.0000e-02		0.4526		0.51063
45	1.0000e-02		0.4089		0.55866
50	1.0000e-02		0.3673		0.61000
55	1.0000e-02		0.3986		0.56843
60	1.0000e-02		0.3759		0.60097
65	1.0000e-02		0.3590		0.61225
70	1.0000e-02		0.3771		0.60381
75	1.0000e-02		0.3670		0.60707
80	1.0000e-02		0.3534		0.62439
85	1.0000e-02		0.3225		0.65674
90	1.0000e-02		0.3241		0.64972
95	1.0000e-02		0.2893		0.68952
100	1.0000e-02		0.3054		0.67765
105	1.0000e-02		0.2890		0.69368
110	1.0000e-02		0.3048		0.67519
115	1.0000e-02		0.2891		0.69155
120	1.0000e-02		0.3228		0.65812
125	1.0000e-02		0.3101		0.67572
130	1.0000e-02		0.2702		0.71115
135	1.0000e-02		0.2860		0.70006
140	1.0000e-02		0.2955		0.68603
145	1.0000e-02		0.2610		0.72340
150	1.0000e-02		0.2731		0.71142
155	1.0000e-02		0.2518		0.73479
160	1.0000e-02		0.2724		0.71347
165	1.0000e-02		0.2451		0.74165
170	1.0000e-02		0.2763		0.70963
175	1.0000e-02		0.2344		0.75404
180	1.0000e-02		0.2483		0.73645
185	1.0000e-02		0.2335		0.75155
190	1.0000e-02		0.2487		0.73682
195	1.0000e-02		0.2244		0.75873
200	1.0000e-02		0.1939		0.79456
205	1.0000e-02		0.2331		0.75218
210	1.0000e-02		0.2003		0.78730
215	1.0000e-02		0.2298		0.75192
220	1.0000e-02		0.2275		0.75723
225	1.0000e-02		0.2202		0.76745
230	1.0000e-02		0.2218		0.76681
235	1.0000e-02		0.2472		0.73529
240	1.0000e-02		0.2034		0.78582
245	1.0000e-02		0.2131		0.77440
250	1.0000e-02		0.2744		0.69909
255	1.0000e-03		0.1945		0.79371
260	1.0000e-03		0.1585		0.82879
265	1.0000e-03		0.1873		0.80378
270	1.0000e-03		0.1852		0.80561
275	1.0000e-03		0.1859		0.80365
280	1.0000e-03		0.1408		0.85120
285	1.0000e-03		0.1819		0.80778
290	1.0000e-03		0.1600		0.82994
295	1.0000e-03		0.1833		0.80571
300	1.0000e-03		0.1610		0.83115
305	1.0000e-03		0.1524		0.83959
310	1.0000e-03		0.1589		0.83194
315	1.0000e-03		0.1697		0.82213
320	1.0000e-03		0.1521		0.83707
325	1.0000e-03		0.1476		0.84411
330	1.0000e-03		0.1563		0.83555
335	1.0000e-04		0.1537		0.83764
340	1.0000e-04		0.1484		0.84208
345	1.0000e-04		0.1418		0.84805
350	1.0000e-04		0.1303		0.86104
355	1.0000e-04		0.1531		0.83643
360	1.0000e-04		0.1399		0.85239
365	1.0000e-04		0.1269		0.86522
370	1.0000e-04		0.1365		0.85363
375	1.0000e-04		0.1244		0.86756
380	1.0000e-04		0.1409		0.85076
385	1.0000e-04		0.1537		0.83586
390	1.0000e-04		0.1404		0.85093
395	1.0000e-04		0.1500		0.83147
400	1.0000e-04		0.1379		0.85432
405	1.0000e-04		0.1642		0.82589
410	1.0000e-04		0.1609		0.83022
415	1.0000e-04		0.1292		0.86232
420	1.0000e-04		0.1305		0.86142
425	1.0000e-04		0.1371		0.85499
430	1.0000e-05		0.1355		0.85733
435	1.0000e-05		0.1563		0.83563
440	1.0000e-05		0.1556		0.83579
445	1.0000e-05		0.1296		0.86277
450	1.0000e-05		0.1411		0.85039
455	1.0000e-05		0.1417		0.84987
460	1.0000e-05		0.1504		0.83945
465	1.0000e-05		0.1439		0.84752
470	1.0000e-05		0.1287		0.86236
475	1.0000e-05		0.1657		0.82458
480	1.0000e-05		0.1324		0.86053
485	1.0000e-06		0.1316		0.86057
490	1.0000e-06		0.1375		0.85405
495	1.0000e-06		0.1219		0.87048
500	1.0000e-06		0.1399		0.85235
505	1.0000e-06		0.1375		0.85324
510	1.0000e-06		0.1500		0.84089
515	1.0000e-06		0.1436		0.84787
520	1.0000e-06		0.1328		0.85877
525	1.0000e-06		0.1325		0.85996
530	1.0000e-06		0.1291		0.86351
535	1.0000e-06		0.1353		0.85672
540	1.0000e-06		0.1306		0.86247
545	1.0000e-06		0.1405		0.85078
550	1.0000e-07		0.1404		0.85190
555	1.0000e-07		0.1377		0.85425
560	1.0000e-07		0.1436		0.84925
565	1.0000e-07		0.1389		0.85263
570	1.0000e-07		0.1411		0.85030
575	1.0000e-07		0.1417		0.85009
580	1.0000e-07		0.1378		0.85498
585	1.0000e-07		0.1420		0.84944
590	1.0000e-07		0.1548		0.83644
595	1.0000e-07		0.1369		0.85469
600	1.0000e-07		0.1392		0.85240
605	1.0000e-08		0.1449		0.84650
610	1.0000e-08		0.1533		0.83638
615	1.0000e-08		0.1344		0.85583
620	1.0000e-08		0.1342		0.85789
625	1.0000e-08		0.1503		0.84016
630	1.0000e-08		0.1308		0.86084
635	1.0000e-08		0.1565		0.83346
640	1.0000e-08		0.1188		0.87360
645	1.0000e-08		0.1362		0.85716
650	1.0000e-08		0.1387		0.85381
655	1.0000e-08		0.1524		0.83767
660	1.0000e-08		0.1405		0.85072
665	1.0000e-08		0.1294		0.86275
670	1.0000e-08		0.1425		0.84890
675	1.0000e-08		0.1342		0.85849
680	1.0000e-08		0.1299		0.85998
685	1.0000e-08		0.1459		0.84533
690	1.0000e-08		0.1398		0.85205
695	1.0000e-08		0.1514		0.83922
700	1.0000e-08		0.1430		0.84786
705	1.0000e-08		0.1281		0.86417
710	1.0000e-08		0.1521		0.83933
715	1.0000e-08		0.1436		0.84727
720	1.0000e-08		0.1390		0.85271
725	1.0000e-08		0.1367		0.85518
730	1.0000e-08		0.1397		0.85020
735	1.0000e-08		0.1374		0.85434
740	1.0000e-08		0.1279		0.86447
745	1.0000e-08		0.1341		0.85728
750	1.0000e-08		0.1267		0.86242
755	1.0000e-08		0.1456		0.84540
760	1.0000e-08		0.1349		0.85724
765	1.0000e-08		0.1580		0.83225
770	1.0000e-08		0.1425		0.84550
775	1.0000e-08		0.1362		0.85524
780	1.0000e-08		0.1439		0.84737
785	1.0000e-08		0.1347		0.85670
790	1.0000e-08		0.1354		0.85569
795	1.0000e-08		0.1334		0.85943
800	1.0000e-08		0.1218		0.87019
805	1.0000e-08		0.1429		0.84967
810	1.0000e-08		0.1455		0.84601
815	1.0000e-08		0.1479		0.84408
820	1.0000e-08		0.1382		0.85242
825	1.0000e-08		0.1236		0.86852
830	1.0000e-08		0.1267		0.86595
835	1.0000e-08		0.1406		0.85112
840	1.0000e-08		0.1475		0.84211
845	1.0000e-08		0.1403		0.85040
850	1.0000e-08		0.1247		0.86786
855	1.0000e-08		0.1426		0.84914
860	1.0000e-08		0.1580		0.83334
865	1.0000e-08		0.1327		0.86013
870	1.0000e-08		0.1399		0.84846
875	1.0000e-08		0.1239		0.86916
880	1.0000e-08		0.1245		0.86840
885	1.0000e-08		0.1519		0.83958
890	1.0000e-08		0.1443		0.84809
895	1.0000e-08		0.1382		0.85352
900	1.0000e-08		0.1419		0.84860
905	1.0000e-08		0.1356		0.85530
910	1.0000e-08		0.1560		0.83302
915	1.0000e-08		0.1382		0.85267
920	1.0000e-08		0.1467		0.84486
925	1.0000e-08		0.1372		0.85448
930	1.0000e-08		0.1298		0.86201
935	1.0000e-08		0.1381		0.85364
940	1.0000e-08		0.1447		0.84762
945	1.0000e-08		0.1409		0.85056
950	1.0000e-08		0.1415		0.84991
955	1.0000e-08		0.1315		0.86146
960	1.0000e-08		0.1412		0.84687
965	1.0000e-08		0.1379		0.85447
970	1.0000e-08		0.1464		0.84546
975	1.0000e-08		0.1223		0.86945
980	1.0000e-08		0.1384		0.85112
985	1.0000e-08		0.1397		0.85140
990	1.0000e-08		0.1358		0.85661
995	1.0000e-08		0.1306		0.86186
1000	1.0000e-08		0.1450		0.84441
1005	1.0000e-08		0.1415		0.85177
1010	1.0000e-08		0.1371		0.85376
1015	1.0000e-08		0.1385		0.85317
1020	1.0000e-08		0.1358		0.85611
