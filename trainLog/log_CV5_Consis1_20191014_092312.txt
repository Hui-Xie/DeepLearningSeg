=============training from sratch============
Program ID: 512

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy', '5', '1', '1']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-14 09:23:12.979460
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191014_092312

Info: this is the 5th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = True and searchWindowSize= 7

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191014_092312.
6-fold cross validation: the 5th fold is for test, the 0th fold is for validation, remaining folds are for training.

training dataset: total 20 image files.

validation dataset: total 5 image files.

test dataset: total 4 image files.
Total 20 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy
0 has 19594676 elements, with a rate of  0.925288877177971 
1 has 1582144 elements, with a rate of  0.07471112282202899 
loss weight = tensor([ 1.0000, 12.3849])
Network has total 113,191,074 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		6.8729		0.06102		18.8331		0.12870		13.5605		0.12429
5	1.0000e-02		3.3252		0.31130		12.6217		0.21144		9.0650		0.18706
10	1.0000e-02		2.5056		0.33055		3.9614		0.22851		4.5482		0.17863
15	1.0000e-02		3.8911		0.25033		3.2266		0.32088		3.0368		0.23955
20	1.0000e-02		2.2262		0.35715		2.7551		0.34243		2.6779		0.33937
25	1.0000e-02		2.3955		0.28068		2.3909		0.40245		2.9741		0.31065
30	1.0000e-02		2.7411		0.33728		3.0335		0.36125		3.4273		0.23216
35	1.0000e-02		1.7457		0.43798		1.8035		0.51508		1.6784		0.42042
40	1.0000e-02		1.8977		0.38813		2.4055		0.38123		1.6392		0.35071
45	1.0000e-02		2.8336		0.36185		7.8469		0.25127		6.8608		0.20908
50	1.0000e-02		1.5688		0.40306		2.2176		0.36469		1.7154		0.43747
55	1.0000e-02		2.0808		0.40664		3.5599		0.31345		3.7163		0.30338
60	1.0000e-02		1.4055		0.41854		2.9385		0.32165		2.8171		0.25840
65	1.0000e-02		1.1954		0.46237		2.5259		0.35311		3.2920		0.25508
70	1.0000e-02		2.0961		0.38022		1.8126		0.41943		1.9093		0.36951
75	1.0000e-02		2.6606		0.38930		2.1368		0.37980		2.1440		0.28496
80	1.0000e-02		1.9634		0.41836		1.9054		0.37721		2.5499		0.31718
85	1.0000e-02		1.5896		0.38625		1.8878		0.37764		2.0785		0.32030
90	1.0000e-02		2.1049		0.34975		1.7780		0.41174		2.1930		0.30784
95	1.0000e-02		1.7938		0.39796		1.7004		0.38651		2.2645		0.31952
100	1.0000e-02		1.7328		0.43100		1.7845		0.37618		2.0663		0.30297
105	1.0000e-02		1.2288		0.45477		2.9793		0.36342		3.1320		0.27182
110	1.0000e-02		1.1187		0.49745		2.8229		0.31875		5.3358		0.12657
115	1.0000e-02		1.2124		0.46865		8.9600		0.24968		9.8751		0.23392
120	1.0000e-02		1.3372		0.43555		5.0562		0.26481		4.7839		0.23599
125	1.0000e-02		1.4098		0.44567		2.9012		0.36479		2.5104		0.29316
130	1.0000e-02		1.3000		0.45898		2.7342		0.32517		3.7979		0.24767
135	1.0000e-02		2.4262		0.41701		12.4295		0.18870		5.1580		0.18536
140	1.0000e-02		1.5077		0.45281		1.6198		0.46350		2.9493		0.28652
145	1.0000e-02		0.9632		0.48912		4.2035		0.36376		3.1251		0.31582
150	1.0000e-02		1.5227		0.41064		2.8487		0.26483		4.6787		0.18435
155	1.0000e-02		1.1771		0.48189		2.1971		0.30422		2.3592		0.27288
160	1.0000e-02		1.5288		0.46752		1.6926		0.42540		2.5327		0.26016
165	1.0000e-02		1.1784		0.46779		1.6759		0.35749		3.7327		0.24239
170	1.0000e-02		1.0793		0.47726		1.4025		0.39313		2.9153		0.29544
175	1.0000e-02		0.7310		0.56827		2.7307		0.32044		5.1413		0.28220
180	1.0000e-02		1.0730		0.47100		3.1380		0.30666		2.8483		0.29780
185	1.0000e-02		0.8205		0.53274		3.9753		0.29325		3.4045		0.32205
190	1.0000e-02		1.1384		0.42066		1.1202		0.48524		2.1959		0.30503
195	1.0000e-02		0.7791		0.54375		2.6516		0.35513		3.7294		0.31844
200	1.0000e-02		0.7182		0.57717		1.6676		0.38176		5.0148		0.29886
205	1.0000e-02		0.7240		0.54289		2.2629		0.40996		4.9031		0.23885
210	1.0000e-02		0.9453		0.56702		1.9614		0.36863		2.5763		0.31647
215	1.0000e-02		0.8393		0.53488		1.7839		0.37245		2.4638		0.32055
220	1.0000e-02		0.6396		0.58905		3.4630		0.29816		6.9951		0.24158
225	1.0000e-02		1.4506		0.39052		1.8915		0.39616		2.3444		0.31281
230	1.0000e-02		1.4858		0.43371		2.0721		0.33749		2.1900		0.28948
235	1.0000e-02		0.8065		0.52771		1.4532		0.46739		1.8210		0.32789
240	1.0000e-02		0.8986		0.54240		2.0431		0.38024		2.5526		0.30209
245	1.0000e-03		0.8051		0.56361		1.6058		0.40553		2.0390		0.34577
250	1.0000e-03		0.6556		0.52955		1.4289		0.40522		2.0215		0.31957
255	1.0000e-03		0.7150		0.55611		1.3359		0.38972		2.0681		0.32800
260	1.0000e-03		0.7097		0.56478		1.3663		0.39270		2.0662		0.33301
265	1.0000e-03		0.7105		0.56883		1.9376		0.35918		2.3691		0.35319
270	1.0000e-03		0.6638		0.56042		1.4753		0.40921		2.2466		0.36237
275	1.0000e-03		0.5427		0.57815		1.5316		0.39259		2.3069		0.34627
280	1.0000e-03		0.5863		0.61185		1.5520		0.38128		2.1588		0.33640
285	1.0000e-03		0.7270		0.58080		2.1967		0.35312		2.6725		0.34203
290	1.0000e-03		0.5561		0.62057		1.6429		0.39013		2.3981		0.35083
295	1.0000e-03		0.6462		0.63368		1.7048		0.37945		2.2266		0.32510
300	1.0000e-04		0.6880		0.60453		1.7871		0.38973		2.6411		0.34846
305	1.0000e-04		0.5071		0.60973		1.7925		0.40054		2.4567		0.34783
310	1.0000e-04		0.6588		0.62339		2.0378		0.36948		2.6448		0.33617
315	1.0000e-04		0.6211		0.62996		2.5999		0.33237		2.8851		0.32795
320	1.0000e-04		0.5303		0.64771		1.9306		0.37248		2.5779		0.33831
325	1.0000e-04		0.5287		0.63654		2.0880		0.37709		3.0452		0.34116
330	1.0000e-04		0.5562		0.62087		2.5683		0.32415		3.0512		0.33187
335	1.0000e-04		0.5292		0.63199		1.9073		0.38442		2.8508		0.33237
340	1.0000e-04		0.5857		0.61882		2.4119		0.33907		2.8323		0.32262
345	1.0000e-04		0.6701		0.59733		1.4627		0.42317		2.5229		0.33754
350	1.0000e-04		0.5625		0.66330		1.4863		0.41537		2.4312		0.34827
355	1.0000e-05		0.5256		0.62794		1.8776		0.38640		2.8689		0.34306
360	1.0000e-05		0.4934		0.60147		2.1253		0.35055		2.7491		0.33542
365	1.0000e-05		0.5470		0.61787		2.0407		0.36212		2.9061		0.33161
370	1.0000e-05		0.4858		0.60821		1.5414		0.39171		2.6648		0.34470
375	1.0000e-05		0.5198		0.63552		2.6355		0.33537		2.7006		0.33063
380	1.0000e-05		0.5136		0.62417		2.4113		0.34792		3.2497		0.33797
385	1.0000e-05		0.5218		0.62201		2.1761		0.36914		3.2359		0.34439
390	1.0000e-05		0.5123		0.63456		2.3322		0.36454		3.2429		0.34279
395	1.0000e-05		0.6149		0.62071		2.3316		0.33750		2.7537		0.33207
400	1.0000e-05		0.4779		0.64155		1.8047		0.38599		2.7239		0.33579
405	1.0000e-05		0.7659		0.66244		2.5293		0.32412		2.8468		0.32917
410	1.0000e-06		0.5804		0.57564		1.9943		0.38563		3.0344		0.33649
415	1.0000e-06		0.4679		0.66126		2.0721		0.35699		2.7696		0.33335
420	1.0000e-06		0.8074		0.60953		2.8798		0.33796		3.6347		0.34209
425	1.0000e-06		0.4651		0.65427		1.8573		0.38175		2.8581		0.33062
430	1.0000e-06		0.5724		0.61778		2.4947		0.34299		3.3623		0.33449
435	1.0000e-06		0.5340		0.65261		1.7664		0.38886		2.7242		0.33672
440	1.0000e-06		0.5391		0.60877		1.5963		0.41648		2.6176		0.33192
445	1.0000e-06		0.5551		0.62755		2.7746		0.32568		2.9593		0.31718
450	1.0000e-06		0.4861		0.63705		1.9968		0.37605		3.0471		0.32602
455	1.0000e-06		0.4725		0.60848		1.4930		0.40917		2.7379		0.33804
460	1.0000e-06		0.4948		0.63382		2.2913		0.36537		2.7602		0.33354
465	1.0000e-07		0.5609		0.62948		2.1879		0.35234		2.8552		0.34327
470	1.0000e-07		0.5624		0.63930		2.3690		0.33415		2.9093		0.33062
475	1.0000e-07		0.5026		0.64182		2.5094		0.33478		3.1830		0.32824
480	1.0000e-07		0.6126		0.61606		1.9654		0.38349		2.9241		0.34157
485	1.0000e-07		0.5264		0.61292		2.1905		0.35134		2.7530		0.32407
490	1.0000e-07		0.6745		0.58007		2.4148		0.34725		3.2649		0.33200
495	1.0000e-07		0.7232		0.64698		2.0579		0.35973		2.6043		0.35339
500	1.0000e-07		0.4422		0.62224		2.0665		0.37309		3.0525		0.34377
505	1.0000e-07		0.5225		0.65364		1.8789		0.38281		2.9545		0.32663
510	1.0000e-07		0.5481		0.56927		2.3705		0.32926		2.7986		0.33189
515	1.0000e-07		0.6411		0.60350		2.1587		0.36965		2.5879		0.33470
520	1.0000e-08		0.5800		0.60884		1.6844		0.38850		2.6242		0.33671
525	1.0000e-08		0.5362		0.62476		2.3203		0.34109		2.6370		0.33723
530	1.0000e-08		0.5296		0.61768		2.2251		0.35356		3.0279		0.33678
535	1.0000e-08		0.5010		0.61500		2.2932		0.37085		3.3130		0.34331
540	1.0000e-08		0.5255		0.64600		2.1039		0.35423		2.7427		0.33465
