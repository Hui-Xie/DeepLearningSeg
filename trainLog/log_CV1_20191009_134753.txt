=============training from sratch============
Program ID: 18170

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy', '1', '3,2,1']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 209
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
          
         

Discarded changes:                  
          

Program starting Time: 2019-10-09 13:47:53.944670
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191009_134753

Info: this is the 1th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 6

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191009_134753.
6-fold cross validation: the 1th fold is for test, the 2th fold is for validation, remaining folds are for training.

training dataset: total 19 image files.

validation dataset: total 5 image files.

test dataset: total 5 image files.
Total 19 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy
0 has 18657366 elements, with a rate of  0.9273976277637033 
1 has 1460613 elements, with a rate of  0.0726023722362967 
loss weight = tensor([ 1.0000, 12.7737])
Network has total 113,191,074 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		13.9869		0.07646		1609.7452		0.00000		489.8737		0.00000
5	1.0000e-02		8.4267		0.30245		63.4364		0.29284		5.7963		0.14784
10	1.0000e-02		6.8185		0.36113		38.9749		0.27644		6.9016		0.14854
15	1.0000e-02		5.8598		0.37606		10.8700		0.38299		5.2236		0.18613
20	1.0000e-02		5.2230		0.39698		7.7136		0.36774		5.4181		0.18711
25	1.0000e-02		4.3721		0.43856		7.1989		0.39896		4.9941		0.24021
30	1.0000e-02		5.9445		0.41009		7.4174		0.37495		4.1786		0.22182
35	1.0000e-02		4.4450		0.38210		10.3276		0.45140		2.8718		0.25298
40	1.0000e-02		4.1871		0.39874		6.5182		0.46433		3.2378		0.22243
45	1.0000e-02		3.4785		0.47632		6.7865		0.46617		2.9149		0.23597
50	1.0000e-02		4.1188		0.47971		8.5526		0.53704		3.8504		0.22811
55	1.0000e-02		3.7583		0.43782		5.7978		0.41125		3.6932		0.21577
60	1.0000e-02		6.2162		0.41540		5.8774		0.52416		2.4953		0.24404
65	1.0000e-02		7.9344		0.41605		18.3990		0.35003		4.7628		0.22411
70	1.0000e-02		4.8941		0.40506		7.2606		0.40615		4.8822		0.21908
75	1.0000e-02		4.3986		0.42495		9.7135		0.43720		5.3039		0.18389
80	1.0000e-02		3.1325		0.45314		4.9243		0.50216		2.8421		0.23841
85	1.0000e-02		3.6836		0.47167		9.8435		0.45985		3.1240		0.24393
90	1.0000e-02		3.8992		0.40565		6.7328		0.57124		4.1947		0.22525
95	1.0000e-02		2.8319		0.46903		8.5813		0.43417		3.4386		0.23382
100	1.0000e-02		4.5752		0.49437		5.9440		0.44432		3.6018		0.23614
105	1.0000e-02		2.8708		0.46626		5.1678		0.53634		3.0841		0.25686
110	1.0000e-02		2.7575		0.43637		6.2610		0.48727		2.7855		0.24852
115	1.0000e-02		2.8654		0.45142		5.6306		0.47914		3.0375		0.24578
120	1.0000e-02		2.6367		0.49839		6.7481		0.48933		2.9570		0.24242
125	1.0000e-02		3.3712		0.43006		9.9022		0.50426		3.9304		0.28029
130	1.0000e-02		2.8218		0.51202		4.5699		0.48846		2.0780		0.27763
135	1.0000e-02		6.6541		0.47404		5.7805		0.47456		2.6246		0.32129
140	1.0000e-02		3.9981		0.47553		11.1624		0.36523		5.7364		0.18737
145	1.0000e-02		3.4729		0.44211		6.8704		0.51170		2.8826		0.24527
150	1.0000e-02		2.8496		0.47361		5.1381		0.52799		3.0086		0.25836
155	1.0000e-02		2.5051		0.51884		10.2938		0.56396		4.2268		0.22805
160	1.0000e-02		3.2761		0.47118		6.9170		0.44230		3.1194		0.23817
165	1.0000e-02		3.1598		0.48743		5.4686		0.54607		2.5543		0.28041
170	1.0000e-02		3.2365		0.48655		4.4212		0.54244		3.2732		0.22748
175	1.0000e-02		3.0208		0.50369		6.2545		0.53068		2.6020		0.30869
180	1.0000e-02		3.0459		0.41499		24.3455		0.20720		13.3958		0.00000
185	1.0000e-02		3.8738		0.42870		22.8332		0.31425		18.9984		0.12541
190	1.0000e-02		3.0487		0.44061		4.5838		0.49019		2.7814		0.26957
195	1.0000e-02		3.1138		0.46078		6.7559		0.53637		2.9473		0.28933
200	1.0000e-02		2.5057		0.48419		8.3863		0.55375		2.5830		0.28568
205	1.0000e-02		2.9080		0.52925		6.5446		0.61154		4.3166		0.26487
210	1.0000e-02		2.4736		0.52514		6.5426		0.53412		2.4693		0.29714
215	1.0000e-02		2.3605		0.53652		5.2426		0.58082		2.8873		0.27773
220	1.0000e-02		2.9793		0.48429		7.2994		0.47949		3.1479		0.29232
225	1.0000e-03		2.4836		0.51635		5.7365		0.49322		3.1161		0.26853
230	1.0000e-03		2.3123		0.51389		5.2984		0.52021		2.8991		0.28854
235	1.0000e-03		1.8020		0.56464		5.2993		0.58579		3.0537		0.30827
240	1.0000e-03		1.7876		0.54114		4.9303		0.56876		3.3268		0.31990
245	1.0000e-03		1.6797		0.55108		4.6367		0.54541		3.0789		0.29487
250	1.0000e-03		1.8650		0.53520		5.1785		0.58067		3.0197		0.30383
255	1.0000e-03		2.2321		0.56560		5.3127		0.60964		3.2645		0.33245
260	1.0000e-03		1.7925		0.55045		5.5338		0.60133		3.0532		0.31974
265	1.0000e-03		2.1241		0.53730		5.5014		0.58929		2.9712		0.32471
270	1.0000e-03		2.1510		0.56972		5.9393		0.63743		3.1042		0.33785
275	1.0000e-03		1.9259		0.53830		5.4581		0.59998		2.8823		0.31445
280	1.0000e-04		1.8990		0.55716		5.1499		0.56689		2.9812		0.29541
285	1.0000e-04		1.9507		0.56005		5.1891		0.60090		3.1120		0.29442
290	1.0000e-04		1.7969		0.55596		5.2878		0.59455		3.0006		0.30153
295	1.0000e-04		1.7560		0.57251		5.3534		0.60647		3.1761		0.30421
300	1.0000e-04		2.0315		0.58776		5.2310		0.57120		3.0940		0.29441
305	1.0000e-04		1.9218		0.57048		5.4831		0.59207		3.0583		0.30068
310	1.0000e-04		1.9349		0.57916		5.3240		0.59168		3.1242		0.29688
315	1.0000e-04		1.8163		0.57140		5.4830		0.61856		3.1243		0.31271
320	1.0000e-04		2.0156		0.56122		5.4561		0.62401		3.3844		0.30456
325	1.0000e-04		1.7796		0.54697		5.2512		0.58212		3.0221		0.29578
330	1.0000e-04		1.6922		0.56058		5.1254		0.57885		2.9923		0.29637
335	1.0000e-05		1.9609		0.57504		5.5047		0.60675		2.9765		0.31223
340	1.0000e-05		1.6240		0.58902		5.3955		0.60493		3.0986		0.31042
345	1.0000e-05		1.8041		0.60136		5.9462		0.63834		3.1793		0.32824
350	1.0000e-05		1.7900		0.55712		5.5913		0.60632		2.9567		0.30959
355	1.0000e-05		2.0380		0.56971		5.6038		0.61560		3.1146		0.31954
360	1.0000e-05		2.0506		0.57650		5.3020		0.60037		2.9774		0.30579
365	1.0000e-05		1.8127		0.61348		5.8276		0.63730		3.6165		0.32432
370	1.0000e-05		1.8289		0.57918		5.4520		0.60310		3.1326		0.30648
375	1.0000e-05		1.7044		0.58794		5.4803		0.61828		3.2599		0.31952
380	1.0000e-05		2.1323		0.56112		5.3739		0.59308		3.2516		0.31083
385	1.0000e-05		1.9383		0.56627		5.6577		0.62711		3.2844		0.31996
390	1.0000e-06		1.8156		0.57327		5.5032		0.60337		3.1721		0.30572
395	1.0000e-06		2.1008		0.56023		5.4877		0.59565		3.0666		0.30367
400	1.0000e-06		1.6339		0.58416		5.8061		0.63160		3.1303		0.32840
405	1.0000e-06		1.8243		0.58716		5.5629		0.60626		3.1238		0.31421
410	1.0000e-06		2.0986		0.57219		5.6574		0.60875		2.9465		0.31583
415	1.0000e-06		1.7263		0.57132		5.2615		0.60416		3.2035		0.30893
420	1.0000e-06		1.7300		0.57875		5.5791		0.61145		3.2460		0.31325
425	1.0000e-06		1.6178		0.56479		5.2867		0.59018		3.1532		0.29699
430	1.0000e-06		1.9590		0.56555		5.5720		0.61549		3.3598		0.31105
435	1.0000e-06		1.9150		0.55894		5.9771		0.64898		3.5904		0.31675
440	1.0000e-06		1.8696		0.55957		5.3677		0.59711		3.4026		0.29308
445	1.0000e-07		2.6453		0.56951		5.3476		0.56780		3.1334		0.29517
450	1.0000e-07		1.5738		0.57534		5.1363		0.57857		3.2738		0.29167
455	1.0000e-07		1.8379		0.57241		5.6685		0.62548		3.2044		0.31878
460	1.0000e-07		1.8913		0.59628		5.4103		0.60684		3.0290		0.30944
465	1.0000e-07		2.0625		0.60735		5.6305		0.61882		3.2960		0.30233
470	1.0000e-07		2.0901		0.58271		5.8257		0.63336		3.6578		0.32149
475	1.0000e-07		1.8359		0.57080		5.6396		0.61333		3.4209		0.31061
480	1.0000e-07		2.1357		0.56544		5.4200		0.61722		3.3490		0.30495
485	1.0000e-07		2.1084		0.56297		5.2529		0.58928		3.1917		0.29475
490	1.0000e-07		1.7935		0.55767		5.1568		0.58551		3.1711		0.29260
495	1.0000e-07		2.0574		0.57448		5.2587		0.60282		3.2740		0.30037
500	1.0000e-08		1.7536		0.56795		5.2936		0.58652		3.3781		0.29482
505	1.0000e-08		1.9581		0.54525		5.3626		0.59127		3.1410		0.29602
510	1.0000e-08		2.5460		0.56079		5.4358		0.59136		3.4915		0.29538
515	1.0000e-08		1.8732		0.56976		5.4842		0.58952		3.0874		0.30351
520	1.0000e-08		1.7099		0.56130		5.5125		0.59737		2.9912		0.30739
525	1.0000e-08		1.6832		0.57433		5.5025		0.59654		2.9879		0.30227
530	1.0000e-08		2.4272		0.56617		5.3492		0.59167		3.3397		0.29108
535	1.0000e-08		2.1873		0.57402		5.4349		0.61228		3.5436		0.30317
540	1.0000e-08		1.7155		0.57283		5.7363		0.64330		3.5090		0.31567
545	1.0000e-08		1.9183		0.56174		5.4098		0.60639		3.2409		0.30555
550	1.0000e-08		2.2577		0.57042		5.5994		0.63332		3.4962		0.32569
