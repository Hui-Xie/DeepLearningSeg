Program ID of SkyWatcher Network training:29195

Program command: 
 ['TrainSkyWatcher.py', '/home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/Images_ROI_29_140_140', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/Labels_ROI_23_127_127', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json']

Major program changes: 
                      merge train and test dataMgr into one.
                      when epoch %5 ==0, do not use mixup.
                      And Only when epoch %5 ==0, print log and save.
                      Along the encoder, use fixed number of filter of 32
                      Use BatchNorm1d in FC layer, instead of InstanceNorm1d.
                      

Experiment setting for Image3d ROI to response:
Input CT data: 29*140*140  3D CT raw image ROI with spacing size(5*2*2)
segmentation label: 23*127*127 with spacing size(5*2*2) segmentation label with value (0,1,2) which erases lymph node label

This is a multi-task learning. 

Predictive Model: 1,  first 3-layer dense conv block with channel size 128.
                  2,  and 3 dense conv DownBB blocks,  each of which includes a stride 2 conv and 3-layers dense conv block; 
                  3,  and 3 fully connected layers  changes the tensor into size 2*1;
                  4,  final a softmax for binary classification;
                  Total network learning parameters are 8 million.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/SkyWatcherModel.py

response Loss Function:   focus loss  with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.
segmentation loss function: focus loss  with weight [1.0416883685076772, 39.37007874015748, 68.39945280437757] for label (0, 1, 2)

Data:   training data has 113 patients, and valdiation data has 27 patients with training/test rate 80/20.
        We randomize all data, and then assign same distrubtion of treat reponse 0,1 into to training and test data set.
        

Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.  

                    Learning Scheduler:  Reduce learning rate on  plateau, and learning rate patience is 30 epochs.                                

            

Program starting Time: 2019-06-17 10:08:07.547788
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher

Now program get 140 input files.
Infor: In all data of 140 files, respone 0 has 41 files,
	  and response 1 has 99 files, where positive response rate = 0.7071428571428572 in full data
==== Regenerate training set and validation set by random with same distribution of 0 and 1 ==== 
Infor: Validation Set has 27 files,and Training Set has 113 files
Infor: In Validataion set, 19 1's, and positive response rate = 0.7037037037037037
Infor: In trainning set, 80 1's,  positive response rate = 0.7079646017699115
Infor: program is in multi samples running model.
TrainTestData Input:  batchSize=4, depth=29, height=140, width=140

Info: the size of bottle neck in the net = (32, 1, 7, 7)

Info: program uses Mixup with alpha=0.4, and mixupProb = 0.5.
Network trains from scratch.
Network has total 514,084 parameters.
Infor: Response Cross Entropy Weight: [3.3333333333333335, 1.4285714285714286] for label[0, 1]
Infor: Segmentation Cross Entropy Weight: [1.0416883685076772, 39.37007874015748, 68.39945280437757] for label (0, 1, 2)
Loss Functions List: FocalCELoss with weight of 1; 	FocalCELoss with weight of 1; 
Hints: Test Dice_0 is the dice coeff for all non-zero labels
Hints: Test Dice_1 is for primary cancer(green), 	
 test Dice_2 is for metastasis(yellow), 	
 and test Dice_3 is for invaded lymph node(brown).
Hints: Test TPR_0 is the TPR for all non-zero labels
Hints: Test TPR_1 is for primary cancer(green), 	
 TPR_2 is for metastasis(yellow), 	
 and TPR_3 is for invaded lymph node(brown).

Dice is based on all 2D segmented slices in the volume from weak annotation, not real 3D dice.

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	TrLoss	Dice0	Dice1	Dice2	TPR_0	TPR_1	TPR_2	Accura	TPR_r	TNR_r		TsLoss	Dice0	Dice1	Dice2	TPR_0	TPR_1	TPR_2	Accura	TPR_r	TNR_r
0	0.9997	0.089	0.117	0.036	0.987	0.746	0.222	0.4732	0.4375	0.5625		0.7425	0.103	0.123	0.050	0.970	0.896	0.361	0.5417	0.7059	0.1429
5	0.5806	0.160	0.225	0.073	0.917	0.631	0.570	0.5893	0.6582	0.4242		0.6764	0.175	0.249	0.111	0.901	0.734	0.632	0.7083	0.8824	0.2857
10	0.4702	0.202	0.321	0.104	0.862	0.573	0.699	0.5804	0.6329	0.4545		0.4827	0.204	0.330	0.144	0.867	0.807	0.725	0.6667	0.8750	0.2500
15	0.4359	0.218	0.345	0.108	0.906	0.629	0.716	0.6250	0.6456	0.5758		0.4602	0.264	0.368	0.211	0.848	0.909	0.697	0.5833	0.7647	0.1429
20	0.4545	0.205	0.326	0.128	0.898	0.723	0.716	0.6339	0.6962	0.4848		0.7074	0.181	0.298	0.121	0.931	0.737	0.875	0.5417	0.7059	0.1429
25	0.3505	0.229	0.406	0.136	0.940	0.732	0.822	0.7054	0.7215	0.6667		0.5393	0.260	0.328	0.211	0.916	0.796	0.846	0.5000	0.6875	0.1250
30	0.3771	0.228	0.406	0.137	0.905	0.748	0.759	0.6071	0.6835	0.4242		0.4648	0.228	0.303	0.174	0.901	0.909	0.722	0.5000	0.6111	0.1667
35	0.3600	0.242	0.403	0.147	0.910	0.798	0.745	0.6339	0.7125	0.4375		0.4323	0.247	0.320	0.173	0.942	0.776	0.892	0.4583	0.5294	0.2857
40	0.3523	0.248	0.407	0.163	0.932	0.764	0.799	0.6696	0.7468	0.4848		0.3936	0.250	0.308	0.222	0.943	0.887	0.865	0.4583	0.5625	0.2500
45	0.3231	0.258	0.424	0.166	0.956	0.783	0.855	0.7054	0.7848	0.5152		0.4308	0.309	0.373	0.300	0.880	0.872	0.788	0.5417	0.6250	0.3750
50	0.2802	0.271	0.449	0.186	0.948	0.868	0.822	0.7589	0.7722	0.7273		0.5305	0.252	0.349	0.213	0.865	0.789	0.810	0.5417	0.5556	0.5000
55	0.2967	0.270	0.436	0.182	0.922	0.823	0.796	0.7589	0.7722	0.7273		0.4213	0.277	0.298	0.267	0.896	0.821	0.863	0.5000	0.5882	0.2857
60	0.2637	0.295	0.460	0.217	0.929	0.867	0.824	0.7232	0.7848	0.5758		0.4151	0.305	0.349	0.280	0.876	0.820	0.825	0.3750	0.4375	0.2500
65	0.2346	0.304	0.477	0.217	0.946	0.909	0.843	0.7679	0.7848	0.7273		0.5083	0.307	0.320	0.326	0.883	0.816	0.834	0.5000	0.5263	0.4000
70	0.2778	0.266	0.455	0.185	0.931	0.828	0.863	0.7589	0.8250	0.5938		0.5135	0.265	0.278	0.265	0.895	0.851	0.795	0.6250	0.7647	0.2857
75	0.2477	0.288	0.456	0.212	0.950	0.895	0.848	0.7411	0.7722	0.6667		0.6605	0.266	0.357	0.223	0.891	0.627	0.804	0.6667	0.8235	0.2857
80	0.2616	0.281	0.444	0.198	0.952	0.855	0.884	0.7679	0.8000	0.6875		0.6575	0.264	0.315	0.229	0.936	0.840	0.901	0.5417	0.5556	0.5000
85	0.1832	0.304	0.472	0.225	0.922	0.918	0.850	0.8750	0.9114	0.7879		0.8385	0.266	0.254	0.283	0.915	0.800	0.846	0.4583	0.6250	0.1250
90	0.2701	0.272	0.434	0.188	0.946	0.863	0.838	0.7679	0.7595	0.7879		0.6008	0.250	0.243	0.186	0.926	0.578	0.929	0.5417	0.6111	0.3333
95	0.2751	0.272	0.457	0.179	0.931	0.888	0.810	0.7411	0.7975	0.6061		0.5539	0.222	0.286	0.165	0.899	0.805	0.877	0.7500	0.8235	0.5714
100	0.2522	0.280	0.482	0.187	0.938	0.861	0.844	0.8393	0.9000	0.6875		0.5834	0.272	0.329	0.243	0.843	0.702	0.871	0.5417	0.6667	0.1667
105	0.2149	0.300	0.473	0.203	0.945	0.842	0.858	0.8750	0.8608	0.9091		0.6898	0.290	0.323	0.232	0.836	0.703	0.742	0.3333	0.4706	0.0000
110	0.1674	0.303	0.458	0.217	0.955	0.912	0.897	0.9107	0.9114	0.9091		0.5448	0.271	0.316	0.232	0.836	0.765	0.771	0.5833	0.7059	0.2857
115	0.1678	0.309	0.460	0.226	0.957	0.895	0.906	0.9018	0.8875	0.9375		0.5840	0.275	0.332	0.214	0.871	0.786	0.828	0.4583	0.4375	0.5000
120	0.1753	0.294	0.471	0.207	0.964	0.881	0.904	0.8929	0.8734	0.9394		0.6579	0.306	0.322	0.249	0.892	0.735	0.823	0.5000	0.5294	0.4286
125	0.1624	0.333	0.482	0.252	0.951	0.907	0.900	0.8839	0.9114	0.8182		0.6286	0.311	0.357	0.268	0.844	0.772	0.806	0.5000	0.6471	0.1429
130	0.1834	0.291	0.455	0.213	0.964	0.909	0.904	0.8571	0.8734	0.8182		0.5669	0.299	0.337	0.287	0.787	0.731	0.781	0.5417	0.7059	0.1429
135	0.1653	0.341	0.485	0.253	0.958	0.931	0.866	0.8393	0.8481	0.8182		0.5106	0.289	0.375	0.227	0.848	0.813	0.788	0.6667	0.7059	0.5714
140	0.1725	0.291	0.440	0.219	0.959	0.916	0.886	0.9107	0.8987	0.9394		0.4614	0.314	0.375	0.238	0.827	0.822	0.646	0.5833	0.6250	0.5000
145	0.1646	0.315	0.471	0.224	0.952	0.926	0.897	0.8839	0.9125	0.8125		0.6440	0.336	0.352	0.244	0.786	0.598	0.663	0.5833	0.6471	0.4286
150	0.1755	0.310	0.451	0.237	0.962	0.936	0.906	0.8571	0.8734	0.8182		0.6248	0.325	0.329	0.262	0.813	0.613	0.740	0.7083	0.8125	0.5000
155	0.1349	0.328	0.473	0.257	0.977	0.925	0.922	0.9196	0.9241	0.9091		0.5638	0.301	0.358	0.239	0.890	0.794	0.771	0.7083	0.7647	0.5714
160	0.1518	0.331	0.481	0.255	0.966	0.913	0.928	0.9196	0.9250	0.9062		0.5470	0.329	0.382	0.262	0.902	0.756	0.873	0.5833	0.7222	0.1667
165	0.1302	0.323	0.472	0.254	0.960	0.922	0.907	0.9375	0.9494	0.9091		0.5581	0.300	0.277	0.266	0.890	0.585	0.889	0.6667	0.7647	0.4286
170	0.1253	0.335	0.495	0.256	0.976	0.914	0.949	0.9196	0.9241	0.9091		0.5068	0.324	0.297	0.274	0.744	0.496	0.765	0.5417	0.5882	0.4286
175	0.1092	0.373	0.497	0.285	0.980	0.952	0.930	0.9196	0.9000	0.9688		0.4884	0.337	0.347	0.297	0.851	0.768	0.797	0.6250	0.6875	0.5000
180	0.0922	0.375	0.524	0.296	0.972	0.936	0.949	0.9464	0.9494	0.9394		0.6013	0.347	0.388	0.265	0.779	0.692	0.735	0.5833	0.7500	0.2500
185	0.1393	0.318	0.474	0.247	0.962	0.941	0.921	0.9375	0.9367	0.9394		0.5645	0.338	0.361	0.300	0.853	0.734	0.800	0.6250	0.8235	0.1429
190	0.0925	0.374	0.519	0.296	0.970	0.944	0.940	0.9554	0.9747	0.9091		0.6715	0.359	0.343	0.311	0.796	0.624	0.731	0.6667	0.7647	0.4286
195	0.0776	0.381	0.513	0.316	0.969	0.929	0.937	0.9821	0.9747	1.0000		0.5878	0.363	0.386	0.308	0.765	0.687	0.669	0.6667	0.8235	0.2857
200	0.0738	0.408	0.527	0.323	0.980	0.957	0.965	0.9911	0.9875	1.0000		0.5563	0.349	0.398	0.318	0.710	0.756	0.583	0.7083	0.8421	0.2000
205	0.1023	0.367	0.541	0.276	0.979	0.956	0.941	0.9375	0.9250	0.9688		0.6707	0.311	0.310	0.288	0.802	0.701	0.770	0.5417	0.6875	0.2500
210	0.0819	0.408	0.524	0.331	0.979	0.954	0.956	0.9821	0.9747	1.0000		0.6347	0.333	0.373	0.290	0.785	0.741	0.715	0.5833	0.7647	0.1429
215	0.0709	0.419	0.553	0.337	0.975	0.965	0.960	0.9732	0.9625	1.0000		0.7067	0.328	0.397	0.282	0.732	0.746	0.674	0.7083	0.9412	0.1429
220	0.0729	0.408	0.570	0.322	0.991	0.963	0.976	0.9821	0.9747	1.0000		0.7415	0.368	0.424	0.296	0.785	0.706	0.775	0.6250	0.8333	0.0000
225	0.0913	0.396	0.573	0.313	0.978	0.973	0.973	0.9375	0.9367	0.9394		0.7714	0.349	0.375	0.286	0.726	0.604	0.666	0.5417	0.5882	0.4286
230	0.0785	0.418	0.577	0.337	0.983	0.960	0.959	0.9643	0.9494	1.0000		0.6811	0.313	0.348	0.275	0.780	0.691	0.753	0.5833	0.7059	0.2857
235	0.1068	0.431	0.569	0.345	0.981	0.966	0.965	0.9196	0.9375	0.8750		0.8523	0.357	0.408	0.284	0.701	0.601	0.678	0.5833	0.7500	0.2500
240	0.1352	0.396	0.550	0.314	0.978	0.951	0.968	0.8839	0.9114	0.8182		0.6242	0.341	0.376	0.281	0.798	0.664	0.735	0.5833	0.7059	0.2857
245	0.0811	0.421	0.555	0.338	0.967	0.958	0.957	0.9464	0.9620	0.9091		0.6110	0.356	0.405	0.296	0.755	0.747	0.622	0.7083	0.8333	0.3333
250	0.1103	0.412	0.544	0.341	0.985	0.963	0.971	0.9286	0.9367	0.9091		0.6481	0.366	0.424	0.285	0.770	0.711	0.652	0.6250	0.6667	0.5000
255	0.0952	0.421	0.570	0.337	0.992	0.982	0.974	0.9286	0.9241	0.9394		0.5544	0.410	0.452	0.346	0.772	0.721	0.688	0.5833	0.6471	0.4286
260	0.0801	0.449	0.583	0.358	0.988	0.970	0.978	0.9464	0.9625	0.9062		0.7321	0.373	0.408	0.303	0.670	0.619	0.589	0.5000	0.7059	0.0000
265	0.0782	0.395	0.545	0.321	0.986	0.955	0.977	0.9643	0.9747	0.9394		0.6629	0.339	0.361	0.289	0.723	0.686	0.626	0.4167	0.5625	0.1250
270	0.0549	0.482	0.635	0.396	0.990	0.980	0.975	0.9821	0.9873	0.9697		0.7413	0.395	0.425	0.313	0.737	0.685	0.631	0.4583	0.6250	0.1250
275	0.0966	0.454	0.618	0.363	0.982	0.965	0.965	0.9107	0.9367	0.8485		0.6956	0.379	0.419	0.319	0.742	0.655	0.666	0.5000	0.7059	0.0000
280	0.0798	0.482	0.616	0.392	0.985	0.983	0.975	0.9196	0.9494	0.8485		0.6976	0.372	0.428	0.274	0.686	0.704	0.509	0.4583	0.5882	0.1429
285	0.0642	0.460	0.595	0.393	0.988	0.991	0.976	0.9821	0.9873	0.9697		0.8248	0.393	0.500	0.266	0.727	0.754	0.553	0.5833	0.7647	0.1429
290	0.0691	0.441	0.614	0.365	0.992	0.961	0.981	0.9732	0.9620	1.0000		0.5190	0.387	0.450	0.292	0.780	0.757	0.618	0.6667	0.8235	0.2857
295	0.0903	0.433	0.569	0.362	0.985	0.965	0.980	0.9643	0.9873	0.9091		0.7408	0.361	0.407	0.272	0.753	0.699	0.618	0.5417	0.7500	0.1250
300	0.0663	0.450	0.598	0.345	0.987	0.953	0.981	0.9821	0.9873	0.9697		0.7526	0.387	0.414	0.307	0.729	0.659	0.625	0.5000	0.6875	0.1250
305	0.0657	0.481	0.636	0.389	0.988	0.985	0.974	0.9643	0.9625	0.9688		0.7412	0.409	0.468	0.309	0.739	0.692	0.631	0.5000	0.6471	0.1429
310	0.0686	0.474	0.633	0.375	0.986	0.978	0.973	0.9643	0.9500	1.0000		0.7346	0.329	0.365	0.260	0.723	0.681	0.570	0.5417	0.7059	0.1429
315	0.0789	0.465	0.586	0.397	0.985	0.972	0.967	0.9732	1.0000	0.9091		0.8478	0.375	0.431	0.299	0.772	0.678	0.691	0.4583	0.6250	0.1250
