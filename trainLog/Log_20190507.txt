Program ID 23450

Program command: 
 ['/home/hxie1/Projects/OvarianCancer/TrainSegV.py', '/home/hxie1/temp_netParameters/OvarianCancer/Label0_1', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainImages', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainLabels', '2D', '0,1']

Major program changes: ConvSeqential use BatchNorm-reLU-Conv structure; 
                       Add ConvSegDecreaseChannels
                       and each block has 5 layers, 
                       Residual connect to each Conv, 
                       skip at least 2 layers.
                       output layer use conv with 3*3 fiter instead of 1*1 filter. 
                       use boundary loss with weight 0 at beginning, and pretrain CE loss. 
            

Program starting Time: 2019-05-07 12:26:21.254955
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/Label0_1

Infor: program test labels: [0, 1]
Infor: program suppressed labels: [2, 3]
Infor: program test labels: [0, 1]
Infor: program suppressed labels: [2, 3]
Building the Segmented Slice Tuple list, which may need 8 mins, please waiting......
Directory of /home/hxie1/data/OvarianCancerCT/Extract_uniform/trainLabels has 164 segmented slices for remained labels [0, 1].
Building the Segmented Slice Tuple list, which may need 8 mins, please waiting......
Directory of /home/hxie1/data/OvarianCancerCT/Extract_uniform/testLabels has 31 segmented slices for remained labels [0, 1].
Info: program uses 2D input.
TrainData Input:  batchSize=8, depth=1, height=281, width=281, NumClassfication=2

TestData Input:  batchSize=8, depth=1, height=281, width=281, NumClassfication=2

Network trains from scratch.
Network has total 404838041 parameters.
Info: network dropout rate = 0.3
Infor: Cross Entropy Weight: [1.0260619741432382, 39.37007874015748]

====================Net Architecture===========================
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
       BatchNorm2d-1          [-1, 1, 281, 281]               2
              ReLU-2          [-1, 1, 281, 281]               0
            Conv2d-3        [-1, 128, 281, 281]           1,280
       BatchNorm2d-4        [-1, 128, 281, 281]             256
              ReLU-5        [-1, 128, 281, 281]               0
            Conv2d-6        [-1, 128, 281, 281]         147,584
       BatchNorm2d-7        [-1, 128, 281, 281]             256
              ReLU-8        [-1, 128, 281, 281]               0
            Conv2d-9        [-1, 128, 281, 281]         147,584
      BatchNorm2d-10        [-1, 128, 281, 281]             256
             ReLU-11        [-1, 128, 281, 281]               0
           Conv2d-12        [-1, 128, 281, 281]         147,584
      BatchNorm2d-13        [-1, 128, 281, 281]             256
             ReLU-14        [-1, 128, 281, 281]               0
           Conv2d-15        [-1, 128, 281, 281]         147,584
   ConvSequential-16        [-1, 128, 281, 281]               0
      BatchNorm2d-17        [-1, 128, 281, 281]             256
           Conv2d-18        [-1, 128, 139, 139]         409,728
      BatchNorm2d-19        [-1, 128, 139, 139]             256
             ReLU-20        [-1, 128, 139, 139]               0
           Conv2d-21        [-1, 128, 139, 139]         147,584
      BatchNorm2d-22        [-1, 128, 139, 139]             256
             ReLU-23        [-1, 128, 139, 139]               0
           Conv2d-24        [-1, 128, 139, 139]         147,584
      BatchNorm2d-25        [-1, 128, 139, 139]             256
             ReLU-26        [-1, 128, 139, 139]               0
           Conv2d-27        [-1, 128, 139, 139]         147,584
      BatchNorm2d-28        [-1, 128, 139, 139]             256
             ReLU-29        [-1, 128, 139, 139]               0
           Conv2d-30        [-1, 128, 139, 139]         147,584
      BatchNorm2d-31        [-1, 128, 139, 139]             256
             ReLU-32        [-1, 128, 139, 139]               0
           Conv2d-33        [-1, 128, 139, 139]         147,584
   ConvSequential-34        [-1, 128, 139, 139]               0
         Down2dBB-35        [-1, 128, 139, 139]               0
        Dropout2d-36        [-1, 128, 139, 139]               0
      BatchNorm2d-37        [-1, 128, 139, 139]             256
           Conv2d-38          [-1, 256, 69, 69]         295,168
      BatchNorm2d-39          [-1, 256, 69, 69]             512
             ReLU-40          [-1, 256, 69, 69]               0
           Conv2d-41          [-1, 256, 69, 69]         590,080
      BatchNorm2d-42          [-1, 256, 69, 69]             512
             ReLU-43          [-1, 256, 69, 69]               0
           Conv2d-44          [-1, 256, 69, 69]         590,080
      BatchNorm2d-45          [-1, 256, 69, 69]             512
             ReLU-46          [-1, 256, 69, 69]               0
           Conv2d-47          [-1, 256, 69, 69]         590,080
      BatchNorm2d-48          [-1, 256, 69, 69]             512
             ReLU-49          [-1, 256, 69, 69]               0
           Conv2d-50          [-1, 256, 69, 69]         590,080
      BatchNorm2d-51          [-1, 256, 69, 69]             512
             ReLU-52          [-1, 256, 69, 69]               0
           Conv2d-53          [-1, 256, 69, 69]         590,080
   ConvSequential-54          [-1, 256, 69, 69]               0
         Down2dBB-55          [-1, 256, 69, 69]               0
        Dropout2d-56          [-1, 256, 69, 69]               0
      BatchNorm2d-57          [-1, 256, 69, 69]             512
           Conv2d-58          [-1, 512, 33, 33]       3,277,312
      BatchNorm2d-59          [-1, 512, 33, 33]           1,024
             ReLU-60          [-1, 512, 33, 33]               0
           Conv2d-61          [-1, 512, 33, 33]       2,359,808
      BatchNorm2d-62          [-1, 512, 33, 33]           1,024
             ReLU-63          [-1, 512, 33, 33]               0
           Conv2d-64          [-1, 512, 33, 33]       2,359,808
      BatchNorm2d-65          [-1, 512, 33, 33]           1,024
             ReLU-66          [-1, 512, 33, 33]               0
           Conv2d-67          [-1, 512, 33, 33]       2,359,808
      BatchNorm2d-68          [-1, 512, 33, 33]           1,024
             ReLU-69          [-1, 512, 33, 33]               0
           Conv2d-70          [-1, 512, 33, 33]       2,359,808
      BatchNorm2d-71          [-1, 512, 33, 33]           1,024
             ReLU-72          [-1, 512, 33, 33]               0
           Conv2d-73          [-1, 512, 33, 33]       2,359,808
   ConvSequential-74          [-1, 512, 33, 33]               0
         Down2dBB-75          [-1, 512, 33, 33]               0
        Dropout2d-76          [-1, 512, 33, 33]               0
      BatchNorm2d-77          [-1, 512, 33, 33]           1,024
           Conv2d-78         [-1, 1024, 15, 15]      13,108,224
      BatchNorm2d-79         [-1, 1024, 15, 15]           2,048
             ReLU-80         [-1, 1024, 15, 15]               0
           Conv2d-81         [-1, 1024, 15, 15]       9,438,208
      BatchNorm2d-82         [-1, 1024, 15, 15]           2,048
             ReLU-83         [-1, 1024, 15, 15]               0
           Conv2d-84         [-1, 1024, 15, 15]       9,438,208
      BatchNorm2d-85         [-1, 1024, 15, 15]           2,048
             ReLU-86         [-1, 1024, 15, 15]               0
           Conv2d-87         [-1, 1024, 15, 15]       9,438,208
      BatchNorm2d-88         [-1, 1024, 15, 15]           2,048
             ReLU-89         [-1, 1024, 15, 15]               0
           Conv2d-90         [-1, 1024, 15, 15]       9,438,208
      BatchNorm2d-91         [-1, 1024, 15, 15]           2,048
             ReLU-92         [-1, 1024, 15, 15]               0
           Conv2d-93         [-1, 1024, 15, 15]       9,438,208
   ConvSequential-94         [-1, 1024, 15, 15]               0
         Down2dBB-95         [-1, 1024, 15, 15]               0
        Dropout2d-96         [-1, 1024, 15, 15]               0
      BatchNorm2d-97         [-1, 1024, 15, 15]           2,048
           Conv2d-98           [-1, 2048, 7, 7]      18,876,416
      BatchNorm2d-99           [-1, 2048, 7, 7]           4,096
            ReLU-100           [-1, 2048, 7, 7]               0
          Conv2d-101           [-1, 2048, 7, 7]      37,750,784
     BatchNorm2d-102           [-1, 2048, 7, 7]           4,096
            ReLU-103           [-1, 2048, 7, 7]               0
          Conv2d-104           [-1, 2048, 7, 7]      37,750,784
     BatchNorm2d-105           [-1, 2048, 7, 7]           4,096
            ReLU-106           [-1, 2048, 7, 7]               0
          Conv2d-107           [-1, 2048, 7, 7]      37,750,784
     BatchNorm2d-108           [-1, 2048, 7, 7]           4,096
            ReLU-109           [-1, 2048, 7, 7]               0
          Conv2d-110           [-1, 2048, 7, 7]      37,750,784
     BatchNorm2d-111           [-1, 2048, 7, 7]           4,096
            ReLU-112           [-1, 2048, 7, 7]               0
          Conv2d-113           [-1, 2048, 7, 7]      37,750,784
  ConvSequential-114           [-1, 2048, 7, 7]               0
        Down2dBB-115           [-1, 2048, 7, 7]               0
       Dropout2d-116           [-1, 2048, 7, 7]               0
     BatchNorm2d-117           [-1, 2048, 7, 7]           4,096
 ConvTranspose2d-118         [-1, 1024, 15, 15]      18,875,392
     BatchNorm2d-119         [-1, 1024, 15, 15]           2,048
            ReLU-120         [-1, 1024, 15, 15]               0
          Conv2d-121         [-1, 1024, 15, 15]       9,438,208
     BatchNorm2d-122         [-1, 1024, 15, 15]           2,048
            ReLU-123         [-1, 1024, 15, 15]               0
          Conv2d-124         [-1, 1024, 15, 15]       9,438,208
     BatchNorm2d-125         [-1, 1024, 15, 15]           2,048
            ReLU-126         [-1, 1024, 15, 15]               0
          Conv2d-127         [-1, 1024, 15, 15]       9,438,208
     BatchNorm2d-128         [-1, 1024, 15, 15]           2,048
            ReLU-129         [-1, 1024, 15, 15]               0
          Conv2d-130         [-1, 1024, 15, 15]       9,438,208
     BatchNorm2d-131         [-1, 1024, 15, 15]           2,048
            ReLU-132         [-1, 1024, 15, 15]               0
          Conv2d-133         [-1, 1024, 15, 15]       9,438,208
  ConvSequential-134         [-1, 1024, 15, 15]               0
          Up2dBB-135         [-1, 1024, 15, 15]               0
       Dropout2d-136         [-1, 1024, 15, 15]               0
     BatchNorm2d-137         [-1, 2048, 15, 15]           4,096
 ConvTranspose2d-138          [-1, 512, 33, 33]      26,214,912
     BatchNorm2d-139          [-1, 512, 33, 33]           1,024
            ReLU-140          [-1, 512, 33, 33]               0
          Conv2d-141          [-1, 512, 33, 33]       2,359,808
     BatchNorm2d-142          [-1, 512, 33, 33]           1,024
            ReLU-143          [-1, 512, 33, 33]               0
          Conv2d-144          [-1, 512, 33, 33]       2,359,808
     BatchNorm2d-145          [-1, 512, 33, 33]           1,024
            ReLU-146          [-1, 512, 33, 33]               0
          Conv2d-147          [-1, 512, 33, 33]       2,359,808
     BatchNorm2d-148          [-1, 512, 33, 33]           1,024
            ReLU-149          [-1, 512, 33, 33]               0
          Conv2d-150          [-1, 512, 33, 33]       2,359,808
     BatchNorm2d-151          [-1, 512, 33, 33]           1,024
            ReLU-152          [-1, 512, 33, 33]               0
          Conv2d-153          [-1, 512, 33, 33]       2,359,808
  ConvSequential-154          [-1, 512, 33, 33]               0
          Up2dBB-155          [-1, 512, 33, 33]               0
       Dropout2d-156          [-1, 512, 33, 33]               0
     BatchNorm2d-157         [-1, 1024, 33, 33]           2,048
 ConvTranspose2d-158          [-1, 256, 69, 69]       6,553,856
     BatchNorm2d-159          [-1, 256, 69, 69]             512
            ReLU-160          [-1, 256, 69, 69]               0
          Conv2d-161          [-1, 256, 69, 69]         590,080
     BatchNorm2d-162          [-1, 256, 69, 69]             512
            ReLU-163          [-1, 256, 69, 69]               0
          Conv2d-164          [-1, 256, 69, 69]         590,080
     BatchNorm2d-165          [-1, 256, 69, 69]             512
            ReLU-166          [-1, 256, 69, 69]               0
          Conv2d-167          [-1, 256, 69, 69]         590,080
     BatchNorm2d-168          [-1, 256, 69, 69]             512
            ReLU-169          [-1, 256, 69, 69]               0
          Conv2d-170          [-1, 256, 69, 69]         590,080
     BatchNorm2d-171          [-1, 256, 69, 69]             512
            ReLU-172          [-1, 256, 69, 69]               0
          Conv2d-173          [-1, 256, 69, 69]         590,080
  ConvSequential-174          [-1, 256, 69, 69]               0
          Up2dBB-175          [-1, 256, 69, 69]               0
       Dropout2d-176          [-1, 256, 69, 69]               0
     BatchNorm2d-177          [-1, 512, 69, 69]           1,024
 ConvTranspose2d-178        [-1, 128, 139, 139]         589,952
     BatchNorm2d-179        [-1, 128, 139, 139]             256
            ReLU-180        [-1, 128, 139, 139]               0
          Conv2d-181        [-1, 128, 139, 139]         147,584
     BatchNorm2d-182        [-1, 128, 139, 139]             256
            ReLU-183        [-1, 128, 139, 139]               0
          Conv2d-184        [-1, 128, 139, 139]         147,584
     BatchNorm2d-185        [-1, 128, 139, 139]             256
            ReLU-186        [-1, 128, 139, 139]               0
          Conv2d-187        [-1, 128, 139, 139]         147,584
     BatchNorm2d-188        [-1, 128, 139, 139]             256
            ReLU-189        [-1, 128, 139, 139]               0
          Conv2d-190        [-1, 128, 139, 139]         147,584
     BatchNorm2d-191        [-1, 128, 139, 139]             256
            ReLU-192        [-1, 128, 139, 139]               0
          Conv2d-193        [-1, 128, 139, 139]         147,584
  ConvSequential-194        [-1, 128, 139, 139]               0
          Up2dBB-195        [-1, 128, 139, 139]               0
       Dropout2d-196        [-1, 128, 139, 139]               0
     BatchNorm2d-197        [-1, 256, 139, 139]             512
 ConvTranspose2d-198        [-1, 128, 281, 281]         819,328
     BatchNorm2d-199        [-1, 128, 281, 281]             256
            ReLU-200        [-1, 128, 281, 281]               0
          Conv2d-201        [-1, 128, 281, 281]         147,584
     BatchNorm2d-202        [-1, 128, 281, 281]             256
            ReLU-203        [-1, 128, 281, 281]               0
          Conv2d-204        [-1, 128, 281, 281]         147,584
     BatchNorm2d-205        [-1, 128, 281, 281]             256
            ReLU-206        [-1, 128, 281, 281]               0
          Conv2d-207        [-1, 128, 281, 281]         147,584
     BatchNorm2d-208        [-1, 128, 281, 281]             256
            ReLU-209        [-1, 128, 281, 281]               0
          Conv2d-210        [-1, 128, 281, 281]         147,584
     BatchNorm2d-211        [-1, 128, 281, 281]             256
            ReLU-212        [-1, 128, 281, 281]               0
          Conv2d-213        [-1, 128, 281, 281]         147,584
  ConvSequential-214        [-1, 128, 281, 281]               0
          Up2dBB-215        [-1, 128, 281, 281]               0
       Dropout2d-216        [-1, 128, 281, 281]               0
     BatchNorm2d-217        [-1, 256, 281, 281]             512
            ReLU-218        [-1, 256, 281, 281]               0
          Conv2d-219        [-1, 129, 281, 281]         297,345
     BatchNorm2d-220        [-1, 129, 281, 281]             258
            ReLU-221        [-1, 129, 281, 281]               0
          Conv2d-222          [-1, 2, 281, 281]           2,324
ConvSeqDecreaseChannels-223          [-1, 2, 281, 281]               0
================================================================
Total params: 404,838,041
Trainable params: 404,838,041
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.30
Forward/backward pass size (MB): 4583.59
Params size (MB): 1544.33
Estimated Total Size (MB): 6128.22
----------------------------------------------------------------
===================End of Net Architecture =====================

Info: program will use 4 GPUs.
Loss Functions List: FocalCELoss with weight of 1; 	BoundaryLoss with weight of 0; 
Hints: Test Dice_0 is the dice coeff for all non-zero labels
Hints: Test Dice_1 is for primary cancer(green), test Dice_2 is for metastasis(yellow), and test Dice_3 is for invaded lymph node(brown).
Hints: Test TPR_0 is the TPR for all non-zero labels
Hints: Test TPR_1 is for primary cancer(green), TPR_2 is for metastasis(yellow), and TPR_3 is for invaded lymph node(brown).

Epoch 	 TrainingLoss 	 TestLoss 	Dice_0	Dice_1	TPR_0	TPR_1
0 	 0.0928 	 0.0726 	0.308	0.308	0.984	0.984
1 	 0.0730 	 0.1486 	0.328	0.328	0.699	0.699
2 	 0.0638 	 1.1393 	0.305	0.305	0.402	0.402
3 	 0.0565 	 0.1489 	0.447	0.447	0.921	0.921
4 	 0.0543 	 0.0838 	0.379	0.379	0.954	0.954
5 	 0.0551 	 0.1382 	0.392	0.392	0.930	0.930
6 	 0.0498 	 0.0434 	0.504	0.504	0.971	0.971
7 	 0.0512 	 0.1024 	0.481	0.481	0.923	0.923
8 	 0.0445 	 0.0504 	0.492	0.492	0.969	0.969
9 	 0.0443 	 0.0464 	0.500	0.500	0.955	0.955
10 	 0.0467 	 0.0431 	0.465	0.465	0.987	0.987
11 	 0.0436 	 0.0627 	0.548	0.548	0.956	0.956
12 	 0.0468 	 0.0478 	0.454	0.454	0.960	0.960
13 	 0.0442 	 0.0564 	0.426	0.426	0.988	0.988
14 	 0.0414 	 0.0479 	0.550	0.550	0.970	0.970
15 	 0.0391 	 0.0399 	0.563	0.563	0.966	0.966
16 	 0.0418 	 0.0799 	0.494	0.494	0.956	0.956
17 	 0.0389 	 0.2587 	0.569	0.569	0.646	0.646
18 	 0.0410 	 0.0362 	0.487	0.487	0.995	0.995
19 	 0.0374 	 0.0566 	0.498	0.498	0.972	0.972
20 	 0.0403 	 0.0438 	0.505	0.505	0.975	0.975
21 	 0.0433 	 0.0671 	0.452	0.452	0.970	0.970
22 	 0.0420 	 0.0567 	0.565	0.565	0.936	0.936
23 	 0.0423 	 0.0796 	0.466	0.466	0.957	0.957
24 	 0.0442 	 0.0726 	0.586	0.586	0.937	0.937
25 	 0.0375 	 0.0351 	0.542	0.542	0.975	0.975
26 	 0.0333 	 0.0644 	0.617	0.617	0.914	0.914
27 	 0.0364 	 0.0517 	0.485	0.485	0.976	0.976
28 	 0.0376 	 0.0400 	0.494	0.494	0.983	0.983
29 	 0.0376 	 0.0514 	0.534	0.534	0.966	0.966
30 	 0.0349 	 0.0408 	0.594	0.594	0.982	0.982
31 	 0.0364 	 0.0428 	0.539	0.539	0.972	0.972
32 	 0.0318 	 0.0662 	0.529	0.529	0.945	0.945
33 	 0.0323 	 0.0373 	0.570	0.570	0.975	0.975
34 	 0.0321 	 0.0641 	0.583	0.583	0.944	0.944
35 	 0.0332 	 0.1084 	0.600	0.600	0.916	0.916
36 	 0.0285 	 0.0366 	0.592	0.592	0.970	0.970
37 	 0.0298 	 0.0557 	0.599	0.599	0.962	0.962
38 	 0.0334 	 0.0960 	0.614	0.614	0.943	0.943
39 	 0.0342 	 0.0527 	0.573	0.573	0.968	0.968
40 	 0.0344 	 0.0369 	0.523	0.523	0.978	0.978
41 	 0.0392 	 0.0360 	0.532	0.532	0.986	0.986
42 	 0.0350 	 0.0376 	0.618	0.618	0.967	0.967
43 	 0.0297 	 0.0413 	0.579	0.579	0.978	0.978
44 	 0.0325 	 0.0747 	0.571	0.571	0.973	0.973
45 	 0.0320 	 0.0548 	0.649	0.649	0.978	0.978
46 	 0.0303 	 0.0459 	0.515	0.515	0.993	0.993
47 	 0.0309 	 0.1798 	0.560	0.560	0.905	0.905
48 	 0.0314 	 0.1652 	0.597	0.597	0.955	0.955
49 	 0.0376 	 0.1528 	0.543	0.543	0.942	0.942
50 	 0.0417 	 0.0524 	0.572	0.572	0.954	0.954
51 	 0.0373 	 0.1386 	0.600	0.600	0.945	0.945
52 	 0.0372 	 0.0550 	0.622	0.622	0.970	0.970
53 	 0.0364 	 0.0985 	0.565	0.565	0.965	0.965
54 	 0.0307 	 0.0558 	0.547	0.547	0.972	0.972
55 	 0.0303 	 0.0363 	0.602	0.602	0.972	0.972
56 	 0.0292 	 0.0445 	0.598	0.598	0.961	0.961
57 	 0.0302 	 0.0748 	0.588	0.588	0.953	0.953
58 	 0.0288 	 0.0734 	0.604	0.604	0.969	0.969
59 	 0.0282 	 0.0749 	0.624	0.624	0.952	0.952
60 	 0.0279 	 0.0711 	0.633	0.633	0.951	0.951
61 	 0.0261 	 0.0744 	0.610	0.610	0.969	0.969
62 	 0.0305 	 0.0732 	0.629	0.629	0.969	0.969
63 	 0.0287 	 0.0912 	0.665	0.665	0.941	0.941
64 	 0.0268 	 0.0580 	0.607	0.607	0.948	0.948
65 	 0.0266 	 0.0565 	0.630	0.630	0.949	0.949
66 	 0.0265 	 0.0623 	0.635	0.635	0.947	0.947
67 	 0.0263 	 0.0619 	0.631	0.631	0.958	0.958
68 	 0.0261 	 0.0514 	0.620	0.620	0.971	0.971
69 	 0.0232 	 0.0630 	0.664	0.664	0.954	0.954
70 	 0.0258 	 0.0985 	0.662	0.662	0.922	0.922
71 	 0.0289 	 0.1088 	0.596	0.596	0.960	0.960
72 	 0.0261 	 0.0726 	0.615	0.615	0.963	0.963
73 	 0.0312 	 0.1877 	0.642	0.642	0.932	0.932
74 	 0.0339 	 0.0779 	0.629	0.629	0.962	0.962
75 	 0.0284 	 0.0508 	0.653	0.653	0.958	0.958
76 	 0.0256 	 0.0959 	0.639	0.639	0.887	0.887
77 	 0.0291 	 0.0507 	0.572	0.572	0.942	0.942
78 	 0.0293 	 0.1227 	0.598	0.598	0.927	0.927
79 	 0.0280 	 0.1096 	0.617	0.617	0.919	0.919
80 	 0.0265 	 0.1259 	0.604	0.604	0.918	0.918
81 	 0.0269 	 0.0716 	0.627	0.627	0.926	0.926
82 	 0.0256 	 0.0616 	0.617	0.617	0.960	0.960
83 	 0.0258 	 0.0643 	0.623	0.623	0.935	0.935
84 	 0.0239 	 0.0887 	0.630	0.630	0.900	0.900
85 	 0.0245 	 0.1740 	0.586	0.586	0.870	0.870
86 	 0.0280 	 0.0960 	0.590	0.590	0.916	0.916
87 	 0.0271 	 0.0824 	0.602	0.602	0.956	0.956
88 	 0.0256 	 0.0663 	0.616	0.616	0.954	0.954
89 	 0.0248 	 0.0799 	0.627	0.627	0.929	0.929
90 	 0.0242 	 0.0652 	0.627	0.627	0.963	0.963
91 	 0.0240 	 0.0967 	0.629	0.629	0.935	0.935
92 	 0.0227 	 0.1080 	0.640	0.640	0.942	0.942
93 	 0.0238 	 0.0758 	0.615	0.615	0.960	0.960
94 	 0.0234 	 0.1242 	0.650	0.650	0.944	0.944
95 	 0.0246 	 0.1022 	0.635	0.635	0.950	0.950
96 	 0.0237 	 0.0966 	0.632	0.632	0.939	0.939
97 	 0.0233 	 0.0797 	0.619	0.619	0.945	0.945
98 	 0.0213 	 0.1059 	0.678	0.678	0.909	0.909
99 	 0.0227 	 0.1709 	0.655	0.655	0.907	0.907
100 	 0.0244 	 0.0533 	0.614	0.614	0.970	0.970
101 	 0.0251 	 0.0937 	0.645	0.645	0.946	0.946
102 	 0.0234 	 0.1639 	0.633	0.633	0.881	0.881
103 	 0.0219 	 0.2026 	0.620	0.620	0.875	0.875
104 	 0.0227 	 0.1596 	0.607	0.607	0.883	0.883
105 	 0.0208 	 0.1335 	0.632	0.632	0.886	0.886
106 	 0.0225 	 0.1656 	0.617	0.617	0.884	0.884
107 	 0.0214 	 0.1286 	0.652	0.652	0.944	0.944
108 	 0.0228 	 0.1019 	0.649	0.649	0.945	0.945
109 	 0.0233 	 0.1126 	0.622	0.622	0.930	0.930
110 	 0.0220 	 0.1221 	0.664	0.664	0.918	0.918
111 	 0.0221 	 0.1739 	0.617	0.617	0.873	0.873
112 	 0.0220 	 0.1668 	0.646	0.646	0.878	0.878
113 	 0.0226 	 0.2166 	0.624	0.624	0.855	0.855
114 	 0.0233 	 0.0914 	0.647	0.647	0.935	0.935
115 	 0.0229 	 0.0817 	0.650	0.650	0.946	0.946
116 	 0.0218 	 0.1011 	0.641	0.641	0.926	0.926
117 	 0.0209 	 0.1263 	0.654	0.654	0.914	0.914
118 	 0.0215 	 0.0946 	0.669	0.669	0.939	0.939
119 	 0.0219 	 0.1224 	0.664	0.664	0.926	0.926
120 	 0.0196 	 0.3418 	0.615	0.615	0.863	0.863
121 	 0.0198 	 0.2613 	0.633	0.633	0.864	0.864
122 	 0.0209 	 0.1316 	0.653	0.653	0.915	0.915
123 	 0.0211 	 0.0861 	0.644	0.644	0.936	0.936
124 	 0.0209 	 0.0893 	0.643	0.643	0.936	0.936
125 	 0.0202 	 0.1681 	0.667	0.667	0.910	0.910
126 	 0.0197 	 0.1685 	0.653	0.653	0.909	0.909
127 	 0.0202 	 0.2052 	0.666	0.666	0.897	0.897
128 	 0.0207 	 0.1190 	0.662	0.662	0.926	0.926
129 	 0.0189 	 0.1262 	0.673	0.673	0.917	0.917
130 	 0.0197 	 0.1007 	0.676	0.676	0.930	0.930
131 	 0.0204 	 0.0750 	0.680	0.680	0.947	0.947
132 	 0.0209 	 0.1188 	0.654	0.654	0.914	0.914
133 	 0.0190 	 0.2027 	0.628	0.628	0.865	0.865
134 	 0.0198 	 0.6182 	0.566	0.566	0.750	0.750
135 	 0.0179 	 0.7263 	0.542	0.542	0.709	0.709
136 	 0.0204 	 0.2672 	0.632	0.632	0.878	0.878
137 	 0.0193 	 0.3821 	0.611	0.611	0.844	0.844
138 	 0.0179 	 0.6243 	0.583	0.583	0.755	0.755
139 	 0.0197 	 0.1620 	0.635	0.635	0.898	0.898
140 	 0.0212 	 0.1202 	0.659	0.659	0.882	0.882
141 	 0.0200 	 0.0912 	0.680	0.680	0.931	0.931
142 	 0.0199 	 0.1331 	0.669	0.669	0.906	0.906
143 	 0.0188 	 0.1549 	0.685	0.685	0.888	0.888
144 	 0.0195 	 0.1472 	0.681	0.681	0.917	0.917
145 	 0.0170 	 0.2071 	0.688	0.688	0.877	0.877
146 	 0.0179 	 0.2747 	0.658	0.658	0.874	0.874
147 	 0.0221 	 0.1676 	0.643	0.643	0.876	0.876
148 	 0.0192 	 0.1022 	0.694	0.694	0.922	0.922
149 	 0.0206 	 0.1178 	0.683	0.683	0.919	0.919
150 	 0.0205 	 0.1075 	0.680	0.680	0.938	0.938
151 	 0.0184 	 0.1284 	0.700	0.700	0.929	0.929
152 	 0.0197 	 0.0975 	0.672	0.672	0.940	0.940
153 	 0.0195 	 0.0954 	0.674	0.674	0.946	0.946
154 	 0.0188 	 0.0919 	0.689	0.689	0.934	0.934
155 	 0.0188 	 0.0846 	0.682	0.682	0.939	0.939
156 	 0.0182 	 0.0965 	0.704	0.704	0.936	0.936
157 	 0.0186 	 0.0869 	0.683	0.683	0.943	0.943
158 	 0.0177 	 0.0972 	0.685	0.685	0.939	0.939
159 	 0.0172 	 0.1012 	0.686	0.686	0.904	0.904
160 	 0.0174 	 0.1052 	0.685	0.685	0.914	0.914
161 	 0.0184 	 0.1361 	0.670	0.670	0.897	0.897
