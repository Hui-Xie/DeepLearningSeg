Program ID 23450

Program command: 
 ['/home/hxie1/Projects/OvarianCancer/TrainSegV.py', '/home/hxie1/temp_netParameters/OvarianCancer/Label0_1', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainImages', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainLabels', '2D', '0,1']

Major program changes: ConvSeqential use BatchNorm-reLU-Conv structure; 
                       Add ConvSegDecreaseChannels
                       and each block has 5 layers, 
                       Residual connect to each Conv, 
                       skip at least 2 layers.
                       output layer use conv with 3*3 fiter instead of 1*1 filter. 
                       use boundary loss with weight 0 at beginning, and pretrain CE loss. 
            

Program starting Time: 2019-05-07 12:26:21.254955
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/Label0_1

Infor: program test labels: [0, 1]
Infor: program suppressed labels: [2, 3]
Infor: program test labels: [0, 1]
Infor: program suppressed labels: [2, 3]
Building the Segmented Slice Tuple list, which may need 8 mins, please waiting......
Directory of /home/hxie1/data/OvarianCancerCT/Extract_uniform/trainLabels has 164 segmented slices for remained labels [0, 1].
Building the Segmented Slice Tuple list, which may need 8 mins, please waiting......
Directory of /home/hxie1/data/OvarianCancerCT/Extract_uniform/testLabels has 31 segmented slices for remained labels [0, 1].
Info: program uses 2D input.
TrainData Input:  batchSize=8, depth=1, height=281, width=281, NumClassfication=2

TestData Input:  batchSize=8, depth=1, height=281, width=281, NumClassfication=2

Network trains from scratch.
Network has total 404838041 parameters.
Info: network dropout rate = 0.3
Infor: Cross Entropy Weight: [1.0260619741432382, 39.37007874015748]

====================Net Architecture===========================
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
       BatchNorm2d-1          [-1, 1, 281, 281]               2
              ReLU-2          [-1, 1, 281, 281]               0
            Conv2d-3        [-1, 128, 281, 281]           1,280
       BatchNorm2d-4        [-1, 128, 281, 281]             256
              ReLU-5        [-1, 128, 281, 281]               0
            Conv2d-6        [-1, 128, 281, 281]         147,584
       BatchNorm2d-7        [-1, 128, 281, 281]             256
              ReLU-8        [-1, 128, 281, 281]               0
            Conv2d-9        [-1, 128, 281, 281]         147,584
      BatchNorm2d-10        [-1, 128, 281, 281]             256
             ReLU-11        [-1, 128, 281, 281]               0
           Conv2d-12        [-1, 128, 281, 281]         147,584
      BatchNorm2d-13        [-1, 128, 281, 281]             256
             ReLU-14        [-1, 128, 281, 281]               0
           Conv2d-15        [-1, 128, 281, 281]         147,584
   ConvSequential-16        [-1, 128, 281, 281]               0
      BatchNorm2d-17        [-1, 128, 281, 281]             256
           Conv2d-18        [-1, 128, 139, 139]         409,728
      BatchNorm2d-19        [-1, 128, 139, 139]             256
             ReLU-20        [-1, 128, 139, 139]               0
           Conv2d-21        [-1, 128, 139, 139]         147,584
      BatchNorm2d-22        [-1, 128, 139, 139]             256
             ReLU-23        [-1, 128, 139, 139]               0
           Conv2d-24        [-1, 128, 139, 139]         147,584
      BatchNorm2d-25        [-1, 128, 139, 139]             256
             ReLU-26        [-1, 128, 139, 139]               0
           Conv2d-27        [-1, 128, 139, 139]         147,584
      BatchNorm2d-28        [-1, 128, 139, 139]             256
             ReLU-29        [-1, 128, 139, 139]               0
           Conv2d-30        [-1, 128, 139, 139]         147,584
      BatchNorm2d-31        [-1, 128, 139, 139]             256
             ReLU-32        [-1, 128, 139, 139]               0
           Conv2d-33        [-1, 128, 139, 139]         147,584
   ConvSequential-34        [-1, 128, 139, 139]               0
         Down2dBB-35        [-1, 128, 139, 139]               0
        Dropout2d-36        [-1, 128, 139, 139]               0
      BatchNorm2d-37        [-1, 128, 139, 139]             256
           Conv2d-38          [-1, 256, 69, 69]         295,168
      BatchNorm2d-39          [-1, 256, 69, 69]             512
             ReLU-40          [-1, 256, 69, 69]               0
           Conv2d-41          [-1, 256, 69, 69]         590,080
      BatchNorm2d-42          [-1, 256, 69, 69]             512
             ReLU-43          [-1, 256, 69, 69]               0
           Conv2d-44          [-1, 256, 69, 69]         590,080
      BatchNorm2d-45          [-1, 256, 69, 69]             512
             ReLU-46          [-1, 256, 69, 69]               0
           Conv2d-47          [-1, 256, 69, 69]         590,080
      BatchNorm2d-48          [-1, 256, 69, 69]             512
             ReLU-49          [-1, 256, 69, 69]               0
           Conv2d-50          [-1, 256, 69, 69]         590,080
      BatchNorm2d-51          [-1, 256, 69, 69]             512
             ReLU-52          [-1, 256, 69, 69]               0
           Conv2d-53          [-1, 256, 69, 69]         590,080
   ConvSequential-54          [-1, 256, 69, 69]               0
         Down2dBB-55          [-1, 256, 69, 69]               0
        Dropout2d-56          [-1, 256, 69, 69]               0
      BatchNorm2d-57          [-1, 256, 69, 69]             512
           Conv2d-58          [-1, 512, 33, 33]       3,277,312
      BatchNorm2d-59          [-1, 512, 33, 33]           1,024
             ReLU-60          [-1, 512, 33, 33]               0
           Conv2d-61          [-1, 512, 33, 33]       2,359,808
      BatchNorm2d-62          [-1, 512, 33, 33]           1,024
             ReLU-63          [-1, 512, 33, 33]               0
           Conv2d-64          [-1, 512, 33, 33]       2,359,808
      BatchNorm2d-65          [-1, 512, 33, 33]           1,024
             ReLU-66          [-1, 512, 33, 33]               0
           Conv2d-67          [-1, 512, 33, 33]       2,359,808
      BatchNorm2d-68          [-1, 512, 33, 33]           1,024
             ReLU-69          [-1, 512, 33, 33]               0
           Conv2d-70          [-1, 512, 33, 33]       2,359,808
      BatchNorm2d-71          [-1, 512, 33, 33]           1,024
             ReLU-72          [-1, 512, 33, 33]               0
           Conv2d-73          [-1, 512, 33, 33]       2,359,808
   ConvSequential-74          [-1, 512, 33, 33]               0
         Down2dBB-75          [-1, 512, 33, 33]               0
        Dropout2d-76          [-1, 512, 33, 33]               0
      BatchNorm2d-77          [-1, 512, 33, 33]           1,024
           Conv2d-78         [-1, 1024, 15, 15]      13,108,224
      BatchNorm2d-79         [-1, 1024, 15, 15]           2,048
             ReLU-80         [-1, 1024, 15, 15]               0
           Conv2d-81         [-1, 1024, 15, 15]       9,438,208
      BatchNorm2d-82         [-1, 1024, 15, 15]           2,048
             ReLU-83         [-1, 1024, 15, 15]               0
           Conv2d-84         [-1, 1024, 15, 15]       9,438,208
      BatchNorm2d-85         [-1, 1024, 15, 15]           2,048
             ReLU-86         [-1, 1024, 15, 15]               0
           Conv2d-87         [-1, 1024, 15, 15]       9,438,208
      BatchNorm2d-88         [-1, 1024, 15, 15]           2,048
             ReLU-89         [-1, 1024, 15, 15]               0
           Conv2d-90         [-1, 1024, 15, 15]       9,438,208
      BatchNorm2d-91         [-1, 1024, 15, 15]           2,048
             ReLU-92         [-1, 1024, 15, 15]               0
           Conv2d-93         [-1, 1024, 15, 15]       9,438,208
   ConvSequential-94         [-1, 1024, 15, 15]               0
         Down2dBB-95         [-1, 1024, 15, 15]               0
        Dropout2d-96         [-1, 1024, 15, 15]               0
      BatchNorm2d-97         [-1, 1024, 15, 15]           2,048
           Conv2d-98           [-1, 2048, 7, 7]      18,876,416
      BatchNorm2d-99           [-1, 2048, 7, 7]           4,096
            ReLU-100           [-1, 2048, 7, 7]               0
          Conv2d-101           [-1, 2048, 7, 7]      37,750,784
     BatchNorm2d-102           [-1, 2048, 7, 7]           4,096
            ReLU-103           [-1, 2048, 7, 7]               0
          Conv2d-104           [-1, 2048, 7, 7]      37,750,784
     BatchNorm2d-105           [-1, 2048, 7, 7]           4,096
            ReLU-106           [-1, 2048, 7, 7]               0
          Conv2d-107           [-1, 2048, 7, 7]      37,750,784
     BatchNorm2d-108           [-1, 2048, 7, 7]           4,096
            ReLU-109           [-1, 2048, 7, 7]               0
          Conv2d-110           [-1, 2048, 7, 7]      37,750,784
     BatchNorm2d-111           [-1, 2048, 7, 7]           4,096
            ReLU-112           [-1, 2048, 7, 7]               0
          Conv2d-113           [-1, 2048, 7, 7]      37,750,784
  ConvSequential-114           [-1, 2048, 7, 7]               0
        Down2dBB-115           [-1, 2048, 7, 7]               0
       Dropout2d-116           [-1, 2048, 7, 7]               0
     BatchNorm2d-117           [-1, 2048, 7, 7]           4,096
 ConvTranspose2d-118         [-1, 1024, 15, 15]      18,875,392
     BatchNorm2d-119         [-1, 1024, 15, 15]           2,048
            ReLU-120         [-1, 1024, 15, 15]               0
          Conv2d-121         [-1, 1024, 15, 15]       9,438,208
     BatchNorm2d-122         [-1, 1024, 15, 15]           2,048
            ReLU-123         [-1, 1024, 15, 15]               0
          Conv2d-124         [-1, 1024, 15, 15]       9,438,208
     BatchNorm2d-125         [-1, 1024, 15, 15]           2,048
            ReLU-126         [-1, 1024, 15, 15]               0
          Conv2d-127         [-1, 1024, 15, 15]       9,438,208
     BatchNorm2d-128         [-1, 1024, 15, 15]           2,048
            ReLU-129         [-1, 1024, 15, 15]               0
          Conv2d-130         [-1, 1024, 15, 15]       9,438,208
     BatchNorm2d-131         [-1, 1024, 15, 15]           2,048
            ReLU-132         [-1, 1024, 15, 15]               0
          Conv2d-133         [-1, 1024, 15, 15]       9,438,208
  ConvSequential-134         [-1, 1024, 15, 15]               0
          Up2dBB-135         [-1, 1024, 15, 15]               0
       Dropout2d-136         [-1, 1024, 15, 15]               0
     BatchNorm2d-137         [-1, 2048, 15, 15]           4,096
 ConvTranspose2d-138          [-1, 512, 33, 33]      26,214,912
     BatchNorm2d-139          [-1, 512, 33, 33]           1,024
            ReLU-140          [-1, 512, 33, 33]               0
          Conv2d-141          [-1, 512, 33, 33]       2,359,808
     BatchNorm2d-142          [-1, 512, 33, 33]           1,024
            ReLU-143          [-1, 512, 33, 33]               0
          Conv2d-144          [-1, 512, 33, 33]       2,359,808
     BatchNorm2d-145          [-1, 512, 33, 33]           1,024
            ReLU-146          [-1, 512, 33, 33]               0
          Conv2d-147          [-1, 512, 33, 33]       2,359,808
     BatchNorm2d-148          [-1, 512, 33, 33]           1,024
            ReLU-149          [-1, 512, 33, 33]               0
          Conv2d-150          [-1, 512, 33, 33]       2,359,808
     BatchNorm2d-151          [-1, 512, 33, 33]           1,024
            ReLU-152          [-1, 512, 33, 33]               0
          Conv2d-153          [-1, 512, 33, 33]       2,359,808
  ConvSequential-154          [-1, 512, 33, 33]               0
          Up2dBB-155          [-1, 512, 33, 33]               0
       Dropout2d-156          [-1, 512, 33, 33]               0
     BatchNorm2d-157         [-1, 1024, 33, 33]           2,048
 ConvTranspose2d-158          [-1, 256, 69, 69]       6,553,856
     BatchNorm2d-159          [-1, 256, 69, 69]             512
            ReLU-160          [-1, 256, 69, 69]               0
          Conv2d-161          [-1, 256, 69, 69]         590,080
     BatchNorm2d-162          [-1, 256, 69, 69]             512
            ReLU-163          [-1, 256, 69, 69]               0
          Conv2d-164          [-1, 256, 69, 69]         590,080
     BatchNorm2d-165          [-1, 256, 69, 69]             512
            ReLU-166          [-1, 256, 69, 69]               0
          Conv2d-167          [-1, 256, 69, 69]         590,080
     BatchNorm2d-168          [-1, 256, 69, 69]             512
            ReLU-169          [-1, 256, 69, 69]               0
          Conv2d-170          [-1, 256, 69, 69]         590,080
     BatchNorm2d-171          [-1, 256, 69, 69]             512
            ReLU-172          [-1, 256, 69, 69]               0
          Conv2d-173          [-1, 256, 69, 69]         590,080
  ConvSequential-174          [-1, 256, 69, 69]               0
          Up2dBB-175          [-1, 256, 69, 69]               0
       Dropout2d-176          [-1, 256, 69, 69]               0
     BatchNorm2d-177          [-1, 512, 69, 69]           1,024
 ConvTranspose2d-178        [-1, 128, 139, 139]         589,952
     BatchNorm2d-179        [-1, 128, 139, 139]             256
            ReLU-180        [-1, 128, 139, 139]               0
          Conv2d-181        [-1, 128, 139, 139]         147,584
     BatchNorm2d-182        [-1, 128, 139, 139]             256
            ReLU-183        [-1, 128, 139, 139]               0
          Conv2d-184        [-1, 128, 139, 139]         147,584
     BatchNorm2d-185        [-1, 128, 139, 139]             256
            ReLU-186        [-1, 128, 139, 139]               0
          Conv2d-187        [-1, 128, 139, 139]         147,584
     BatchNorm2d-188        [-1, 128, 139, 139]             256
            ReLU-189        [-1, 128, 139, 139]               0
          Conv2d-190        [-1, 128, 139, 139]         147,584
     BatchNorm2d-191        [-1, 128, 139, 139]             256
            ReLU-192        [-1, 128, 139, 139]               0
          Conv2d-193        [-1, 128, 139, 139]         147,584
  ConvSequential-194        [-1, 128, 139, 139]               0
          Up2dBB-195        [-1, 128, 139, 139]               0
       Dropout2d-196        [-1, 128, 139, 139]               0
     BatchNorm2d-197        [-1, 256, 139, 139]             512
 ConvTranspose2d-198        [-1, 128, 281, 281]         819,328
     BatchNorm2d-199        [-1, 128, 281, 281]             256
            ReLU-200        [-1, 128, 281, 281]               0
          Conv2d-201        [-1, 128, 281, 281]         147,584
     BatchNorm2d-202        [-1, 128, 281, 281]             256
            ReLU-203        [-1, 128, 281, 281]               0
          Conv2d-204        [-1, 128, 281, 281]         147,584
     BatchNorm2d-205        [-1, 128, 281, 281]             256
            ReLU-206        [-1, 128, 281, 281]               0
          Conv2d-207        [-1, 128, 281, 281]         147,584
     BatchNorm2d-208        [-1, 128, 281, 281]             256
            ReLU-209        [-1, 128, 281, 281]               0
          Conv2d-210        [-1, 128, 281, 281]         147,584
     BatchNorm2d-211        [-1, 128, 281, 281]             256
            ReLU-212        [-1, 128, 281, 281]               0
          Conv2d-213        [-1, 128, 281, 281]         147,584
  ConvSequential-214        [-1, 128, 281, 281]               0
          Up2dBB-215        [-1, 128, 281, 281]               0
       Dropout2d-216        [-1, 128, 281, 281]               0
     BatchNorm2d-217        [-1, 256, 281, 281]             512
            ReLU-218        [-1, 256, 281, 281]               0
          Conv2d-219        [-1, 129, 281, 281]         297,345
     BatchNorm2d-220        [-1, 129, 281, 281]             258
            ReLU-221        [-1, 129, 281, 281]               0
          Conv2d-222          [-1, 2, 281, 281]           2,324
ConvSeqDecreaseChannels-223          [-1, 2, 281, 281]               0
================================================================
Total params: 404,838,041
Trainable params: 404,838,041
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.30
Forward/backward pass size (MB): 4583.59
Params size (MB): 1544.33
Estimated Total Size (MB): 6128.22
----------------------------------------------------------------
===================End of Net Architecture =====================

Info: program will use 4 GPUs.
Loss Functions List: FocalCELoss with weight of 1; 	BoundaryLoss with weight of 0; 
Hints: Test Dice_0 is the dice coeff for all non-zero labels
Hints: Test Dice_1 is for primary cancer(green), test Dice_2 is for metastasis(yellow), and test Dice_3 is for invaded lymph node(brown).
Hints: Test TPR_0 is the TPR for all non-zero labels
Hints: Test TPR_1 is for primary cancer(green), TPR_2 is for metastasis(yellow), and TPR_3 is for invaded lymph node(brown).

Epoch 	 TrainingLoss 	 TestLoss 	Dice_0	Dice_1	TPR_0	TPR_1
0 	 0.0928 	 0.0726 	0.308	0.308	0.984	0.984
1 	 0.0730 	 0.1486 	0.328	0.328	0.699	0.699
2 	 0.0638 	 1.1393 	0.305	0.305	0.402	0.402
3 	 0.0565 	 0.1489 	0.447	0.447	0.921	0.921
4 	 0.0543 	 0.0838 	0.379	0.379	0.954	0.954
5 	 0.0551 	 0.1382 	0.392	0.392	0.930	0.930
6 	 0.0498 	 0.0434 	0.504	0.504	0.971	0.971
7 	 0.0512 	 0.1024 	0.481	0.481	0.923	0.923
8 	 0.0445 	 0.0504 	0.492	0.492	0.969	0.969
9 	 0.0443 	 0.0464 	0.500	0.500	0.955	0.955
10 	 0.0467 	 0.0431 	0.465	0.465	0.987	0.987
11 	 0.0436 	 0.0627 	0.548	0.548	0.956	0.956
12 	 0.0468 	 0.0478 	0.454	0.454	0.960	0.960
13 	 0.0442 	 0.0564 	0.426	0.426	0.988	0.988
14 	 0.0414 	 0.0479 	0.550	0.550	0.970	0.970
15 	 0.0391 	 0.0399 	0.563	0.563	0.966	0.966
16 	 0.0418 	 0.0799 	0.494	0.494	0.956	0.956
17 	 0.0389 	 0.2587 	0.569	0.569	0.646	0.646
18 	 0.0410 	 0.0362 	0.487	0.487	0.995	0.995
19 	 0.0374 	 0.0566 	0.498	0.498	0.972	0.972
20 	 0.0403 	 0.0438 	0.505	0.505	0.975	0.975
21 	 0.0433 	 0.0671 	0.452	0.452	0.970	0.970
22 	 0.0420 	 0.0567 	0.565	0.565	0.936	0.936
23 	 0.0423 	 0.0796 	0.466	0.466	0.957	0.957
24 	 0.0442 	 0.0726 	0.586	0.586	0.937	0.937
25 	 0.0375 	 0.0351 	0.542	0.542	0.975	0.975
26 	 0.0333 	 0.0644 	0.617	0.617	0.914	0.914
27 	 0.0364 	 0.0517 	0.485	0.485	0.976	0.976
28 	 0.0376 	 0.0400 	0.494	0.494	0.983	0.983
29 	 0.0376 	 0.0514 	0.534	0.534	0.966	0.966
30 	 0.0349 	 0.0408 	0.594	0.594	0.982	0.982
31 	 0.0364 	 0.0428 	0.539	0.539	0.972	0.972
32 	 0.0318 	 0.0662 	0.529	0.529	0.945	0.945
33 	 0.0323 	 0.0373 	0.570	0.570	0.975	0.975
34 	 0.0321 	 0.0641 	0.583	0.583	0.944	0.944
35 	 0.0332 	 0.1084 	0.600	0.600	0.916	0.916
36 	 0.0285 	 0.0366 	0.592	0.592	0.970	0.970
37 	 0.0298 	 0.0557 	0.599	0.599	0.962	0.962
38 	 0.0334 	 0.0960 	0.614	0.614	0.943	0.943
39 	 0.0342 	 0.0527 	0.573	0.573	0.968	0.968
40 	 0.0344 	 0.0369 	0.523	0.523	0.978	0.978
41 	 0.0392 	 0.0360 	0.532	0.532	0.986	0.986
42 	 0.0350 	 0.0376 	0.618	0.618	0.967	0.967
43 	 0.0297 	 0.0413 	0.579	0.579	0.978	0.978
44 	 0.0325 	 0.0747 	0.571	0.571	0.973	0.973
45 	 0.0320 	 0.0548 	0.649	0.649	0.978	0.978
46 	 0.0303 	 0.0459 	0.515	0.515	0.993	0.993
47 	 0.0309 	 0.1798 	0.560	0.560	0.905	0.905
48 	 0.0314 	 0.1652 	0.597	0.597	0.955	0.955
49 	 0.0376 	 0.1528 	0.543	0.543	0.942	0.942
50 	 0.0417 	 0.0524 	0.572	0.572	0.954	0.954
51 	 0.0373 	 0.1386 	0.600	0.600	0.945	0.945
52 	 0.0372 	 0.0550 	0.622	0.622	0.970	0.970
53 	 0.0364 	 0.0985 	0.565	0.565	0.965	0.965
54 	 0.0307 	 0.0558 	0.547	0.547	0.972	0.972
55 	 0.0303 	 0.0363 	0.602	0.602	0.972	0.972
56 	 0.0292 	 0.0445 	0.598	0.598	0.961	0.961
57 	 0.0302 	 0.0748 	0.588	0.588	0.953	0.953
