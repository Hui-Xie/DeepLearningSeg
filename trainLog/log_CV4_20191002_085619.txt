=============training from sratch============
Program ID: 21390

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy', '4', '3,2,1']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      
         

Discarded changes:                  

Experiment setting:
Input CT data: 51*171*171 ROI around primary cancer

Loss Function:  SoftMax

Data:   total 36 patients with 50-80% label, 6-fold cross validation, test 6, validation 6, and training 24.  
    script: python3.7 statisticsLabelFiles.py 
    Total 36 in /home/hxie1/data/OvarianCancerCT/primaryROI/labels_npy
    0 has 48159408 elements, with a rate of  0.8970491562903105 
    1 has 5527068 elements, with a rate of  0.10295084370968957

Training strategy: 

          

Program starting Time: 2019-10-02 08:56:19.872734
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191002_085619

Info: this is the 4th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 6

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191002_085619.
6-fold cross validation: the 4th fold is for test, the 5th fold is for validation, remaining folds are for training.

training dataset: total 24 image files.

validation dataset: total 5 image files.

test dataset: total 6 image files.
Total 24 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy
0 has 20955544 elements, with a rate of  0.771160870395934 
1 has 6218480 elements, with a rate of  0.228839129604066 
loss weight = tensor([1.0000, 3.3699])
Network has total 73,047,746 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		48.5390		0.08223		5879.6895		0.00000		4235.0723		0.00000
5	1.0000e-02		12.3542		0.39396		11.1831		0.44049		10.4717		0.46892
10	1.0000e-02		9.1407		0.56068		10.8982		0.39519		9.6940		0.50300
15	1.0000e-02		5.8202		0.67811		12.0768		0.49331		10.9047		0.55863
20	1.0000e-02		5.7209		0.67145		9.6970		0.55713		9.0564		0.61352
25	1.0000e-02		5.3166		0.68147		5.5715		0.63785		5.7479		0.68139
30	1.0000e-02		4.9163		0.70980		6.4339		0.62601		6.2520		0.68392
35	1.0000e-02		5.7178		0.67446		5.3997		0.67396		6.5342		0.66381
40	1.0000e-02		5.1532		0.72960		5.8991		0.63913		6.1006		0.69242
45	1.0000e-02		3.7153		0.75624		11.5410		0.57796		8.0512		0.64001
50	1.0000e-02		3.6849		0.74919		4.5476		0.69911		9.9570		0.54847
55	1.0000e-02		4.2416		0.74578		4.5414		0.68044		6.2721		0.65824
60	1.0000e-02		3.4687		0.78568		4.8823		0.68766		6.5876		0.66764
65	1.0000e-02		3.4008		0.76547		3.9566		0.71228		5.7528		0.69203
70	1.0000e-02		3.6827		0.74978		7.5130		0.63265		7.9684		0.62839
75	1.0000e-02		3.5734		0.78167		5.0811		0.67334		7.0453		0.66918
80	1.0000e-02		3.5623		0.78165		5.2026		0.68776		7.2623		0.64871
85	1.0000e-02		3.0371		0.77621		4.4082		0.69632		5.2046		0.70726
90	1.0000e-02		2.8531		0.78764		4.4734		0.68756		5.6945		0.69718
95	1.0000e-02		3.4338		0.79223		4.5993		0.69361		8.3304		0.58341
100	1.0000e-02		2.9298		0.81027		5.4345		0.70387		7.1072		0.67421
105	1.0000e-02		3.0994		0.77820		4.3805		0.71292		7.6333		0.66551
110	1.0000e-02		2.9853		0.80294		4.4170		0.71115		7.9696		0.61018
115	1.0000e-02		2.9331		0.78606		4.0758		0.70793		7.1133		0.63887
120	1.0000e-03		3.4252		0.80036		5.4202		0.66662		5.6556		0.69667
125	1.0000e-03		2.7410		0.80874		4.5593		0.70426		5.6450		0.69465
130	1.0000e-03		2.6948		0.80805		4.6168		0.70397		5.4952		0.70047
135	1.0000e-03		2.4152		0.80280		4.4356		0.71106		5.3607		0.70747
140	1.0000e-03		2.2237		0.82308		4.7064		0.71107		5.5312		0.70451
145	1.0000e-03		2.6201		0.80876		4.6844		0.71282		5.6246		0.69991
150	1.0000e-03		2.7058		0.81485		3.8415		0.73447		5.6570		0.70025
155	1.0000e-03		2.4296		0.82694		4.3294		0.72121		5.6395		0.70305
160	1.0000e-03		2.5702		0.82323		4.2164		0.73352		5.5294		0.71636
165	1.0000e-03		2.9370		0.81069		4.3143		0.72077		6.1786		0.69013
170	1.0000e-03		2.9265		0.82339		4.7260		0.71285		6.2036		0.69051
175	1.0000e-03		3.0271		0.81042		4.9295		0.70785		5.6230		0.70680
180	1.0000e-03		2.5611		0.81456		4.3629		0.72525		5.5467		0.71426
185	1.0000e-03		2.3167		0.80680		4.0379		0.72770		5.1371		0.71683
190	1.0000e-03		2.5765		0.82256		4.3668		0.71065		5.7410		0.71177
195	1.0000e-03		2.1352		0.83637		4.4951		0.72284		5.7134		0.71256
200	1.0000e-03		3.0786		0.82374		4.5398		0.72235		5.8927		0.70173
205	1.0000e-04		2.5020		0.81906		4.6830		0.70982		5.8678		0.69852
210	1.0000e-04		1.9659		0.83786		4.5111		0.71661		5.7266		0.70500
215	1.0000e-04		2.2493		0.82206		4.4823		0.71761		5.8867		0.70558
220	1.0000e-04		2.1819		0.82133		4.5178		0.71062		5.4721		0.71069
225	1.0000e-04		2.0245		0.83325		4.4145		0.71727		6.1743		0.69717
230	1.0000e-04		1.9588		0.82375		4.7219		0.70151		5.8703		0.70603
235	1.0000e-04		2.1061		0.85166		4.6055		0.71879		5.9574		0.70847
240	1.0000e-04		2.3476		0.83676		4.5075		0.71860		5.8379		0.70669
245	1.0000e-04		1.8859		0.84465		4.2963		0.72203		5.9602		0.70526
250	1.0000e-04		2.0138		0.83631		4.2362		0.72360		5.8335		0.70937
255	1.0000e-04		1.8263		0.82788		4.4073		0.71638		5.9347		0.70667
260	1.0000e-05		2.2778		0.83544		4.5607		0.71284		6.1377		0.70364
265	1.0000e-05		2.0963		0.82371		4.5126		0.71462		5.7767		0.70931
270	1.0000e-05		2.2341		0.83005		4.6539		0.71431		6.1481		0.70403
275	1.0000e-05		1.8173		0.83403		4.5921		0.71162		6.0071		0.70717
280	1.0000e-05		1.9847		0.83975		4.3841		0.71995		6.7322		0.68695
285	1.0000e-05		1.9163		0.83824		4.2957		0.72295		6.5930		0.68350
290	1.0000e-05		1.9085		0.83087		4.4964		0.71637		6.0436		0.70557
295	1.0000e-05		2.1764		0.83530		4.3301		0.72690		5.8971		0.70790
300	1.0000e-05		1.9082		0.83311		4.3373		0.72920		5.7732		0.70925
305	1.0000e-05		2.0852		0.82915		4.3477		0.72046		5.9104		0.70905
310	1.0000e-05		1.9936		0.82885		4.3369		0.72174		5.8143		0.71008
315	1.0000e-06		2.1197		0.82735		4.8126		0.70495		5.8615		0.70732
320	1.0000e-06		2.1520		0.84184		4.5514		0.71298		5.6199		0.71080
325	1.0000e-06		1.9765		0.82846		4.2855		0.72594		5.8494		0.70788
330	1.0000e-06		2.6319		0.82146		4.3646		0.72378		5.9275		0.70340
335	1.0000e-06		2.1606		0.83984		4.5465		0.71151		5.3807		0.71561
340	1.0000e-06		2.0563		0.82868		4.4331		0.71927		5.9437		0.70802
345	1.0000e-06		1.9122		0.82816		4.4036		0.71797		6.3081		0.69811
350	1.0000e-06		1.9047		0.83404		4.5402		0.71176		5.8198		0.70639
355	1.0000e-06		1.9015		0.83738		4.2231		0.72378		6.4545		0.69377
360	1.0000e-06		1.8536		0.83469		4.5164		0.71297		5.7340		0.70884
365	1.0000e-06		1.9229		0.83088		4.5744		0.71120		5.8227		0.71195
370	1.0000e-07		1.8109		0.83839		4.5809		0.71287		5.5545		0.71464
375	1.0000e-07		1.9286		0.83361		4.3513		0.72090		6.0611		0.70269
380	1.0000e-07		1.8288		0.82365		4.4874		0.71704		5.9163		0.70257
385	1.0000e-07		2.0995		0.81941		4.3698		0.72029		5.9703		0.70251
390	1.0000e-07		2.3357		0.83804		4.3070		0.72314		5.7853		0.70427
395	1.0000e-07		1.7488		0.83332		4.5171		0.71559		5.8224		0.71328
400	1.0000e-07		2.0068		0.83956		4.2540		0.72469		6.0935		0.69865
405	1.0000e-07		2.1452		0.82185		4.3305		0.72032		6.2366		0.69863
410	1.0000e-07		1.8584		0.83226		4.5467		0.71389		5.5721		0.71660
415	1.0000e-07		1.6928		0.82896		4.3623		0.72362		5.7254		0.71381
420	1.0000e-07		1.7997		0.83473		4.2283		0.73188		5.9393		0.70898
425	1.0000e-08		2.2379		0.82822		4.6080		0.71096		5.6829		0.70859
430	1.0000e-08		2.0027		0.84035		4.3326		0.72280		5.9234		0.70229
435	1.0000e-08		2.4737		0.82726		4.4022		0.71917		5.9294		0.70440
440	1.0000e-08		1.8940		0.83720		4.2662		0.72527		6.0097		0.70030
445	1.0000e-08		1.8879		0.84756		4.5233		0.71532		5.8807		0.70882
450	1.0000e-08		2.0278		0.83398		4.4102		0.71735		5.8676		0.70558
455	1.0000e-08		1.9817		0.83750		4.3950		0.72026		5.8604		0.70876
460	1.0000e-08		1.8909		0.84319		4.6807		0.71160		5.7236		0.71375
465	1.0000e-08		2.0940		0.85381		4.3152		0.72793		5.8519		0.70827
470	1.0000e-08		2.0397		0.84005		4.5380		0.71525		5.8844		0.70794
475	1.0000e-08		2.2844		0.83347		4.4752		0.72001		5.7329		0.71042
480	1.0000e-08		2.4188		0.83888		4.4140		0.72509		6.1310		0.70425
485	1.0000e-08		2.0467		0.84475		4.5138		0.71846		5.9958		0.70817
