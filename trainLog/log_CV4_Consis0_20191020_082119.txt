=============training from sratch============
Program ID: 31510

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy', '4', '2', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
       
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-20 08:21:19.245267
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191020_082119

Info: this is the 4th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191020_082119.
6-fold cross validation: the 4th fold is for test, the 5th fold is for validation, remaining folds are for training.

training dataset: total 24 image files.

validation dataset: total 5 image files.

test dataset: total 6 image files.
Total 24 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy
0 has 19588550 elements, with a rate of  0.7708329988481116 
1 has 5823634 elements, with a rate of  0.2291670011518884 
loss weight = tensor([1.0000, 3.3636])
Network has total 254,664,050 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		72.6118		0.35124		176.4202		0.26104		91.4146		0.26994
5	1.0000e-02		4.0219		0.62615		63.5120		0.41047		9.4831		0.58639
10	1.0000e-02		2.1061		0.65084		22.8616		0.52578		2.2702		0.67779
15	1.0000e-02		2.3867		0.63086		33.3216		0.49965		3.0923		0.63537
20	1.0000e-02		1.8918		0.67303		14.4851		0.53457		1.9931		0.64971
25	1.0000e-02		1.9436		0.65786		11.5555		0.54516		5.1580		0.58481
30	1.0000e-02		2.0608		0.64740		1.8211		0.63750		1.7755		0.67631
35	1.0000e-02		2.2411		0.63535		7.6388		0.52616		2.4570		0.65942
40	1.0000e-02		1.7998		0.69535		64.5110		0.46479		18.3273		0.53913
45	1.0000e-02		3.2251		0.64443		64.0707		0.39255		11.3661		0.56301
50	1.0000e-02		1.6453		0.70307		2.4258		0.59386		2.2230		0.62655
55	1.0000e-02		1.6480		0.71324		12.7752		0.47115		8.6393		0.53253
60	1.0000e-02		1.9180		0.66207		65.5600		0.38029		86.3445		0.28752
65	1.0000e-02		1.6322		0.70653		13.8340		0.47176		14.4041		0.57787
70	1.0000e-02		1.5176		0.73294		9.7072		0.42161		9.2383		0.58460
75	1.0000e-02		1.4319		0.74291		2.3830		0.61958		2.0435		0.68829
80	1.0000e-02		1.4745		0.74577		28.8543		0.40557		22.7133		0.60764
85	1.0000e-03		2.0988		0.67845		3.3244		0.61131		4.6183		0.51665
90	1.0000e-03		1.6488		0.71410		2.7809		0.61082		3.2460		0.60742
95	1.0000e-03		1.6739		0.72568		7.4564		0.55273		5.0445		0.60159
100	1.0000e-03		1.3832		0.73246		2.6621		0.58576		3.1127		0.61994
105	1.0000e-03		1.5380		0.72896		2.4410		0.62707		3.2733		0.58407
110	1.0000e-03		3.3609		0.69568		12.0242		0.43483		7.1416		0.56596
115	1.0000e-03		1.4745		0.72496		2.9332		0.52431		3.3487		0.62946
120	1.0000e-03		1.1698		0.77128		2.6584		0.59397		3.5780		0.59808
125	1.0000e-03		1.1510		0.76971		2.5314		0.62111		3.2942		0.65644
130	1.0000e-03		1.0495		0.78485		3.2782		0.59583		4.5224		0.54986
135	1.0000e-03		1.3820		0.78430		3.1683		0.50437		3.4916		0.59982
140	1.0000e-04		1.1000		0.78050		3.5983		0.54482		4.2895		0.54823
145	1.0000e-04		1.1267		0.79200		3.4239		0.52969		2.9146		0.65186
150	1.0000e-04		1.1998		0.77018		3.3970		0.57806		4.8787		0.54598
155	1.0000e-04		1.2798		0.78123		3.3684		0.58653		4.2327		0.60181
160	1.0000e-04		0.9652		0.77411		4.3055		0.45057		4.3856		0.55603
165	1.0000e-04		0.9070		0.79658		3.4194		0.51714		2.8914		0.65748
170	1.0000e-04		1.0657		0.78959		3.1489		0.53837		3.3122		0.59116
175	1.0000e-04		1.0171		0.79898		3.1670		0.52861		2.9540		0.65689
180	1.0000e-04		1.2465		0.78239		3.4647		0.55782		4.1528		0.57283
185	1.0000e-04		1.0730		0.78555		2.7594		0.61943		3.6580		0.64512
190	1.0000e-04		1.0982		0.77816		2.9924		0.58877		3.4183		0.64759
195	1.0000e-05		0.9434		0.78905		3.2599		0.55753		4.0156		0.60552
200	1.0000e-05		0.9659		0.80884		2.9969		0.55815		3.0713		0.68124
205	1.0000e-05		0.8865		0.78303		3.6281		0.39603		2.8101		0.66552
210	1.0000e-05		0.9357		0.79733		3.9390		0.48086		3.9381		0.58256
215	1.0000e-05		0.9581		0.79955		2.5480		0.62178		3.5684		0.63169
220	1.0000e-05		1.0780		0.79899		3.2117		0.48807		3.0708		0.66273
225	1.0000e-05		1.1587		0.79655		3.2747		0.49150		3.3076		0.61786
230	1.0000e-05		0.9694		0.79832		3.1955		0.55083		3.2145		0.65798
235	1.0000e-05		0.9821		0.79938		3.9086		0.39293		2.5041		0.68218
240	1.0000e-05		1.0570		0.80334		2.8982		0.57347		3.1430		0.64396
245	1.0000e-05		1.1452		0.77870		4.0186		0.42097		3.3505		0.63430
250	1.0000e-06		0.9366		0.80753		3.3066		0.52227		2.8009		0.67387
255	1.0000e-06		1.0346		0.79660		3.2283		0.51510		3.4965		0.60255
260	1.0000e-06		0.9923		0.78357		3.4796		0.43539		3.9854		0.53340
265	1.0000e-06		1.1321		0.78603		3.3485		0.46219		3.5104		0.59060
270	1.0000e-06		1.2101		0.78811		2.9407		0.56491		3.0306		0.66532
275	1.0000e-06		1.1756		0.79865		3.3366		0.50317		3.3446		0.64050
280	1.0000e-06		0.9897		0.79808		3.5480		0.42564		3.1381		0.61987
285	1.0000e-06		0.9363		0.80345		3.6518		0.47633		3.9953		0.55059
290	1.0000e-06		1.0329		0.78473		3.3028		0.55486		3.9987		0.60367
295	1.0000e-06		0.8526		0.79779		3.8409		0.42702		3.6720		0.60508
300	1.0000e-06		1.1477		0.78266		3.1346		0.59546		4.8123		0.56407
305	1.0000e-07		0.9646		0.78560		3.7425		0.42405		3.5487		0.61727
310	1.0000e-07		1.0724		0.78897		3.6669		0.44101		3.9385		0.57353
315	1.0000e-07		1.0532		0.79511		2.4504		0.63474		3.6708		0.63138
320	1.0000e-07		0.9508		0.78213		3.4871		0.54947		3.9593		0.61186
325	1.0000e-07		1.1200		0.78740		3.6503		0.44016		2.9550		0.64903
330	1.0000e-07		0.8608		0.78156		3.6430		0.50240		3.9257		0.58351
335	1.0000e-07		0.9542		0.80034		3.8024		0.40047		2.4226		0.69695
340	1.0000e-07		1.0126		0.78482		3.6575		0.46586		3.2872		0.63763
345	1.0000e-07		1.0130		0.80014		3.1939		0.53052		3.7716		0.60593
350	1.0000e-07		1.0002		0.78960		4.0656		0.52271		4.4226		0.55239
355	1.0000e-07		1.0176		0.78954		3.1337		0.53717		3.7004		0.59005
360	1.0000e-08		1.0266		0.78492		3.9314		0.41794		3.4556		0.62769
365	1.0000e-08		0.9845		0.79282		3.7436		0.45128		3.1288		0.64165
370	1.0000e-08		0.9637		0.79156		3.2574		0.55029		3.6656		0.63737
375	1.0000e-08		0.8983		0.79582		3.6521		0.44512		3.9155		0.56635
380	1.0000e-08		0.9594		0.79216		4.1734		0.36890		3.5956		0.52475
385	1.0000e-08		0.8969		0.79102		3.5248		0.46497		3.5932		0.57378
390	1.0000e-08		1.0089		0.80982		2.7328		0.61237		3.6058		0.64513
395	1.0000e-08		0.9720		0.80532		3.5138		0.45224		2.7825		0.69168
400	1.0000e-08		0.9172		0.79086		3.7008		0.41707		3.0434		0.61893
405	1.0000e-08		0.9596		0.79934		2.5477		0.62351		3.8847		0.62239
410	1.0000e-08		0.9558		0.79644		3.7295		0.42453		2.4873		0.68660
415	1.0000e-08		1.2738		0.79447		3.5450		0.44312		3.3419		0.60558
420	1.0000e-08		1.2116		0.77543		3.6677		0.51081		3.9340		0.61211
425	1.0000e-08		0.9822		0.80372		3.2023		0.50452		3.3821		0.62393
430	1.0000e-08		1.3735		0.77863		2.8245		0.60577		4.1037		0.58902
435	1.0000e-08		0.8714		0.80211		3.1980		0.56451		4.2612		0.56257
440	1.0000e-08		0.9378		0.80540		3.0747		0.51948		3.4480		0.61146
445	1.0000e-08		0.8470		0.79891		3.6343		0.42471		3.7853		0.55076
450	1.0000e-08		0.9986		0.79813		3.4306		0.44951		3.4967		0.58545
455	1.0000e-08		1.3763		0.79345		3.2447		0.57457		4.1927		0.57283
460	1.0000e-08		0.9518		0.78524		4.2986		0.36172		3.3259		0.52020
465	1.0000e-08		1.0536		0.79495		3.4467		0.56642		4.6704		0.54372
470	1.0000e-08		0.9157		0.78605		3.3123		0.45924		2.7464		0.66886
475	1.0000e-08		1.0012		0.79224		3.1456		0.57229		3.8470		0.59906
480	1.0000e-08		0.9603		0.78879		3.3451		0.48410		3.1501		0.65591
485	1.0000e-08		0.9211		0.77819		2.9207		0.59667		4.3400		0.56078
490	1.0000e-08		0.9529		0.80154		2.8205		0.54642		2.8611		0.66975
495	1.0000e-08		0.9769		0.80635		2.7696		0.60265		3.2949		0.65412
500	1.0000e-08		0.9979		0.80009		3.0545		0.54401		3.9760		0.56525
505	1.0000e-08		1.1554		0.79516		2.9808		0.57806		3.7765		0.60662
510	1.0000e-08		0.9088		0.79540		4.7643		0.50615		4.5264		0.55805
515	1.0000e-08		0.8704		0.79337		2.9022		0.57937		3.7510		0.62163
520	1.0000e-08		1.7221		0.78729		2.6996		0.61523		3.9049		0.62907
525	1.0000e-08		1.0183		0.79968		2.9716		0.59639		3.9441		0.57991
530	1.0000e-08		1.0734		0.77764		3.6218		0.52359		4.2020		0.55163
535	1.0000e-08		1.0314		0.79731		3.1594		0.52797		3.1670		0.66665
540	1.0000e-08		0.9052		0.78917		3.6554		0.42904		3.2608		0.63455
545	1.0000e-08		0.9377		0.79145		3.5898		0.42143		2.6364		0.66987
550	1.0000e-08		0.9555		0.79428		3.7606		0.43348		2.5752		0.67112
555	1.0000e-08		0.9438		0.79120		3.8432		0.42006		3.0043		0.65720
560	1.0000e-08		1.3164		0.79004		3.4352		0.46580		3.9355		0.55284
565	1.0000e-08		0.9686		0.79757		3.3403		0.52121		2.7800		0.66937
570	1.0000e-08		0.9762		0.80060		2.9428		0.56629		3.1349		0.63642
575	1.0000e-08		0.9211		0.78728		3.9604		0.53306		4.2057		0.59831
580	1.0000e-08		1.0443		0.79714		2.6945		0.62059		3.2205		0.63900
585	1.0000e-08		1.0833		0.78472		3.2352		0.56285		4.2786		0.56108
590	1.0000e-08		0.9264		0.77739		3.8490		0.51411		4.1703		0.58322
595	1.0000e-08		0.9188		0.80298		3.4994		0.43313		2.5914		0.68824
600	1.0000e-08		0.9866		0.79592		3.1673		0.52876		3.6122		0.61301
605	1.0000e-08		1.0024		0.80448		2.9575		0.55246		3.3825		0.64751
610	1.0000e-08		0.9146		0.79443		3.4469		0.49476		3.6826		0.58182
615	1.0000e-08		1.3048		0.79955		2.7418		0.59038		2.8893		0.67461
