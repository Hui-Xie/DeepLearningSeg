=============training from sratch============
Program ID: 30644

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt', '1', '/home/hxie1/data/OvarianCancerCT/pixelSize223/numpy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '0', '2,3']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
            8    at July 30th 03:00, 2019:
                    add learning rate print;
                    use convStride =2;
                    add filter number by 2 times along deeper layers.
            9    at July 30th, 10:13, 2019:
                    add MaxPool2d in stage1;
                    add final filters to 2048.
            10   at July 30th, 15:23, 2019
                    final conv layer filter number: 1024
            11   at Aug 10th, 2019:
                    A. Add new patient data; and exclude non-standard patient data;
                    B. test the k-th fold,  validation on the (k+1)th fold;
                    C. new inputsize: 231*251*251 with pixels size 3*2*2 mm
                    D. window level shresthold [0,300]
                    E. put data padding in to converting from nrrd to numpy;
                    F. Add STN network as first laye of network
                    G. change input data into gaussian normalization in slice by slice; (x-mean)/std;
            12   Aug 12th, 2019
                    A. fix the bug that SITk reads int imape, and normalization into interger error;
                    B  test without SPN.           
            13   Aug 13th, 2019
                    A change input data into Gausssian distribution with non-zero mean,
                      it will make the padding zero do not occupy meaning of gaussian distribution.        
                    B add filters in the ResNeXt network to 128 at first stage, and reduce batchSize to 3. 
                    C Add STN at the beginning of the ResNeXt network
            14   Aug 14th, 2019
                    A. Fix the Bug the STN convert all image into 0 problem;
                    B. change DataTransform: change shear into [-30, 30], and add scale[0.6,1.2]
                    C  put STN at teh begginning of the network;
                    D  change optimizer from SGD to Adam; and change learning rate decay with gamma=0.5 per 20 steps.                                             
            15   Aug 16th, 2019
                    A The affine matrix in the STN is divided by its spectral norm;
                    B All conv layer add spectralNorm, while all FC layer do not add spectral Norm;
                    C reduce initial LR at 0.0001, and decay step to 30.
            16   Aug 17th, 2019
                    A  Cancel STN;
                    B  Add LeakyReLU;
                    C  before final FC layer, add ReLU and LocalResponseNorm   
            17   Aug 17th, 2019:10:44am
                    A enable STN;
                    B STN support LeakyReLU and localResponseNorm  
            18   Aug 17th, 2019 14:38 pm
                    A  del STN at beginining;
                    B  put STN at final 2 layer before FC. 
            19  Aug 17th 18:09 2019
                    A add surgical result file support;                    
            20  Aug 18th 08:00 am 2019
                    A  change all bias in Conv2d, and Linear into True.   
            21  Aug 19th 10:33am 2019
                    A initalize the bias in the final Linear layer as 0.3; (1-2*0.65 = -0.3)                          
            
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   total 220 patients, 5-fold cross validation, test 45, validation 45, and training 130.  

Training strategy: 

          

Program starting Time: 2019-08-19 10:37:28.294488
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt/20190819_103728

Info: this is the 0th fold leave for test in the 5-fold cross-validation.

Infor: In all data of 220 files, label 0 has 77 files,
	  and label 1 has 143 files, where positive response rate = 0.65 in full data
5-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.
training dataset:	 total 130 files, where 1 has 85.0 with rate of 0.6538461538461539
validation dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
test dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
Info: batchSize = 6

=== Network trains from scratch ====
Network has total 331,776,449 parameters.
This predict optimal response.
Info: program will use GPU [2, 3] from all 4 GPUs.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	LearningRate		TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	1.0000e-04		5.1760	0.5769	0.5882	0.5556		7.0315	0.6444	1.0000	0.0000		7.0306	0.6444	1.0000	0.0000
5	1.0000e-04		1.4916	0.4692	0.4353	0.5333		1.3864	0.5556	0.7931	0.1250		1.5007	0.6000	0.7931	0.2500
10	1.0000e-04		0.6179	0.5154	0.5412	0.4667		0.5804	0.4889	0.6207	0.2500		0.5389	0.6667	0.8966	0.2500
15	1.0000e-04		0.6953	0.4923	0.5647	0.3556		0.5263	0.4222	0.3103	0.6250		0.5405	0.4444	0.3103	0.6875
20	5.0000e-05		0.5702	0.3923	0.3647	0.4444		0.5294	0.5333	0.5862	0.4375		0.5645	0.5111	0.6207	0.3125
25	5.0000e-05		0.5137	0.5231	0.5765	0.4222		0.9246	0.4000	0.1034	0.9375		0.8962	0.4222	0.1379	0.9375
30	5.0000e-05		0.5211	0.5000	0.5176	0.4667		0.5735	0.4444	0.4138	0.5000		0.5755	0.6000	0.5517	0.6875
35	5.0000e-05		0.5168	0.6077	0.6353	0.5556		0.5911	0.5333	0.7241	0.1875		0.5522	0.6667	0.9310	0.1875
40	2.5000e-05		0.4978	0.5615	0.5647	0.5556		0.6374	0.4444	0.3793	0.5625		0.5358	0.5556	0.5862	0.5000
45	2.5000e-05		0.4796	0.5692	0.5882	0.5333		0.5781	0.3333	0.2414	0.5000		0.5262	0.4667	0.4138	0.5625
50	2.5000e-05		0.4803	0.6077	0.6706	0.4889		0.6188	0.3778	0.1724	0.7500		0.6098	0.4000	0.1724	0.8125
55	2.5000e-05		0.4947	0.5769	0.6824	0.3778		0.5093	0.6000	0.6552	0.5000		0.5164	0.6000	0.6897	0.4375
60	1.2500e-05		0.4667	0.6538	0.7765	0.4222		0.5909	0.5111	0.5172	0.5000		0.5402	0.5556	0.5862	0.5000
65	1.2500e-05		0.4468	0.6462	0.7294	0.4889		0.5662	0.5333	0.4828	0.6250		0.5259	0.5556	0.5517	0.5625
70	1.2500e-05		0.4555	0.5692	0.5059	0.6889		0.5548	0.5333	0.5172	0.5625		0.5752	0.4000	0.3103	0.5625
75	1.2500e-05		0.4649	0.6077	0.5647	0.6889		0.6420	0.4000	0.1724	0.8125		0.7169	0.4222	0.2069	0.8125
80	6.2500e-06		0.4355	0.6615	0.7529	0.4889		0.5722	0.4444	0.3103	0.6875		0.6542	0.4222	0.2414	0.7500
85	6.2500e-06		0.4303	0.6615	0.6706	0.6444		0.5511	0.4000	0.3448	0.5000		0.6179	0.4444	0.3103	0.6875
90	6.2500e-06		0.4509	0.5769	0.5765	0.5778		0.5507	0.5333	0.5517	0.5000		0.5598	0.5333	0.4828	0.6250
95	6.2500e-06		0.4490	0.6615	0.7176	0.5556		0.5505	0.4889	0.5172	0.4375		0.5723	0.4889	0.4138	0.6250
100	3.1250e-06		0.4103	0.7154	0.7294	0.6889		0.6184	0.5111	0.5172	0.5000		0.6194	0.5111	0.4483	0.6250
105	3.1250e-06		0.4254	0.6615	0.6118	0.7556		0.5676	0.5333	0.5862	0.4375		0.6162	0.5111	0.4138	0.6875
110	3.1250e-06		0.3884	0.7308	0.8000	0.6000		0.6260	0.5333	0.5862	0.4375		0.6136	0.5778	0.6207	0.5000
115	3.1250e-06		0.4477	0.6308	0.6000	0.6889		0.6405	0.5333	0.5517	0.5000		0.6484	0.5778	0.5862	0.5625
120	1.5625e-06		0.3862	0.6923	0.6588	0.7556		0.6434	0.5333	0.5862	0.4375		0.6435	0.6000	0.6207	0.5625
125	1.5625e-06		0.4364	0.7000	0.7059	0.6889		0.6181	0.4444	0.3793	0.5625		0.6820	0.4889	0.3793	0.6875
130	1.5625e-06		0.4199	0.6538	0.6824	0.6000		0.6171	0.4889	0.4828	0.5000		0.6507	0.5333	0.4828	0.6250
135	1.5625e-06		0.3872	0.7385	0.7765	0.6667		0.6061	0.5111	0.5517	0.4375		0.7335	0.4889	0.4828	0.5000
140	7.8125e-07		0.3596	0.7692	0.7882	0.7333		0.6510	0.4889	0.5172	0.4375		0.6744	0.5556	0.5517	0.5625
145	7.8125e-07		0.3884	0.7154	0.7765	0.6000		0.6733	0.5111	0.5172	0.5000		0.7440	0.5556	0.4828	0.6875
150	7.8125e-07		0.3784	0.7308	0.7059	0.7778		0.6369	0.5778	0.6207	0.5000		0.6772	0.5778	0.5517	0.6250
155	7.8125e-07		0.3755	0.7308	0.7647	0.6667		0.6658	0.4444	0.3793	0.5625		0.7372	0.5111	0.4138	0.6875
160	3.9063e-07		0.3823	0.7231	0.7176	0.7333		0.6415	0.5333	0.5517	0.5000		0.7580	0.4889	0.4483	0.5625
165	3.9063e-07		0.4459	0.6692	0.6824	0.6444		0.6207	0.5778	0.6552	0.4375		0.6911	0.5111	0.5172	0.5000
170	3.9063e-07		0.4200	0.6538	0.6706	0.6222		0.6283	0.5556	0.5862	0.5000		0.7188	0.5111	0.4483	0.6250
175	3.9063e-07		0.3896	0.7077	0.7176	0.6889		0.6482	0.4889	0.5172	0.4375		0.7035	0.4889	0.4483	0.5625
180	1.9531e-07		0.3658	0.7846	0.7647	0.8222		0.6801	0.5778	0.6552	0.4375		0.7202	0.5333	0.5172	0.5625
185	1.9531e-07		0.3771	0.7231	0.7294	0.7111		0.6516	0.5111	0.5517	0.4375		0.6830	0.5333	0.5172	0.5625
190	1.9531e-07		0.4142	0.6462	0.6588	0.6222		0.6574	0.5111	0.5517	0.4375		0.7041	0.5111	0.4828	0.5625
195	1.9531e-07		0.4009	0.7000	0.6824	0.7333		0.6523	0.5556	0.5862	0.5000		0.7041	0.5333	0.5172	0.5625
200	9.7656e-08		0.3697	0.7538	0.7529	0.7556		0.6402	0.5111	0.5172	0.5000		0.7416	0.5333	0.4483	0.6875
205	9.7656e-08		0.3903	0.6923	0.6706	0.7333		0.6390	0.6222	0.7241	0.4375		0.6850	0.5778	0.5862	0.5625
210	9.7656e-08		0.3567	0.7923	0.8000	0.7778		0.6337	0.5556	0.6207	0.4375		0.7049	0.5111	0.4828	0.5625
215	9.7656e-08		0.3882	0.6769	0.6706	0.6889		0.6136	0.6000	0.6552	0.5000		0.6987	0.5556	0.5172	0.6250
220	4.8828e-08		0.3588	0.7385	0.7294	0.7556		0.6475	0.6222	0.7241	0.4375		0.7289	0.5111	0.4483	0.6250
225	4.8828e-08		0.4189	0.7538	0.7882	0.6889		0.6408	0.5556	0.5862	0.5000		0.7365	0.5111	0.4483	0.6250
230	4.8828e-08		0.3811	0.7538	0.7412	0.7778		0.6226	0.5111	0.5517	0.4375		0.7042	0.5111	0.4483	0.6250
235	4.8828e-08		0.4320	0.7000	0.7176	0.6667		0.6384	0.5556	0.6207	0.4375		0.7328	0.4889	0.4483	0.5625
240	2.4414e-08		0.3614	0.7308	0.7059	0.7778		0.6613	0.5111	0.4828	0.5625		0.7992	0.4444	0.3448	0.6250
245	2.4414e-08		0.3828	0.7462	0.7294	0.7778		0.6623	0.5333	0.5172	0.5625		0.8104	0.5111	0.4138	0.6875
250	2.4414e-08		0.3886	0.7385	0.7412	0.7333		0.6642	0.5333	0.5517	0.5000		0.7919	0.4444	0.3103	0.6875
255	2.4414e-08		0.3945	0.7000	0.7176	0.6667		0.6476	0.5333	0.5517	0.5000		0.7565	0.5111	0.4138	0.6875
260	1.2207e-08		0.3666	0.7769	0.7647	0.8000		0.6543	0.4222	0.4138	0.4375		0.6882	0.5111	0.4828	0.5625
265	1.2207e-08		0.3904	0.7538	0.7765	0.7111		0.6456	0.4889	0.4828	0.5000		0.7514	0.4667	0.3448	0.6875
270	1.2207e-08		0.4129	0.7308	0.7294	0.7333		0.6269	0.5333	0.5517	0.5000		0.7256	0.5333	0.4483	0.6875
275	1.2207e-08		0.3532	0.7846	0.7647	0.8222		0.6519	0.5556	0.6207	0.4375		0.7435	0.4889	0.4483	0.5625
280	6.1035e-09		0.3966	0.6923	0.7176	0.6444		0.6592	0.5333	0.5172	0.5625		0.7175	0.4667	0.3793	0.6250
285	6.1035e-09		0.4076	0.6846	0.7059	0.6444		0.6207	0.5778	0.6552	0.4375		0.7165	0.5556	0.5172	0.6250
290	6.1035e-09		0.4203	0.6769	0.6353	0.7556		0.6335	0.5111	0.5172	0.5000		0.7555	0.5111	0.4138	0.6875
295	6.1035e-09		0.4050	0.7077	0.7176	0.6889		0.6543	0.4444	0.4138	0.5000		0.7572	0.4444	0.3103	0.6875
300	3.0518e-09		0.3789	0.7231	0.7529	0.6667		0.6416	0.4889	0.5172	0.4375		0.7318	0.4889	0.3793	0.6875
305	3.0518e-09		0.4156	0.7077	0.7412	0.6444		0.6461	0.5556	0.5862	0.5000		0.7339	0.4889	0.4483	0.5625
310	3.0518e-09		0.4226	0.6462	0.6706	0.6000		0.6545	0.5778	0.6552	0.4375		0.6969	0.5778	0.5862	0.5625
315	3.0518e-09		0.4013	0.6538	0.6706	0.6222		0.6460	0.4667	0.4483	0.5000		0.7590	0.4889	0.4138	0.6250
320	1.5259e-09		0.4526	0.6846	0.7412	0.5778		0.6017	0.5556	0.5862	0.5000		0.7541	0.4889	0.3793	0.6875
325	1.5259e-09		0.3756	0.7000	0.6941	0.7111		0.6363	0.5556	0.5862	0.5000		0.7497	0.4667	0.3793	0.6250
330	1.5259e-09		0.3704	0.6923	0.6588	0.7556		0.6290	0.4889	0.4483	0.5625		0.7283	0.5111	0.4483	0.6250
335	1.5259e-09		0.3987	0.6923	0.7294	0.6222		0.6329	0.6000	0.6897	0.4375		0.6508	0.6000	0.5862	0.6250
340	7.6294e-10		0.3704	0.7154	0.6706	0.8000		0.6411	0.5778	0.6552	0.4375		0.7018	0.5333	0.5172	0.5625
345	7.6294e-10		0.4354	0.6462	0.6941	0.5556		0.6632	0.5333	0.5517	0.5000		0.7432	0.4889	0.4138	0.6250
350	7.6294e-10		0.4049	0.7000	0.7176	0.6667		0.6353	0.5111	0.4828	0.5625		0.7545	0.5333	0.4483	0.6875
355	7.6294e-10		0.3697	0.7385	0.6941	0.8222		0.6409	0.5111	0.5517	0.4375		0.7043	0.4889	0.4483	0.5625
360	3.8147e-10		0.4095	0.7000	0.6824	0.7333		0.6375	0.6222	0.7241	0.4375		0.6633	0.6000	0.5862	0.6250
365	3.8147e-10		0.3676	0.7538	0.7765	0.7111		0.6706	0.6000	0.6897	0.4375		0.7008	0.5333	0.5172	0.5625
370	3.8147e-10		0.3891	0.7231	0.7059	0.7556		0.6296	0.5111	0.5172	0.5000		0.7480	0.5111	0.4138	0.6875
375	3.8147e-10		0.3894	0.7462	0.7412	0.7556		0.6266	0.6000	0.6552	0.5000		0.7377	0.4667	0.4138	0.5625
380	1.9073e-10		0.3550	0.7615	0.7294	0.8222		0.6409	0.6222	0.6897	0.5000		0.7242	0.5111	0.4483	0.6250
385	1.9073e-10		0.4110	0.7231	0.7529	0.6667		0.6557	0.4889	0.5172	0.4375		0.6874	0.4889	0.4483	0.5625
390	1.9073e-10		0.3943	0.7462	0.7412	0.7556		0.6697	0.4667	0.4828	0.4375		0.7048	0.5556	0.4828	0.6875
395	1.9073e-10		0.3888	0.7077	0.7412	0.6444		0.6530	0.5556	0.5862	0.5000		0.7489	0.5333	0.4483	0.6875
400	9.5367e-11		0.3834	0.7000	0.7059	0.6889		0.6386	0.5333	0.5172	0.5625		0.7263	0.5111	0.4138	0.6875
405	9.5367e-11		0.3647	0.7231	0.7059	0.7556		0.6301	0.5556	0.6207	0.4375		0.6894	0.4889	0.4483	0.5625
410	9.5367e-11		0.3729	0.7385	0.7412	0.7333		0.6322	0.5778	0.6552	0.4375		0.7093	0.5111	0.4483	0.6250
415	9.5367e-11		0.3803	0.7231	0.6824	0.8000		0.6742	0.4667	0.4828	0.4375		0.7445	0.5111	0.4483	0.6250
420	4.7684e-11		0.3803	0.7385	0.7647	0.6889		0.6591	0.5333	0.5862	0.4375		0.7374	0.4889	0.4483	0.5625
425	4.7684e-11		0.3874	0.7462	0.7529	0.7333		0.6691	0.5778	0.6552	0.4375		0.6948	0.6000	0.5862	0.6250
430	4.7684e-11		0.4126	0.6923	0.7176	0.6444		0.6418	0.4889	0.4828	0.5000		0.7596	0.5111	0.4138	0.6875
435	4.7684e-11		0.4145	0.7000	0.6941	0.7111		0.6104	0.6000	0.6552	0.5000		0.7309	0.4889	0.4483	0.5625
440	2.3842e-11		0.3960	0.6769	0.6588	0.7111		0.6647	0.5778	0.6552	0.4375		0.7102	0.5778	0.5862	0.5625
445	2.3842e-11		0.3951	0.6846	0.6824	0.6889		0.6339	0.6222	0.7241	0.4375		0.7171	0.4889	0.4483	0.5625
450	2.3842e-11		0.3972	0.6846	0.7059	0.6444		0.6462	0.6000	0.6897	0.4375		0.6959	0.5333	0.5172	0.5625
455	2.3842e-11		0.4151	0.6846	0.7059	0.6444		0.6420	0.4667	0.4483	0.5000		0.7288	0.5111	0.4138	0.6875
460	1.1921e-11		0.3785	0.7231	0.7529	0.6667		0.6368	0.4667	0.4828	0.4375		0.7560	0.4889	0.3793	0.6875
465	1.1921e-11		0.3867	0.7154	0.7412	0.6667		0.6123	0.4889	0.5172	0.4375		0.7068	0.5333	0.5172	0.5625
470	1.1921e-11		0.4313	0.6692	0.6941	0.6222		0.6408	0.5556	0.6207	0.4375		0.7358	0.4889	0.4483	0.5625
475	1.1921e-11		0.3801	0.7385	0.7647	0.6889		0.6184	0.5556	0.5862	0.5000		0.7138	0.5556	0.4828	0.6875
480	5.9605e-12		0.3990	0.7308	0.7412	0.7111		0.6492	0.5778	0.6552	0.4375		0.7237	0.5333	0.5172	0.5625
485	5.9605e-12		0.4306	0.6462	0.6824	0.5778		0.6615	0.5556	0.5862	0.5000		0.7152	0.5778	0.5172	0.6875
490	5.9605e-12		0.4186	0.6615	0.7059	0.5778		0.6152	0.5333	0.5517	0.5000		0.7550	0.4889	0.3793	0.6875
495	5.9605e-12		0.3707	0.7615	0.7412	0.8000		0.6801	0.6000	0.7241	0.3750		0.6931	0.5556	0.5862	0.5000
500	2.9802e-12		0.3822	0.7154	0.7412	0.6667		0.6577	0.5556	0.5862	0.5000		0.6980	0.5333	0.5172	0.5625
505	2.9802e-12		0.4076	0.6923	0.7059	0.6667		0.6482	0.5111	0.4828	0.5625		0.7763	0.4667	0.3448	0.6875
510	2.9802e-12		0.4213	0.6615	0.6588	0.6667		0.6381	0.4444	0.4138	0.5000		0.7278	0.5333	0.4483	0.6875
515	2.9802e-12		0.4406	0.6538	0.6706	0.6222		0.6573	0.5556	0.6207	0.4375		0.7320	0.5111	0.4483	0.6250
520	1.4901e-12		0.3812	0.7077	0.7176	0.6889		0.6342	0.5111	0.5517	0.4375		0.7077	0.4889	0.4138	0.6250
525	1.4901e-12		0.3607	0.7615	0.7647	0.7556		0.6666	0.5333	0.5862	0.4375		0.7026	0.5333	0.5172	0.5625
530	1.4901e-12		0.3758	0.7154	0.7176	0.7111		0.6740	0.5333	0.5517	0.5000		0.7721	0.4889	0.3793	0.6875
535	1.4901e-12		0.3846	0.7077	0.7176	0.6889		0.6363	0.5333	0.5172	0.5625		0.7490	0.4889	0.4138	0.6250
540	7.4506e-13		0.4119	0.7077	0.7294	0.6667		0.6609	0.5778	0.6207	0.5000		0.7683	0.4667	0.4138	0.5625
545	7.4506e-13		0.4023	0.7231	0.7294	0.7111		0.6615	0.5333	0.5862	0.4375		0.7148	0.5111	0.4828	0.5625
550	7.4506e-13		0.3754	0.7769	0.7882	0.7556		0.6649	0.5333	0.5862	0.4375		0.7247	0.5111	0.4828	0.5625
555	7.4506e-13		0.3918	0.6923	0.6824	0.7111		0.6390	0.6000	0.6552	0.5000		0.7072	0.5111	0.4483	0.6250
560	3.7253e-13		0.4033	0.7077	0.6824	0.7556		0.7077	0.4667	0.4483	0.5000		0.7600	0.4889	0.3793	0.6875
565	3.7253e-13		0.4295	0.6846	0.7059	0.6444		0.6801	0.5556	0.6207	0.4375		0.7333	0.5111	0.4828	0.5625
