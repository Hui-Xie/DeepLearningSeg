Program ID 18431

Program command: 
 ['/home/hxie1/Projects/OvarianCancer/TrainSegV.py', '/home/hxie1/temp_netParameters/OvarianCancer/Label0_1', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainImages', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainLabels', '2D', '0,1']

Major program changes: ConvResidual use BatchNorm-reLU-Conv structure; 
                       ConsDense also use BatchNorm-reLU-Conv structure.
                       Add ConvSegDecreaseChannels
                       and each block has 5 layers, 
                       Residual connect to each Conv, 
                       skip at least 2 layers.
                       output layer use conv with 3*3 fiter instead of 1*1 filter. 
                       use boundary loss with weight 0 at beginning, and pretrain CE loss. 
                       special convInput Module
                       convOutput moudel uses 1*1 conv to tranparent gradident 
                       ConvOutput use residual module.
                       Use Dense Net in the Building Block
                       add ConvBlock to wrapp the ConvResidual and ConvDense
                       first layer filter = 96, reducing from 128 of previous experiment
                       the nLayers in block is 4, increase from 2 of previous experiment
                       
            

Program starting Time: 2019-05-10 09:10:55.111350
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/Label0_1

Infor: program test labels: [0, 1]
Infor: program suppressed labels: [2, 3]
Infor: program test labels: [0, 1]
Infor: program suppressed labels: [2, 3]
Building the Segmented Slice Tuple list, which may need 8 mins, please waiting......
Directory of /home/hxie1/data/OvarianCancerCT/Extract_uniform/trainLabels has 164 segmented slices for remained labels [0, 1].
Building the Segmented Slice Tuple list, which may need 8 mins, please waiting......
Directory of /home/hxie1/data/OvarianCancerCT/Extract_uniform/testLabels has 31 segmented slices for remained labels [0, 1].
Info: program uses 2D input.
TrainData Input:  batchSize=8, depth=1, height=281, width=281, NumClassfication=2

TestData Input:  batchSize=8, depth=1, height=281, width=281, NumClassfication=2

Network trains from scratch.
Network has total 115919522 parameters.
Info: network dropout rate = 0.3
Infor: Cross Entropy Weight: [1.0260619741432382, 39.37007874015748]

====================Net Architecture===========================
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 96, 281, 281]             960
       BatchNorm2d-2         [-1, 96, 281, 281]             192
              ReLU-3         [-1, 96, 281, 281]               0
            Conv2d-4         [-1, 96, 281, 281]           9,312
       BatchNorm2d-5         [-1, 96, 281, 281]             192
              ReLU-6         [-1, 96, 281, 281]               0
            Conv2d-7         [-1, 24, 281, 281]          20,760
       BatchNorm2d-8        [-1, 120, 281, 281]             240
              ReLU-9        [-1, 120, 281, 281]               0
           Conv2d-10         [-1, 96, 281, 281]          11,616
      BatchNorm2d-11         [-1, 96, 281, 281]             192
             ReLU-12         [-1, 96, 281, 281]               0
           Conv2d-13         [-1, 24, 281, 281]          20,760
      BatchNorm2d-14        [-1, 144, 281, 281]             288
             ReLU-15        [-1, 144, 281, 281]               0
           Conv2d-16         [-1, 96, 281, 281]          13,920
      BatchNorm2d-17         [-1, 96, 281, 281]             192
             ReLU-18         [-1, 96, 281, 281]               0
           Conv2d-19         [-1, 24, 281, 281]          20,760
      BatchNorm2d-20        [-1, 168, 281, 281]             336
             ReLU-21        [-1, 168, 281, 281]               0
           Conv2d-22         [-1, 96, 281, 281]          16,224
      BatchNorm2d-23         [-1, 96, 281, 281]             192
             ReLU-24         [-1, 96, 281, 281]               0
           Conv2d-25         [-1, 24, 281, 281]          20,760
      BatchNorm2d-26        [-1, 192, 281, 281]             384
             ReLU-27        [-1, 192, 281, 281]               0
           Conv2d-28         [-1, 96, 281, 281]          18,528
        ConvDense-29         [-1, 96, 281, 281]               0
        ConvBlock-30         [-1, 96, 281, 281]               0
        ConvInput-31         [-1, 96, 281, 281]               0
      BatchNorm2d-32         [-1, 96, 281, 281]             192
           Conv2d-33         [-1, 96, 139, 139]         230,496
      BatchNorm2d-34         [-1, 96, 139, 139]             192
             ReLU-35         [-1, 96, 139, 139]               0
           Conv2d-36         [-1, 96, 139, 139]           9,312
      BatchNorm2d-37         [-1, 96, 139, 139]             192
             ReLU-38         [-1, 96, 139, 139]               0
           Conv2d-39         [-1, 24, 139, 139]          20,760
      BatchNorm2d-40        [-1, 120, 139, 139]             240
             ReLU-41        [-1, 120, 139, 139]               0
           Conv2d-42         [-1, 96, 139, 139]          11,616
      BatchNorm2d-43         [-1, 96, 139, 139]             192
             ReLU-44         [-1, 96, 139, 139]               0
           Conv2d-45         [-1, 24, 139, 139]          20,760
      BatchNorm2d-46        [-1, 144, 139, 139]             288
             ReLU-47        [-1, 144, 139, 139]               0
           Conv2d-48         [-1, 96, 139, 139]          13,920
      BatchNorm2d-49         [-1, 96, 139, 139]             192
             ReLU-50         [-1, 96, 139, 139]               0
           Conv2d-51         [-1, 24, 139, 139]          20,760
      BatchNorm2d-52        [-1, 168, 139, 139]             336
             ReLU-53        [-1, 168, 139, 139]               0
           Conv2d-54         [-1, 96, 139, 139]          16,224
      BatchNorm2d-55         [-1, 96, 139, 139]             192
             ReLU-56         [-1, 96, 139, 139]               0
           Conv2d-57         [-1, 24, 139, 139]          20,760
      BatchNorm2d-58        [-1, 192, 139, 139]             384
             ReLU-59        [-1, 192, 139, 139]               0
           Conv2d-60         [-1, 96, 139, 139]          18,528
        ConvDense-61         [-1, 96, 139, 139]               0
        ConvBlock-62         [-1, 96, 139, 139]               0
         Down2dBB-63         [-1, 96, 139, 139]               0
        Dropout2d-64         [-1, 96, 139, 139]               0
      BatchNorm2d-65         [-1, 96, 139, 139]             192
           Conv2d-66          [-1, 192, 69, 69]         166,080
      BatchNorm2d-67          [-1, 192, 69, 69]             384
             ReLU-68          [-1, 192, 69, 69]               0
           Conv2d-69          [-1, 192, 69, 69]          37,056
      BatchNorm2d-70          [-1, 192, 69, 69]             384
             ReLU-71          [-1, 192, 69, 69]               0
           Conv2d-72           [-1, 48, 69, 69]          82,992
      BatchNorm2d-73          [-1, 240, 69, 69]             480
             ReLU-74          [-1, 240, 69, 69]               0
           Conv2d-75          [-1, 192, 69, 69]          46,272
      BatchNorm2d-76          [-1, 192, 69, 69]             384
             ReLU-77          [-1, 192, 69, 69]               0
           Conv2d-78           [-1, 48, 69, 69]          82,992
      BatchNorm2d-79          [-1, 288, 69, 69]             576
             ReLU-80          [-1, 288, 69, 69]               0
           Conv2d-81          [-1, 192, 69, 69]          55,488
      BatchNorm2d-82          [-1, 192, 69, 69]             384
             ReLU-83          [-1, 192, 69, 69]               0
           Conv2d-84           [-1, 48, 69, 69]          82,992
      BatchNorm2d-85          [-1, 336, 69, 69]             672
             ReLU-86          [-1, 336, 69, 69]               0
           Conv2d-87          [-1, 192, 69, 69]          64,704
      BatchNorm2d-88          [-1, 192, 69, 69]             384
             ReLU-89          [-1, 192, 69, 69]               0
           Conv2d-90           [-1, 48, 69, 69]          82,992
      BatchNorm2d-91          [-1, 384, 69, 69]             768
             ReLU-92          [-1, 384, 69, 69]               0
           Conv2d-93          [-1, 192, 69, 69]          73,920
        ConvDense-94          [-1, 192, 69, 69]               0
        ConvBlock-95          [-1, 192, 69, 69]               0
         Down2dBB-96          [-1, 192, 69, 69]               0
        Dropout2d-97          [-1, 192, 69, 69]               0
      BatchNorm2d-98          [-1, 192, 69, 69]             384
           Conv2d-99          [-1, 384, 33, 33]       1,843,584
     BatchNorm2d-100          [-1, 384, 33, 33]             768
            ReLU-101          [-1, 384, 33, 33]               0
          Conv2d-102          [-1, 384, 33, 33]         147,840
     BatchNorm2d-103          [-1, 384, 33, 33]             768
            ReLU-104          [-1, 384, 33, 33]               0
          Conv2d-105           [-1, 96, 33, 33]         331,872
     BatchNorm2d-106          [-1, 480, 33, 33]             960
            ReLU-107          [-1, 480, 33, 33]               0
          Conv2d-108          [-1, 384, 33, 33]         184,704
     BatchNorm2d-109          [-1, 384, 33, 33]             768
            ReLU-110          [-1, 384, 33, 33]               0
          Conv2d-111           [-1, 96, 33, 33]         331,872
     BatchNorm2d-112          [-1, 576, 33, 33]           1,152
            ReLU-113          [-1, 576, 33, 33]               0
          Conv2d-114          [-1, 384, 33, 33]         221,568
     BatchNorm2d-115          [-1, 384, 33, 33]             768
            ReLU-116          [-1, 384, 33, 33]               0
          Conv2d-117           [-1, 96, 33, 33]         331,872
     BatchNorm2d-118          [-1, 672, 33, 33]           1,344
            ReLU-119          [-1, 672, 33, 33]               0
          Conv2d-120          [-1, 384, 33, 33]         258,432
     BatchNorm2d-121          [-1, 384, 33, 33]             768
            ReLU-122          [-1, 384, 33, 33]               0
          Conv2d-123           [-1, 96, 33, 33]         331,872
     BatchNorm2d-124          [-1, 768, 33, 33]           1,536
            ReLU-125          [-1, 768, 33, 33]               0
          Conv2d-126          [-1, 384, 33, 33]         295,296
       ConvDense-127          [-1, 384, 33, 33]               0
       ConvBlock-128          [-1, 384, 33, 33]               0
        Down2dBB-129          [-1, 384, 33, 33]               0
       Dropout2d-130          [-1, 384, 33, 33]               0
     BatchNorm2d-131          [-1, 384, 33, 33]             768
          Conv2d-132          [-1, 768, 15, 15]       7,373,568
     BatchNorm2d-133          [-1, 768, 15, 15]           1,536
            ReLU-134          [-1, 768, 15, 15]               0
          Conv2d-135          [-1, 768, 15, 15]         590,592
     BatchNorm2d-136          [-1, 768, 15, 15]           1,536
            ReLU-137          [-1, 768, 15, 15]               0
          Conv2d-138          [-1, 192, 15, 15]       1,327,296
     BatchNorm2d-139          [-1, 960, 15, 15]           1,920
            ReLU-140          [-1, 960, 15, 15]               0
          Conv2d-141          [-1, 768, 15, 15]         738,048
     BatchNorm2d-142          [-1, 768, 15, 15]           1,536
            ReLU-143          [-1, 768, 15, 15]               0
          Conv2d-144          [-1, 192, 15, 15]       1,327,296
     BatchNorm2d-145         [-1, 1152, 15, 15]           2,304
            ReLU-146         [-1, 1152, 15, 15]               0
          Conv2d-147          [-1, 768, 15, 15]         885,504
     BatchNorm2d-148          [-1, 768, 15, 15]           1,536
            ReLU-149          [-1, 768, 15, 15]               0
          Conv2d-150          [-1, 192, 15, 15]       1,327,296
     BatchNorm2d-151         [-1, 1344, 15, 15]           2,688
            ReLU-152         [-1, 1344, 15, 15]               0
          Conv2d-153          [-1, 768, 15, 15]       1,032,960
     BatchNorm2d-154          [-1, 768, 15, 15]           1,536
            ReLU-155          [-1, 768, 15, 15]               0
          Conv2d-156          [-1, 192, 15, 15]       1,327,296
     BatchNorm2d-157         [-1, 1536, 15, 15]           3,072
            ReLU-158         [-1, 1536, 15, 15]               0
          Conv2d-159          [-1, 768, 15, 15]       1,180,416
       ConvDense-160          [-1, 768, 15, 15]               0
       ConvBlock-161          [-1, 768, 15, 15]               0
        Down2dBB-162          [-1, 768, 15, 15]               0
       Dropout2d-163          [-1, 768, 15, 15]               0
     BatchNorm2d-164          [-1, 768, 15, 15]           1,536
          Conv2d-165           [-1, 1536, 7, 7]      10,618,368
     BatchNorm2d-166           [-1, 1536, 7, 7]           3,072
            ReLU-167           [-1, 1536, 7, 7]               0
          Conv2d-168           [-1, 1536, 7, 7]       2,360,832
     BatchNorm2d-169           [-1, 1536, 7, 7]           3,072
            ReLU-170           [-1, 1536, 7, 7]               0
          Conv2d-171            [-1, 384, 7, 7]       5,308,800
     BatchNorm2d-172           [-1, 1920, 7, 7]           3,840
            ReLU-173           [-1, 1920, 7, 7]               0
          Conv2d-174           [-1, 1536, 7, 7]       2,950,656
     BatchNorm2d-175           [-1, 1536, 7, 7]           3,072
            ReLU-176           [-1, 1536, 7, 7]               0
          Conv2d-177            [-1, 384, 7, 7]       5,308,800
     BatchNorm2d-178           [-1, 2304, 7, 7]           4,608
            ReLU-179           [-1, 2304, 7, 7]               0
          Conv2d-180           [-1, 1536, 7, 7]       3,540,480
     BatchNorm2d-181           [-1, 1536, 7, 7]           3,072
            ReLU-182           [-1, 1536, 7, 7]               0
          Conv2d-183            [-1, 384, 7, 7]       5,308,800
     BatchNorm2d-184           [-1, 2688, 7, 7]           5,376
            ReLU-185           [-1, 2688, 7, 7]               0
          Conv2d-186           [-1, 1536, 7, 7]       4,130,304
     BatchNorm2d-187           [-1, 1536, 7, 7]           3,072
            ReLU-188           [-1, 1536, 7, 7]               0
          Conv2d-189            [-1, 384, 7, 7]       5,308,800
     BatchNorm2d-190           [-1, 3072, 7, 7]           6,144
            ReLU-191           [-1, 3072, 7, 7]               0
          Conv2d-192           [-1, 1536, 7, 7]       4,720,128
       ConvDense-193           [-1, 1536, 7, 7]               0
       ConvBlock-194           [-1, 1536, 7, 7]               0
        Down2dBB-195           [-1, 1536, 7, 7]               0
       Dropout2d-196           [-1, 1536, 7, 7]               0
     BatchNorm2d-197           [-1, 1536, 7, 7]           3,072
 ConvTranspose2d-198          [-1, 768, 15, 15]      10,617,600
     BatchNorm2d-199          [-1, 768, 15, 15]           1,536
            ReLU-200          [-1, 768, 15, 15]               0
          Conv2d-201          [-1, 768, 15, 15]         590,592
     BatchNorm2d-202          [-1, 768, 15, 15]           1,536
            ReLU-203          [-1, 768, 15, 15]               0
          Conv2d-204          [-1, 192, 15, 15]       1,327,296
     BatchNorm2d-205          [-1, 960, 15, 15]           1,920
            ReLU-206          [-1, 960, 15, 15]               0
          Conv2d-207          [-1, 768, 15, 15]         738,048
     BatchNorm2d-208          [-1, 768, 15, 15]           1,536
            ReLU-209          [-1, 768, 15, 15]               0
          Conv2d-210          [-1, 192, 15, 15]       1,327,296
     BatchNorm2d-211         [-1, 1152, 15, 15]           2,304
            ReLU-212         [-1, 1152, 15, 15]               0
          Conv2d-213          [-1, 768, 15, 15]         885,504
     BatchNorm2d-214          [-1, 768, 15, 15]           1,536
            ReLU-215          [-1, 768, 15, 15]               0
          Conv2d-216          [-1, 192, 15, 15]       1,327,296
     BatchNorm2d-217         [-1, 1344, 15, 15]           2,688
            ReLU-218         [-1, 1344, 15, 15]               0
          Conv2d-219          [-1, 768, 15, 15]       1,032,960
     BatchNorm2d-220          [-1, 768, 15, 15]           1,536
            ReLU-221          [-1, 768, 15, 15]               0
          Conv2d-222          [-1, 192, 15, 15]       1,327,296
     BatchNorm2d-223         [-1, 1536, 15, 15]           3,072
            ReLU-224         [-1, 1536, 15, 15]               0
          Conv2d-225          [-1, 768, 15, 15]       1,180,416
       ConvDense-226          [-1, 768, 15, 15]               0
       ConvBlock-227          [-1, 768, 15, 15]               0
          Up2dBB-228          [-1, 768, 15, 15]               0
       Dropout2d-229          [-1, 768, 15, 15]               0
     BatchNorm2d-230         [-1, 1536, 15, 15]           3,072
 ConvTranspose2d-231          [-1, 384, 33, 33]      14,745,984
     BatchNorm2d-232          [-1, 384, 33, 33]             768
            ReLU-233          [-1, 384, 33, 33]               0
          Conv2d-234          [-1, 384, 33, 33]         147,840
     BatchNorm2d-235          [-1, 384, 33, 33]             768
            ReLU-236          [-1, 384, 33, 33]               0
          Conv2d-237           [-1, 96, 33, 33]         331,872
     BatchNorm2d-238          [-1, 480, 33, 33]             960
            ReLU-239          [-1, 480, 33, 33]               0
          Conv2d-240          [-1, 384, 33, 33]         184,704
     BatchNorm2d-241          [-1, 384, 33, 33]             768
            ReLU-242          [-1, 384, 33, 33]               0
          Conv2d-243           [-1, 96, 33, 33]         331,872
     BatchNorm2d-244          [-1, 576, 33, 33]           1,152
            ReLU-245          [-1, 576, 33, 33]               0
          Conv2d-246          [-1, 384, 33, 33]         221,568
     BatchNorm2d-247          [-1, 384, 33, 33]             768
            ReLU-248          [-1, 384, 33, 33]               0
          Conv2d-249           [-1, 96, 33, 33]         331,872
     BatchNorm2d-250          [-1, 672, 33, 33]           1,344
            ReLU-251          [-1, 672, 33, 33]               0
          Conv2d-252          [-1, 384, 33, 33]         258,432
     BatchNorm2d-253          [-1, 384, 33, 33]             768
            ReLU-254          [-1, 384, 33, 33]               0
          Conv2d-255           [-1, 96, 33, 33]         331,872
     BatchNorm2d-256          [-1, 768, 33, 33]           1,536
            ReLU-257          [-1, 768, 33, 33]               0
          Conv2d-258          [-1, 384, 33, 33]         295,296
       ConvDense-259          [-1, 384, 33, 33]               0
       ConvBlock-260          [-1, 384, 33, 33]               0
          Up2dBB-261          [-1, 384, 33, 33]               0
       Dropout2d-262          [-1, 384, 33, 33]               0
     BatchNorm2d-263          [-1, 768, 33, 33]           1,536
 ConvTranspose2d-264          [-1, 192, 69, 69]       3,686,592
     BatchNorm2d-265          [-1, 192, 69, 69]             384
            ReLU-266          [-1, 192, 69, 69]               0
          Conv2d-267          [-1, 192, 69, 69]          37,056
     BatchNorm2d-268          [-1, 192, 69, 69]             384
            ReLU-269          [-1, 192, 69, 69]               0
          Conv2d-270           [-1, 48, 69, 69]          82,992
     BatchNorm2d-271          [-1, 240, 69, 69]             480
            ReLU-272          [-1, 240, 69, 69]               0
          Conv2d-273          [-1, 192, 69, 69]          46,272
     BatchNorm2d-274          [-1, 192, 69, 69]             384
            ReLU-275          [-1, 192, 69, 69]               0
          Conv2d-276           [-1, 48, 69, 69]          82,992
     BatchNorm2d-277          [-1, 288, 69, 69]             576
            ReLU-278          [-1, 288, 69, 69]               0
          Conv2d-279          [-1, 192, 69, 69]          55,488
     BatchNorm2d-280          [-1, 192, 69, 69]             384
            ReLU-281          [-1, 192, 69, 69]               0
          Conv2d-282           [-1, 48, 69, 69]          82,992
     BatchNorm2d-283          [-1, 336, 69, 69]             672
            ReLU-284          [-1, 336, 69, 69]               0
          Conv2d-285          [-1, 192, 69, 69]          64,704
     BatchNorm2d-286          [-1, 192, 69, 69]             384
            ReLU-287          [-1, 192, 69, 69]               0
          Conv2d-288           [-1, 48, 69, 69]          82,992
     BatchNorm2d-289          [-1, 384, 69, 69]             768
            ReLU-290          [-1, 384, 69, 69]               0
          Conv2d-291          [-1, 192, 69, 69]          73,920
       ConvDense-292          [-1, 192, 69, 69]               0
       ConvBlock-293          [-1, 192, 69, 69]               0
          Up2dBB-294          [-1, 192, 69, 69]               0
       Dropout2d-295          [-1, 192, 69, 69]               0
     BatchNorm2d-296          [-1, 384, 69, 69]             768
 ConvTranspose2d-297         [-1, 96, 139, 139]         331,872
     BatchNorm2d-298         [-1, 96, 139, 139]             192
            ReLU-299         [-1, 96, 139, 139]               0
          Conv2d-300         [-1, 96, 139, 139]           9,312
     BatchNorm2d-301         [-1, 96, 139, 139]             192
            ReLU-302         [-1, 96, 139, 139]               0
          Conv2d-303         [-1, 24, 139, 139]          20,760
     BatchNorm2d-304        [-1, 120, 139, 139]             240
            ReLU-305        [-1, 120, 139, 139]               0
          Conv2d-306         [-1, 96, 139, 139]          11,616
     BatchNorm2d-307         [-1, 96, 139, 139]             192
            ReLU-308         [-1, 96, 139, 139]               0
          Conv2d-309         [-1, 24, 139, 139]          20,760
     BatchNorm2d-310        [-1, 144, 139, 139]             288
            ReLU-311        [-1, 144, 139, 139]               0
          Conv2d-312         [-1, 96, 139, 139]          13,920
     BatchNorm2d-313         [-1, 96, 139, 139]             192
            ReLU-314         [-1, 96, 139, 139]               0
          Conv2d-315         [-1, 24, 139, 139]          20,760
     BatchNorm2d-316        [-1, 168, 139, 139]             336
            ReLU-317        [-1, 168, 139, 139]               0
          Conv2d-318         [-1, 96, 139, 139]          16,224
     BatchNorm2d-319         [-1, 96, 139, 139]             192
            ReLU-320         [-1, 96, 139, 139]               0
          Conv2d-321         [-1, 24, 139, 139]          20,760
     BatchNorm2d-322        [-1, 192, 139, 139]             384
            ReLU-323        [-1, 192, 139, 139]               0
          Conv2d-324         [-1, 96, 139, 139]          18,528
       ConvDense-325         [-1, 96, 139, 139]               0
       ConvBlock-326         [-1, 96, 139, 139]               0
          Up2dBB-327         [-1, 96, 139, 139]               0
       Dropout2d-328         [-1, 96, 139, 139]               0
     BatchNorm2d-329        [-1, 192, 139, 139]             384
 ConvTranspose2d-330         [-1, 96, 281, 281]         460,896
     BatchNorm2d-331         [-1, 96, 281, 281]             192
            ReLU-332         [-1, 96, 281, 281]               0
          Conv2d-333         [-1, 96, 281, 281]           9,312
     BatchNorm2d-334         [-1, 96, 281, 281]             192
            ReLU-335         [-1, 96, 281, 281]               0
          Conv2d-336         [-1, 24, 281, 281]          20,760
     BatchNorm2d-337        [-1, 120, 281, 281]             240
            ReLU-338        [-1, 120, 281, 281]               0
          Conv2d-339         [-1, 96, 281, 281]          11,616
     BatchNorm2d-340         [-1, 96, 281, 281]             192
            ReLU-341         [-1, 96, 281, 281]               0
          Conv2d-342         [-1, 24, 281, 281]          20,760
     BatchNorm2d-343        [-1, 144, 281, 281]             288
            ReLU-344        [-1, 144, 281, 281]               0
          Conv2d-345         [-1, 96, 281, 281]          13,920
     BatchNorm2d-346         [-1, 96, 281, 281]             192
            ReLU-347         [-1, 96, 281, 281]               0
          Conv2d-348         [-1, 24, 281, 281]          20,760
     BatchNorm2d-349        [-1, 168, 281, 281]             336
            ReLU-350        [-1, 168, 281, 281]               0
          Conv2d-351         [-1, 96, 281, 281]          16,224
     BatchNorm2d-352         [-1, 96, 281, 281]             192
            ReLU-353         [-1, 96, 281, 281]               0
          Conv2d-354         [-1, 24, 281, 281]          20,760
     BatchNorm2d-355        [-1, 192, 281, 281]             384
            ReLU-356        [-1, 192, 281, 281]               0
          Conv2d-357         [-1, 96, 281, 281]          18,528
       ConvDense-358         [-1, 96, 281, 281]               0
       ConvBlock-359         [-1, 96, 281, 281]               0
          Up2dBB-360         [-1, 96, 281, 281]               0
       Dropout2d-361         [-1, 96, 281, 281]               0
     BatchNorm2d-362        [-1, 192, 281, 281]             384
            ReLU-363        [-1, 192, 281, 281]               0
          Conv2d-364        [-1, 192, 281, 281]          37,056
     BatchNorm2d-365        [-1, 192, 281, 281]             384
            ReLU-366        [-1, 192, 281, 281]               0
          Conv2d-367         [-1, 48, 281, 281]          82,992
     BatchNorm2d-368        [-1, 240, 281, 281]             480
            ReLU-369        [-1, 240, 281, 281]               0
          Conv2d-370        [-1, 192, 281, 281]          46,272
     BatchNorm2d-371        [-1, 192, 281, 281]             384
            ReLU-372        [-1, 192, 281, 281]               0
          Conv2d-373         [-1, 48, 281, 281]          82,992
     BatchNorm2d-374        [-1, 288, 281, 281]             576
            ReLU-375        [-1, 288, 281, 281]               0
          Conv2d-376        [-1, 192, 281, 281]          55,488
     BatchNorm2d-377        [-1, 192, 281, 281]             384
            ReLU-378        [-1, 192, 281, 281]               0
          Conv2d-379         [-1, 48, 281, 281]          82,992
     BatchNorm2d-380        [-1, 336, 281, 281]             672
            ReLU-381        [-1, 336, 281, 281]               0
          Conv2d-382        [-1, 192, 281, 281]          64,704
     BatchNorm2d-383        [-1, 192, 281, 281]             384
            ReLU-384        [-1, 192, 281, 281]               0
          Conv2d-385         [-1, 48, 281, 281]          82,992
     BatchNorm2d-386        [-1, 384, 281, 281]             768
            ReLU-387        [-1, 384, 281, 281]               0
          Conv2d-388        [-1, 192, 281, 281]          73,920
       ConvDense-389        [-1, 192, 281, 281]               0
       ConvBlock-390        [-1, 192, 281, 281]               0
     BatchNorm2d-391        [-1, 192, 281, 281]             384
          Conv2d-392          [-1, 2, 281, 281]             386
      ConvOutput-393          [-1, 2, 281, 281]               0
================================================================
Total params: 115,919,522
Trainable params: 115,919,522
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.30
Forward/backward pass size (MB): 9476.45
Params size (MB): 442.20
Estimated Total Size (MB): 9918.95
----------------------------------------------------------------
===================End of Net Architecture =====================

Info: program will use 4 GPUs.
Loss Functions List: FocalCELoss with weight of 1; 	BoundaryLoss with weight of 0; 
Hints: Test Dice_0 is the dice coeff for all non-zero labels
Hints: Test Dice_1 is for primary cancer(green), test Dice_2 is for metastasis(yellow), and test Dice_3 is for invaded lymph node(brown).
Hints: Test TPR_0 is the TPR for all non-zero labels
Hints: Test TPR_1 is for primary cancer(green), TPR_2 is for metastasis(yellow), and TPR_3 is for invaded lymph node(brown).

Epoch 	 TrainingLoss 	 TestLoss 	Dice_0	Dice_1	TPR_0	TPR_1
0 	 0.0993 	 0.1166 	0.171	0.171	1.000	1.000
1 	 0.0786 	 0.0867 	0.287	0.287	0.999	0.999
2 	 0.0682 	 0.0572 	0.479	0.479	0.933	0.933
3 	 0.0590 	 0.1396 	0.519	0.519	0.813	0.813
4 	 0.0569 	 0.0437 	0.552	0.552	0.953	0.953
5 	 0.0471 	 0.0440 	0.551	0.551	0.969	0.969
6 	 0.0464 	 0.0561 	0.549	0.549	0.943	0.943
7 	 0.0557 	 0.0753 	0.460	0.460	0.945	0.945
8 	 0.0484 	 0.0473 	0.493	0.493	0.986	0.986
9 	 0.0524 	 0.1104 	0.567	0.567	0.909	0.909
10 	 0.0551 	 0.1002 	0.560	0.560	0.935	0.935
11 	 0.0501 	 0.0390 	0.484	0.484	0.986	0.986
12 	 0.0418 	 0.0384 	0.607	0.607	0.961	0.961
13 	 0.0433 	 0.0635 	0.577	0.577	0.918	0.918
14 	 0.0427 	 0.0461 	0.481	0.481	0.965	0.965
15 	 0.0375 	 0.0431 	0.616	0.616	0.962	0.962
16 	 0.0369 	 0.0369 	0.537	0.537	0.992	0.992
17 	 0.0379 	 0.0890 	0.603	0.603	0.934	0.934
18 	 0.0402 	 0.0310 	0.648	0.648	0.962	0.962
19 	 0.0347 	 0.0374 	0.590	0.590	0.963	0.963
20 	 0.0331 	 0.0363 	0.579	0.579	0.970	0.970
21 	 0.0355 	 0.0394 	0.600	0.600	0.959	0.959
22 	 0.0362 	 0.0446 	0.611	0.611	0.955	0.955
23 	 0.0367 	 0.0367 	0.544	0.544	0.974	0.974
24 	 0.0366 	 0.0673 	0.657	0.657	0.917	0.917
25 	 0.0330 	 0.0352 	0.538	0.538	0.985	0.985
26 	 0.0323 	 0.0306 	0.567	0.567	0.988	0.988
27 	 0.0306 	 0.0390 	0.623	0.623	0.964	0.964
28 	 0.0319 	 0.0721 	0.625	0.625	0.933	0.933
29 	 0.0341 	 0.0593 	0.593	0.593	0.957	0.957
30 	 0.0352 	 0.0300 	0.606	0.606	0.984	0.984
31 	 0.0338 	 0.0849 	0.657	0.657	0.892	0.892
32 	 0.0333 	 0.0370 	0.634	0.634	0.956	0.956
33 	 0.0369 	 0.0553 	0.556	0.556	0.952	0.952
34 	 0.0344 	 0.0379 	0.592	0.592	0.973	0.973
35 	 0.0314 	 0.0521 	0.476	0.476	0.998	0.998
36 	 0.0333 	 0.0359 	0.503	0.503	0.999	0.999
37 	 0.0336 	 0.0671 	0.648	0.648	0.917	0.917
38 	 0.0308 	 0.0328 	0.589	0.589	0.988	0.988
39 	 0.0309 	 0.0360 	0.609	0.609	0.977	0.977
40 	 0.0311 	 0.0564 	0.579	0.579	0.961	0.961
41 	 0.0290 	 0.0275 	0.601	0.601	0.986	0.986
42 	 0.0282 	 0.0492 	0.644	0.644	0.950	0.950
43 	 0.0314 	 0.0322 	0.618	0.618	0.983	0.983
44 	 0.0291 	 0.0431 	0.624	0.624	0.955	0.955
45 	 0.0268 	 0.0426 	0.678	0.678	0.938	0.938
46 	 0.0310 	 0.0351 	0.632	0.632	0.969	0.969
47 	 0.0287 	 0.0422 	0.626	0.626	0.962	0.962
48 	 0.0300 	 0.0293 	0.599	0.599	0.988	0.988
49 	 0.0295 	 0.0447 	0.610	0.610	0.962	0.962
50 	 0.0347 	 0.0388 	0.615	0.615	0.974	0.974
51 	 0.0323 	 0.0964 	0.634	0.634	0.935	0.935
52 	 0.0308 	 0.0283 	0.612	0.612	0.983	0.983
53 	 0.0302 	 0.0511 	0.619	0.619	0.953	0.953
54 	 0.0291 	 0.0612 	0.633	0.633	0.926	0.926
55 	 0.0286 	 0.0457 	0.648	0.648	0.941	0.941
56 	 0.0271 	 0.0443 	0.614	0.614	0.954	0.954
57 	 0.0266 	 0.0366 	0.584	0.584	0.986	0.986
58 	 0.0275 	 0.0377 	0.633	0.633	0.978	0.978
59 	 0.0366 	 0.0443 	0.645	0.645	0.962	0.962
60 	 0.0297 	 0.0431 	0.622	0.622	0.965	0.965
61 	 0.0285 	 0.0569 	0.662	0.662	0.942	0.942
62 	 0.0258 	 0.0327 	0.630	0.630	0.982	0.982
63 	 0.0281 	 0.0413 	0.589	0.589	0.971	0.971
64 	 0.0271 	 0.0417 	0.625	0.625	0.971	0.971
65 	 0.0254 	 0.0360 	0.626	0.626	0.975	0.975
66 	 0.0289 	 0.0383 	0.599	0.599	0.963	0.963
67 	 0.0269 	 0.0430 	0.658	0.658	0.957	0.957
68 	 0.0240 	 0.0401 	0.648	0.648	0.965	0.965
69 	 0.0315 	 0.0420 	0.644	0.644	0.974	0.974
70 	 0.0265 	 0.0591 	0.680	0.680	0.937	0.937
71 	 0.0254 	 0.0313 	0.640	0.640	0.983	0.983
72 	 0.0242 	 0.0306 	0.612	0.612	0.986	0.986
73 	 0.0276 	 0.0378 	0.548	0.548	0.993	0.993
74 	 0.0254 	 0.0416 	0.621	0.621	0.973	0.973
75 	 0.0229 	 0.0395 	0.661	0.661	0.966	0.966
76 	 0.0222 	 0.0382 	0.650	0.650	0.969	0.969
77 	 0.0234 	 0.0359 	0.670	0.670	0.969	0.969
78 	 0.0246 	 0.0343 	0.642	0.642	0.975	0.975
79 	 0.0217 	 0.0412 	0.667	0.667	0.964	0.964
80 	 0.0240 	 0.0493 	0.675	0.675	0.954	0.954
81 	 0.0242 	 0.0490 	0.676	0.676	0.953	0.953
82 	 0.0253 	 0.0340 	0.636	0.636	0.976	0.976
83 	 0.0247 	 0.0450 	0.673	0.673	0.957	0.957
84 	 0.0274 	 0.0525 	0.677	0.677	0.960	0.960
85 	 0.0247 	 0.0397 	0.643	0.643	0.971	0.971
86 	 0.0232 	 0.0327 	0.637	0.637	0.973	0.973
87 	 0.0234 	 0.0424 	0.663	0.663	0.960	0.960
88 	 0.0255 	 0.0377 	0.629	0.629	0.973	0.973
89 	 0.0232 	 0.0419 	0.642	0.642	0.966	0.966
90 	 0.0232 	 0.0411 	0.658	0.658	0.963	0.963
91 	 0.0211 	 0.0344 	0.640	0.640	0.978	0.978
92 	 0.0255 	 0.0414 	0.618	0.618	0.970	0.970
93 	 0.0243 	 0.0379 	0.638	0.638	0.972	0.972
94 	 0.0241 	 0.0366 	0.633	0.633	0.976	0.976
95 	 0.0221 	 0.0427 	0.666	0.666	0.967	0.967
96 	 0.0213 	 0.0507 	0.662	0.662	0.952	0.952
97 	 0.0213 	 0.0546 	0.674	0.674	0.945	0.945
98 	 0.0228 	 0.0352 	0.651	0.651	0.972	0.972
99 	 0.0203 	 0.0360 	0.664	0.664	0.975	0.975
100 	 0.0217 	 0.0390 	0.637	0.637	0.980	0.980
101 	 0.0238 	 0.0533 	0.672	0.672	0.963	0.963
102 	 0.0225 	 0.0469 	0.672	0.672	0.965	0.965
103 	 0.0216 	 0.0427 	0.690	0.690	0.961	0.961
104 	 0.0215 	 0.0410 	0.673	0.673	0.968	0.968
105 	 0.0199 	 0.0433 	0.691	0.691	0.959	0.959
106 	 0.0200 	 0.0427 	0.671	0.671	0.967	0.967
107 	 0.0238 	 0.0543 	0.658	0.658	0.957	0.957
108 	 0.0224 	 0.0480 	0.673	0.673	0.956	0.956
109 	 0.0207 	 0.0412 	0.665	0.665	0.963	0.963
110 	 0.0198 	 0.0404 	0.668	0.668	0.957	0.957
111 	 0.0198 	 0.0422 	0.672	0.672	0.957	0.957
112 	 0.0197 	 0.0408 	0.671	0.671	0.963	0.963
113 	 0.0193 	 0.0420 	0.679	0.679	0.957	0.957
114 	 0.0223 	 0.0373 	0.638	0.638	0.968	0.968
115 	 0.0204 	 0.0478 	0.683	0.683	0.947	0.947
116 	 0.0245 	 0.0430 	0.676	0.676	0.947	0.947
117 	 0.0202 	 0.0425 	0.692	0.692	0.940	0.940
118 	 0.0195 	 0.0338 	0.667	0.667	0.972	0.972
119 	 0.0201 	 0.0442 	0.664	0.664	0.961	0.961
120 	 0.0191 	 0.0429 	0.650	0.650	0.966	0.966
121 	 0.0207 	 0.0489 	0.661	0.661	0.963	0.963
122 	 0.0187 	 0.0483 	0.673	0.673	0.958	0.958
123 	 0.0195 	 0.0460 	0.667	0.667	0.964	0.964
124 	 0.0185 	 0.0518 	0.666	0.666	0.956	0.956
125 	 0.0177 	 0.0474 	0.679	0.679	0.955	0.955
126 	 0.0181 	 0.0433 	0.685	0.685	0.962	0.962
127 	 0.0188 	 0.0450 	0.691	0.691	0.963	0.963
128 	 0.0196 	 0.0418 	0.647	0.647	0.964	0.964
129 	 0.0188 	 0.0498 	0.693	0.693	0.951	0.951
130 	 0.0194 	 0.0394 	0.668	0.668	0.968	0.968
131 	 0.0186 	 0.0430 	0.681	0.681	0.962	0.962
132 	 0.0193 	 0.0388 	0.676	0.676	0.966	0.966
133 	 0.0190 	 0.0405 	0.668	0.668	0.956	0.956
134 	 0.0188 	 0.0339 	0.681	0.681	0.973	0.973
135 	 0.0176 	 0.0376 	0.686	0.686	0.968	0.968
136 	 0.0196 	 0.0431 	0.676	0.676	0.966	0.966
137 	 0.0168 	 0.0427 	0.691	0.691	0.958	0.958
138 	 0.0190 	 0.0416 	0.683	0.683	0.963	0.963
139 	 0.0172 	 0.0416 	0.667	0.667	0.971	0.971
140 	 0.0170 	 0.0390 	0.655	0.655	0.975	0.975
141 	 0.0178 	 0.0422 	0.679	0.679	0.962	0.962
142 	 0.0179 	 0.0474 	0.676	0.676	0.956	0.956
143 	 0.0173 	 0.0503 	0.684	0.684	0.948	0.948
144 	 0.0169 	 0.0503 	0.684	0.684	0.952	0.952
145 	 0.0166 	 0.0457 	0.673	0.673	0.959	0.959
146 	 0.0159 	 0.0537 	0.673	0.673	0.954	0.954
147 	 0.0170 	 0.0502 	0.663	0.663	0.963	0.963
148 	 0.0162 	 0.0502 	0.678	0.678	0.958	0.958
149 	 0.0173 	 0.0572 	0.689	0.689	0.953	0.953
150 	 0.0176 	 0.0498 	0.683	0.683	0.954	0.954
151 	 0.0152 	 0.0486 	0.690	0.690	0.955	0.955
152 	 0.0168 	 0.0518 	0.692	0.692	0.949	0.949
153 	 0.0163 	 0.0487 	0.683	0.683	0.961	0.961
154 	 0.0167 	 0.0420 	0.681	0.681	0.966	0.966
155 	 0.0158 	 0.0447 	0.680	0.680	0.957	0.957
156 	 0.0167 	 0.0432 	0.674	0.674	0.962	0.962
157 	 0.0164 	 0.0520 	0.698	0.698	0.944	0.944
158 	 0.0166 	 0.0452 	0.676	0.676	0.951	0.951
159 	 0.0154 	 0.0459 	0.689	0.689	0.959	0.959
160 	 0.0159 	 0.0454 	0.670	0.670	0.955	0.955
161 	 0.0166 	 0.0387 	0.668	0.668	0.969	0.969
162 	 0.0153 	 0.0446 	0.683	0.683	0.962	0.962
163 	 0.0145 	 0.0584 	0.697	0.697	0.948	0.948
164 	 0.0161 	 0.0443 	0.676	0.676	0.969	0.969
165 	 0.0151 	 0.0440 	0.688	0.688	0.966	0.966
166 	 0.0164 	 0.0482 	0.685	0.685	0.960	0.960
167 	 0.0158 	 0.0445 	0.688	0.688	0.968	0.968
168 	 0.0146 	 0.0462 	0.705	0.705	0.962	0.962
169 	 0.0144 	 0.0472 	0.692	0.692	0.964	0.964
170 	 0.0151 	 0.0426 	0.689	0.689	0.965	0.965
171 	 0.0154 	 0.0431 	0.693	0.693	0.966	0.966
172 	 0.0145 	 0.0453 	0.704	0.704	0.962	0.962
173 	 0.0142 	 0.0471 	0.699	0.699	0.961	0.961
174 	 0.0153 	 0.0480 	0.684	0.684	0.963	0.963
175 	 0.0154 	 0.0428 	0.689	0.689	0.966	0.966
176 	 0.0144 	 0.0452 	0.683	0.683	0.964	0.964
177 	 0.0140 	 0.0496 	0.692	0.692	0.953	0.953
178 	 0.0146 	 0.0497 	0.688	0.688	0.957	0.957
179 	 0.0141 	 0.0474 	0.686	0.686	0.965	0.965
180 	 0.0147 	 0.0480 	0.697	0.697	0.958	0.958
181 	 0.0143 	 0.0487 	0.684	0.684	0.962	0.962
182 	 0.0144 	 0.0444 	0.691	0.691	0.963	0.963
183 	 0.0129 	 0.0438 	0.682	0.682	0.966	0.966
184 	 0.0160 	 0.0434 	0.687	0.687	0.965	0.965
185 	 0.0163 	 0.0421 	0.683	0.683	0.964	0.964
186 	 0.0142 	 0.0393 	0.680	0.680	0.965	0.965
187 	 0.0141 	 0.0444 	0.689	0.689	0.958	0.958
188 	 0.0139 	 0.0509 	0.685	0.685	0.953	0.953
189 	 0.0138 	 0.0522 	0.689	0.689	0.949	0.949
190 	 0.0143 	 0.0478 	0.690	0.690	0.956	0.956
191 	 0.0140 	 0.0498 	0.696	0.696	0.954	0.954
192 	 0.0136 	 0.0461 	0.690	0.690	0.960	0.960
193 	 0.0153 	 0.0466 	0.693	0.693	0.955	0.955
194 	 0.0156 	 0.0439 	0.704	0.704	0.940	0.940
195 	 0.0150 	 0.0388 	0.688	0.688	0.964	0.964
196 	 0.0143 	 0.0405 	0.680	0.680	0.964	0.964
197 	 0.0146 	 0.0386 	0.679	0.679	0.965	0.965
198 	 0.0144 	 0.0426 	0.691	0.691	0.958	0.958
199 	 0.0125 	 0.0445 	0.686	0.686	0.958	0.958
200 	 0.0136 	 0.0431 	0.680	0.680	0.963	0.963
201 	 0.0133 	 0.0421 	0.682	0.682	0.963	0.963
202 	 0.0125 	 0.0475 	0.687	0.687	0.957	0.957
203 	 0.0134 	 0.0490 	0.684	0.684	0.957	0.957
204 	 0.0136 	 0.0482 	0.685	0.685	0.951	0.951
205 	 0.0117 	 0.0577 	0.696	0.696	0.934	0.934
206 	 0.0140 	 0.0470 	0.690	0.690	0.944	0.944
207 	 0.0133 	 0.0457 	0.676	0.676	0.957	0.957
208 	 0.0137 	 0.0463 	0.684	0.684	0.950	0.950
209 	 0.0120 	 0.0475 	0.690	0.690	0.948	0.948
210 	 0.0132 	 0.0478 	0.693	0.693	0.943	0.943
211 	 0.0134 	 0.0439 	0.684	0.684	0.957	0.957
212 	 0.0127 	 0.0448 	0.688	0.688	0.957	0.957
213 	 0.0129 	 0.0459 	0.699	0.699	0.956	0.956
214 	 0.0126 	 0.0435 	0.697	0.697	0.952	0.952
215 	 0.0127 	 0.0475 	0.699	0.699	0.952	0.952
216 	 0.0122 	 0.0411 	0.692	0.692	0.965	0.965
217 	 0.0136 	 0.0433 	0.690	0.690	0.960	0.960
218 	 0.0144 	 0.0444 	0.693	0.693	0.956	0.956
219 	 0.0127 	 0.0452 	0.692	0.692	0.960	0.960
220 	 0.0119 	 0.0437 	0.691	0.691	0.961	0.961
221 	 0.0120 	 0.0450 	0.697	0.697	0.952	0.952
222 	 0.0133 	 0.0473 	0.702	0.702	0.950	0.950
223 	 0.0136 	 0.0413 	0.669	0.669	0.960	0.960
224 	 0.0126 	 0.0434 	0.677	0.677	0.954	0.954
225 	 0.0117 	 0.0457 	0.689	0.689	0.953	0.953
226 	 0.0139 	 0.0488 	0.691	0.691	0.957	0.957
227 	 0.0130 	 0.0476 	0.702	0.702	0.945	0.945
228 	 0.0113 	 0.0458 	0.693	0.693	0.951	0.951
229 	 0.0121 	 0.0449 	0.699	0.699	0.950	0.950
230 	 0.0121 	 0.0447 	0.696	0.696	0.959	0.959
231 	 0.0117 	 0.0486 	0.696	0.696	0.954	0.954
232 	 0.0122 	 0.0476 	0.697	0.697	0.945	0.945
233 	 0.0117 	 0.0490 	0.700	0.700	0.946	0.946
234 	 0.0120 	 0.0413 	0.691	0.691	0.955	0.955
235 	 0.0109 	 0.0444 	0.702	0.702	0.950	0.950
236 	 0.0121 	 0.0455 	0.708	0.708	0.951	0.951
237 	 0.0124 	 0.0439 	0.705	0.705	0.949	0.949
238 	 0.0117 	 0.0463 	0.711	0.711	0.949	0.949
239 	 0.0121 	 0.0426 	0.701	0.701	0.956	0.956
240 	 0.0116 	 0.0457 	0.709	0.709	0.952	0.952
241 	 0.0120 	 0.0457 	0.697	0.697	0.953	0.953
242 	 0.0118 	 0.0424 	0.703	0.703	0.953	0.953
243 	 0.0114 	 0.0465 	0.704	0.704	0.943	0.943
244 	 0.0109 	 0.0463 	0.699	0.699	0.948	0.948
245 	 0.0114 	 0.0488 	0.707	0.707	0.945	0.945
246 	 0.0109 	 0.0458 	0.711	0.711	0.943	0.943
247 	 0.0107 	 0.0492 	0.712	0.712	0.945	0.945
248 	 0.0112 	 0.0472 	0.700	0.700	0.946	0.946
249 	 0.0108 	 0.0507 	0.713	0.713	0.941	0.941
250 	 0.0114 	 0.0527 	0.711	0.711	0.944	0.944
251 	 0.0119 	 0.0482 	0.700	0.700	0.953	0.953
252 	 0.0110 	 0.0417 	0.694	0.694	0.959	0.959
253 	 0.0126 	 0.0459 	0.705	0.705	0.946	0.946
254 	 0.0120 	 0.0428 	0.697	0.697	0.957	0.957
255 	 0.0114 	 0.0445 	0.693	0.693	0.948	0.948
256 	 0.0113 	 0.0452 	0.699	0.699	0.947	0.947
257 	 0.0113 	 0.0426 	0.695	0.695	0.956	0.956
258 	 0.0119 	 0.0443 	0.701	0.701	0.950	0.950
259 	 0.0107 	 0.0440 	0.700	0.700	0.949	0.949
260 	 0.0103 	 0.0434 	0.696	0.696	0.946	0.946
261 	 0.0103 	 0.0500 	0.706	0.706	0.943	0.943
262 	 0.0110 	 0.0453 	0.703	0.703	0.947	0.947
263 	 0.0109 	 0.0459 	0.701	0.701	0.943	0.943
264 	 0.0104 	 0.0441 	0.699	0.699	0.950	0.950
265 	 0.0110 	 0.0470 	0.706	0.706	0.942	0.942
266 	 0.0106 	 0.0441 	0.698	0.698	0.948	0.948
267 	 0.0099 	 0.0441 	0.706	0.706	0.946	0.946
268 	 0.0106 	 0.0447 	0.704	0.704	0.950	0.950
269 	 0.0104 	 0.0461 	0.696	0.696	0.949	0.949
270 	 0.0104 	 0.0455 	0.700	0.700	0.946	0.946
271 	 0.0107 	 0.0434 	0.692	0.692	0.952	0.952
272 	 0.0101 	 0.0475 	0.693	0.693	0.944	0.944
273 	 0.0110 	 0.0410 	0.698	0.698	0.950	0.950
