=============training from sratch============
Program ID: 22974

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/training/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/training/labels_npy', '0', '1', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
      Oct 23th, 2019
      1   change to MaxPool with 2*2*2 with stride 2;
      2   Loss use GeneralizedDiceLoss function;
      
      Oct 25th, 2019
      1   add trainAllData switch
      
      Oct 29th, 2019
      1   revise reduce learnging rate at training loss;
      2   reduce min_lr = 1e-9
      
      Nov 1st, 2019
      1   erase batchNorm, for training.
      
      Nov 2nd, 2019
      1   Use InstanceNormalization, instead of BatchNorm
      
      Nov 25th, 2019
      1   Add boundary loss into training.  
      
      Dec 3rd, 2019
      1  cancel Boundary Loss;
      2  add 13 network generated files with accuracy >88% into 3D ground truth, then train;
      
      Dec 7th, 2019
      1  Change latent vector in the V model into 1*1 in feature map size.
      
      Dce 9th, 2019
      1  Instance Norm can not apply for one value per channel, which will make the value =0
      
      
        
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-12-09 16:31:39.314030
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191209_163139

Info: this is the 0th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191209_163139.
All files are in one partition.

all dataset: total 35 image files.
Network has total 155,528,690 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		0.7070		0.27541
5	1.0000e-02		0.5301		0.41125
10	1.0000e-02		0.5057		0.46162
15	1.0000e-02		0.5030		0.45339
20	1.0000e-02		0.4796		0.47725
25	1.0000e-02		0.4519		0.50723
30	1.0000e-02		0.4494		0.51109
35	1.0000e-02		0.4117		0.55612
40	1.0000e-02		0.4532		0.51485
45	1.0000e-02		0.3946		0.57338
50	1.0000e-02		0.4003		0.56398
55	1.0000e-02		0.3593		0.60824
60	1.0000e-02		0.3696		0.60204
65	1.0000e-02		0.3666		0.60054
70	1.0000e-02		0.3464		0.62787
75	1.0000e-02		0.3332		0.63828
80	1.0000e-02		0.3693		0.60456
85	1.0000e-02		0.3466		0.62776
90	1.0000e-02		0.3171		0.66285
95	1.0000e-02		0.2938		0.68636
100	1.0000e-02		0.3299		0.64317
105	1.0000e-02		0.3090		0.66656
110	1.0000e-02		0.3341		0.64572
115	1.0000e-02		0.3454		0.63461
120	1.0000e-02		0.3080		0.67481
125	1.0000e-02		0.2901		0.68891
130	1.0000e-02		0.2863		0.69440
135	1.0000e-02		0.2878		0.69323
140	1.0000e-02		0.2959		0.68757
145	1.0000e-02		0.2814		0.70210
150	1.0000e-02		0.2641		0.71689
155	1.0000e-02		0.2658		0.71445
160	1.0000e-02		0.2511		0.73339
165	1.0000e-02		0.2596		0.72498
170	1.0000e-02		0.2422		0.74433
175	1.0000e-02		0.2836		0.69752
180	1.0000e-02		0.2078		0.78073
185	1.0000e-02		0.2355		0.74763
190	1.0000e-02		0.2438		0.74208
195	1.0000e-02		0.2259		0.76088
200	1.0000e-02		0.3600		0.62434
205	1.0000e-02		0.2273		0.75596
210	1.0000e-02		0.2650		0.71686
215	1.0000e-02		0.2501		0.73610
220	1.0000e-02		0.2381		0.74767
225	1.0000e-02		0.1985		0.78515
230	1.0000e-02		0.2941		0.68938
235	1.0000e-02		0.2037		0.78550
240	1.0000e-02		0.2756		0.70661
245	1.0000e-02		0.2127		0.77690
250	1.0000e-02		0.1871		0.80316
255	1.0000e-02		0.1882		0.79777
260	1.0000e-02		0.2067		0.77486
265	1.0000e-02		0.1868		0.79699
270	1.0000e-02		0.2087		0.77909
275	1.0000e-02		0.1668		0.82393
280	1.0000e-02		0.1885		0.80276
285	1.0000e-02		0.1709		0.81866
290	1.0000e-02		0.2128		0.77444
295	1.0000e-02		0.2070		0.77924
300	1.0000e-02		0.2044		0.78271
305	1.0000e-02		0.1728		0.81541
310	1.0000e-02		0.1726		0.81556
315	1.0000e-02		0.1623		0.82808
320	1.0000e-02		0.1870		0.80128
325	1.0000e-02		0.1957		0.79495
330	1.0000e-02		0.1482		0.84360
335	1.0000e-02		0.1702		0.81756
340	1.0000e-02		0.1632		0.82670
345	1.0000e-02		0.1626		0.82710
350	1.0000e-02		0.1644		0.82467
355	1.0000e-02		0.1581		0.83300
360	1.0000e-02		0.1656		0.82481
365	1.0000e-02		0.1784		0.81053
370	1.0000e-02		0.1705		0.81850
375	1.0000e-02		0.1649		0.82585
380	1.0000e-02		0.1613		0.82867
385	1.0000e-03		0.1679		0.82012
390	1.0000e-03		0.1220		0.87076
395	1.0000e-03		0.1359		0.85671
400	1.0000e-03		0.1252		0.86737
405	1.0000e-03		0.1266		0.86510
410	1.0000e-03		0.1219		0.87124
415	1.0000e-03		0.1220		0.86949
420	1.0000e-03		0.1295		0.86333
425	1.0000e-03		0.1092		0.88460
430	1.0000e-03		0.1125		0.88139
435	1.0000e-03		0.1199		0.87217
440	1.0000e-03		0.1047		0.88902
445	1.0000e-03		0.1168		0.87645
450	1.0000e-03		0.1226		0.87111
455	1.0000e-03		0.1055		0.88879
460	1.0000e-03		0.1153		0.87836
465	1.0000e-03		0.1095		0.88464
470	1.0000e-03		0.1213		0.87186
475	1.0000e-03		0.1117		0.88398
480	1.0000e-03		0.1157		0.87692
485	1.0000e-03		0.1011		0.89324
490	1.0000e-03		0.1107		0.88364
495	1.0000e-03		0.1084		0.88519
500	1.0000e-03		0.0975		0.89653
505	1.0000e-03		0.1015		0.89221
510	1.0000e-03		0.1094		0.88401
515	1.0000e-03		0.1027		0.89142
520	1.0000e-03		0.0999		0.89372
525	1.0000e-03		0.1020		0.89240
530	1.0000e-03		0.0951		0.89968
535	1.0000e-03		0.1057		0.88906
540	1.0000e-03		0.0976		0.89697
545	1.0000e-03		0.0981		0.89654
550	1.0000e-03		0.1001		0.89407
555	1.0000e-03		0.1020		0.89306
560	1.0000e-03		0.0982		0.89625
565	1.0000e-03		0.1126		0.88179
570	1.0000e-03		0.0973		0.89755
575	1.0000e-03		0.0932		0.90185
580	1.0000e-03		0.0953		0.90021
585	1.0000e-03		0.0963		0.89765
590	1.0000e-03		0.0977		0.89683
595	1.0000e-03		0.0966		0.89746
600	1.0000e-03		0.0941		0.90013
605	1.0000e-03		0.0937		0.90076
610	1.0000e-03		0.0890		0.90491
615	1.0000e-03		0.0955		0.89946
620	1.0000e-03		0.0968		0.89791
625	1.0000e-03		0.0973		0.89713
630	1.0000e-03		0.0858		0.90902
635	1.0000e-03		0.0950		0.89993
640	1.0000e-03		0.0835		0.91135
645	1.0000e-03		0.0980		0.89696
650	1.0000e-03		0.0916		0.90219
655	1.0000e-03		0.0870		0.90790
660	1.0000e-03		0.0953		0.89920
665	1.0000e-03		0.0987		0.89565
670	1.0000e-03		0.0941		0.90094
675	1.0000e-03		0.0896		0.90540
680	1.0000e-03		0.0952		0.89939
685	1.0000e-03		0.0915		0.90281
690	1.0000e-03		0.0819		0.91256
695	1.0000e-03		0.0856		0.90899
700	1.0000e-03		0.0965		0.89860
705	1.0000e-03		0.0900		0.90510
710	1.0000e-03		0.0923		0.90183
715	1.0000e-03		0.0841		0.91074
720	1.0000e-03		0.0870		0.90715
725	1.0000e-03		0.0856		0.90901
730	1.0000e-03		0.0844		0.91103
735	1.0000e-03		0.0818		0.91286
740	1.0000e-03		0.0797		0.91596
745	1.0000e-03		0.0878		0.90798
750	1.0000e-03		0.0918		0.90321
755	1.0000e-03		0.0895		0.90605
760	1.0000e-03		0.0842		0.91026
765	1.0000e-03		0.0888		0.90579
770	1.0000e-03		0.0797		0.91662
775	1.0000e-03		0.0825		0.91163
780	1.0000e-03		0.0843		0.91020
785	1.0000e-03		0.0886		0.90586
790	1.0000e-03		0.0821		0.91259
795	1.0000e-04		0.0850		0.90983
800	1.0000e-04		0.0803		0.91502
805	1.0000e-04		0.0866		0.90790
810	1.0000e-04		0.0911		0.90480
815	1.0000e-04		0.0740		0.92141
820	1.0000e-04		0.0816		0.91323
825	1.0000e-04		0.0776		0.91868
830	1.0000e-04		0.0753		0.92024
835	1.0000e-04		0.0791		0.91632
840	1.0000e-04		0.0807		0.91398
845	1.0000e-04		0.0839		0.91071
850	1.0000e-04		0.0831		0.91040
855	1.0000e-04		0.0787		0.91655
860	1.0000e-04		0.0796		0.91553
865	1.0000e-04		0.0806		0.91443
870	1.0000e-05		0.0752		0.92060
875	1.0000e-05		0.0815		0.91361
880	1.0000e-05		0.0809		0.91478
885	1.0000e-05		0.0800		0.91562
890	1.0000e-05		0.0781		0.91758
895	1.0000e-05		0.0819		0.91363
900	1.0000e-05		0.0785		0.91662
905	1.0000e-05		0.0767		0.91830
910	1.0000e-05		0.0804		0.91544
915	1.0000e-05		0.0818		0.91288
920	1.0000e-05		0.0765		0.91909
925	1.0000e-06		0.0802		0.91504
930	1.0000e-06		0.0804		0.91527
935	1.0000e-06		0.0774		0.91847
940	1.0000e-06		0.0799		0.91597
945	1.0000e-06		0.0807		0.91451
950	1.0000e-06		0.0755		0.92002
955	1.0000e-06		0.0797		0.91566
960	1.0000e-06		0.0753		0.92049
965	1.0000e-06		0.0741		0.92141
970	1.0000e-06		0.0782		0.91715
975	1.0000e-06		0.0889		0.90712
980	1.0000e-07		0.0806		0.91478
985	1.0000e-07		0.0771		0.91831
990	1.0000e-07		0.0764		0.91932
995	1.0000e-07		0.0747		0.92051
1000	1.0000e-07		0.0828		0.91260
1005	1.0000e-07		0.0763		0.91894
1010	1.0000e-07		0.0847		0.91062
1015	1.0000e-07		0.0747		0.92109
1020	1.0000e-07		0.0838		0.91129
1025	1.0000e-07		0.0806		0.91494
1030	1.0000e-07		0.0853		0.90932
1035	1.0000e-08		0.0822		0.91316
1040	1.0000e-08		0.0801		0.91554
1045	1.0000e-08		0.0791		0.91619
1050	1.0000e-08		0.0823		0.91325
1055	1.0000e-08		0.0784		0.91657
1060	1.0000e-08		0.0748		0.92058
1065	1.0000e-08		0.0796		0.91579
1070	1.0000e-08		0.0766		0.91988
1075	1.0000e-08		0.0809		0.91467
1080	1.0000e-08		0.0749		0.92072
1085	1.0000e-08		0.0794		0.91629
1090	1.0000e-08		0.0817		0.91343
1095	1.0000e-08		0.0748		0.91999
1100	1.0000e-08		0.0795		0.91627
1105	1.0000e-08		0.0779		0.91762
1110	1.0000e-08		0.0813		0.91366
1115	1.0000e-08		0.0781		0.91785
1120	1.0000e-08		0.0816		0.91352
1125	1.0000e-08		0.0772		0.91831
1130	1.0000e-08		0.0833		0.91158
1135	1.0000e-08		0.0796		0.91565
1140	1.0000e-08		0.0824		0.91195
1145	1.0000e-08		0.0790		0.91647
1150	1.0000e-08		0.0789		0.91646
1155	1.0000e-08		0.0809		0.91443
1160	1.0000e-08		0.0819		0.91334
1165	1.0000e-08		0.0857		0.90912
1170	1.0000e-08		0.0774		0.91799
1175	1.0000e-08		0.0773		0.91853
1180	1.0000e-08		0.0751		0.92026
1185	1.0000e-08		0.0867		0.90836
1190	1.0000e-08		0.0814		0.91351
1195	1.0000e-08		0.0770		0.91880
1200	1.0000e-08		0.0900		0.90517
