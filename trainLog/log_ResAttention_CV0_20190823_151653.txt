=============training from sratch============
Program ID: 23723

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt', '1', '/home/hxie1/data/OvarianCancerCT/pixelSize223/numpy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '0', '2,3']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
            8    at July 30th 03:00, 2019:
                    add learning rate print;
                    use convStride =2;
                    add filter number by 2 times along deeper layers.
            9    at July 30th, 10:13, 2019:
                    add MaxPool2d in stage1;
                    add final filters to 2048.
            10   at July 30th, 15:23, 2019
                    final conv layer filter number: 1024
            11   at Aug 10th, 2019:
                    A. Add new patient data; and exclude non-standard patient data;
                    B. test the k-th fold,  validation on the (k+1)th fold;
                    C. new inputsize: 231*251*251 with pixels size 3*2*2 mm
                    D. window level shresthold [0,300]
                    E. put data padding in to converting from nrrd to numpy;
                    F. Add STN network as first laye of network
                    G. change input data into gaussian normalization in slice by slice; (x-mean)/std;
            12   Aug 12th, 2019
                    A. fix the bug that SITk reads int imape, and normalization into interger error;
                    B  test without SPN.           
            13   Aug 13th, 2019
                    A change input data into Gausssian distribution with non-zero mean,
                      it will make the padding zero do not occupy meaning of gaussian distribution.        
                    B add filters in the ResNeXt network to 128 at first stage, and reduce batchSize to 3. 
                    C Add STN at the beginning of the ResNeXt network
            14   Aug 14th, 2019
                    A. Fix the Bug the STN convert all image into 0 problem;
                    B. change DataTransform: change shear into [-30, 30], and add scale[0.6,1.2]
                    C  put STN at teh begginning of the network;
                    D  change optimizer from SGD to Adam; and change learning rate decay with gamma=0.5 per 20 steps.                                             
            15   Aug 16th, 2019
                    A The affine matrix in the STN is divided by its spectral norm;
                    B All conv layer add spectralNorm, while all FC layer do not add spectral Norm;
                    C reduce initial LR at 0.0001, and decay step to 30.
            16   Aug 17th, 2019
                    A  Cancel STN;
                    B  Add LeakyReLU;
                    C  before final FC layer, add ReLU and LocalResponseNorm   
            17   Aug 17th, 2019:10:44am
                    A enable STN;
                    B STN support LeakyReLU and localResponseNorm  
            18   Aug 17th, 2019 14:38 pm
                    A  del STN at beginining;
                    B  put STN at final 2 layer before FC. 
            19  Aug 17th 18:09 2019
                    A add surgical result file support;                    
            20  Aug 18th 08:00 am 2019
                    A  change all bias in Conv2d, and Linear into True.   
            21  Aug 19th 10:33am 2019
                    A initalize the bias in the final Linear layer as 0.3; (1-2*0.65 = -0.3)
                    16:17 pm:
                    B  add inductive bias 0.3 in the network forward function. 
                    C  initial LR =0.1, with decay steps =30                          
            22  Aug 20th 10:24am 2019
                    A change inductive bias = 0.2;
                    16:29pm:
                    B add modulation factor in the STN
            23  Aug 21th, 10:16, 2019
                    A delete the inductive bias in the final FC.  
                    15:47 pm
                    B change LRscheduler into MultiStepLR;
            24  Aug 22nd, 11:14, 2019
                    A replace ResNeXtBlock with DeformConvBlock in the stage3,4,5.
            25  Agu 23th 10:31, 2019
                    A in stage 3,4,4, reduce DeformConnBlock into 1 block;
                    B reduce the final FC layer width into 512.
                    15:10pm
                    C before regression with bias = 0, nomalize x. 
                            
                    
                          
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   total 220 patients, 5-fold cross validation, test 45, validation 45, and training 130.  

Training strategy: 

          

Program starting Time: 2019-08-23 15:16:53.691022
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt/20190823_151653

Info: this is the 0th fold leave for test in the 5-fold cross-validation.

Infor: In all data of 220 files, label 0 has 77 files,
	  and label 1 has 143 files, where positive response rate = 0.65 in full data
5-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.
training dataset:	 total 130 files, where 1 has 85.0 with rate of 0.6538461538461539
validation dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
test dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
Info: batchSize = 8

=== Network trains from scratch ====
Network has total 45,564,597 parameters.
This predict optimal response.
Info: program will use GPU [2, 3] from all 4 GPUs.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	LearningRate		TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	1.0000e-01		0.5496	0.4846	0.4824	0.4889		0.5042	0.3556	0.0000	1.0000		0.5042	0.3556	0.0000	1.0000
5	1.0000e-01		0.4996	0.5077	0.5176	0.4889		0.4855	0.6444	1.0000	0.0000		0.4861	0.6444	1.0000	0.0000
10	1.0000e-01		0.4825	0.4846	0.4824	0.4889		0.4876	0.3556	0.0000	1.0000		0.4877	0.3556	0.0000	1.0000
15	1.0000e-01		0.4825	0.3769	0.0706	0.9556		0.4801	0.3556	0.0000	1.0000		0.4802	0.3556	0.0000	1.0000
20	1.0000e-01		0.4866	0.5000	0.5882	0.3333		0.4808	0.3556	0.0000	1.0000		0.4808	0.3556	0.0000	1.0000
25	1.0000e-01		0.4903	0.6538	1.0000	0.0000		0.4795	0.6444	1.0000	0.0000		0.4795	0.6444	1.0000	0.0000
30	1.0000e-01		0.4803	0.6538	1.0000	0.0000		0.4796	0.6444	1.0000	0.0000		0.4796	0.6444	1.0000	0.0000
35	1.0000e-01		0.4884	0.5308	0.6588	0.2889		0.4818	0.3556	0.0000	1.0000		0.4818	0.3556	0.0000	1.0000
40	1.0000e-01		0.4912	0.4385	0.3647	0.5778		0.4794	0.6444	1.0000	0.0000		0.4795	0.6444	1.0000	0.0000
45	1.0000e-01		0.4940	0.5923	0.8588	0.0889		0.4800	0.6444	1.0000	0.0000		0.4800	0.6444	1.0000	0.0000
50	1.0000e-02		0.4896	0.6538	1.0000	0.0000		0.4800	0.6444	1.0000	0.0000		0.4800	0.6444	1.0000	0.0000
55	1.0000e-02		0.4791	0.6538	1.0000	0.0000		0.4794	0.6444	1.0000	0.0000		0.4794	0.6444	1.0000	0.0000
60	1.0000e-02		0.4864	0.6462	0.9765	0.0222		0.4798	0.6444	1.0000	0.0000		0.4798	0.6444	1.0000	0.0000
65	1.0000e-02		0.4789	0.6538	1.0000	0.0000		0.4795	0.6444	1.0000	0.0000		0.4795	0.6444	1.0000	0.0000
70	1.0000e-02		0.4936	0.6538	1.0000	0.0000		0.4797	0.6444	1.0000	0.0000		0.4797	0.6444	1.0000	0.0000
75	1.0000e-02		0.4866	0.5615	0.7882	0.1333		0.4796	0.6444	1.0000	0.0000		0.4796	0.6444	1.0000	0.0000
80	1.0000e-02		0.4798	0.3462	0.3294	0.3778		0.4799	0.3556	0.0000	1.0000		0.4799	0.3556	0.0000	1.0000
85	1.0000e-02		0.4790	0.6538	1.0000	0.0000		0.4796	0.6444	1.0000	0.0000		0.4796	0.6444	1.0000	0.0000
90	1.0000e-02		0.4789	0.6538	1.0000	0.0000		0.4796	0.6444	1.0000	0.0000		0.4796	0.6444	1.0000	0.0000
95	1.0000e-02		0.4790	0.6538	1.0000	0.0000		0.4798	0.6444	1.0000	0.0000		0.4798	0.6444	1.0000	0.0000
100	1.0000e-02		0.4787	0.6538	1.0000	0.0000		0.4794	0.6444	1.0000	0.0000		0.4794	0.6444	1.0000	0.0000
105	1.0000e-02		0.4865	0.6538	1.0000	0.0000		0.4795	0.6444	1.0000	0.0000		0.4795	0.6444	1.0000	0.0000
110	1.0000e-02		0.4793	0.6538	1.0000	0.0000		0.4796	0.6444	1.0000	0.0000		0.4796	0.6444	1.0000	0.0000
115	1.0000e-02		0.4863	0.6538	1.0000	0.0000		0.4796	0.6444	1.0000	0.0000		0.4796	0.6444	1.0000	0.0000
120	1.0000e-02		0.4790	0.6538	1.0000	0.0000		0.4796	0.6444	1.0000	0.0000		0.4796	0.6444	1.0000	0.0000
125	1.0000e-02		0.4867	0.6538	1.0000	0.0000		0.4798	0.6444	1.0000	0.0000		0.4798	0.6444	1.0000	0.0000
130	1.0000e-02		0.4795	0.3462	0.0000	1.0000		0.4801	0.3556	0.0000	1.0000		0.4801	0.3556	0.0000	1.0000
135	1.0000e-02		0.4866	0.6538	1.0000	0.0000		0.4794	0.6444	1.0000	0.0000		0.4794	0.6444	1.0000	0.0000
140	1.0000e-02		0.4788	0.6538	1.0000	0.0000		0.4795	0.6444	1.0000	0.0000		0.4795	0.6444	1.0000	0.0000
145	1.0000e-02		0.4791	0.6538	1.0000	0.0000		0.4797	0.6444	1.0000	0.0000		0.4797	0.6444	1.0000	0.0000
150	1.0000e-03		0.4791	0.5846	0.8000	0.1778		0.4798	0.6444	0.9655	0.0625		0.4798	0.6444	1.0000	0.0000
155	1.0000e-03		0.4789	0.6462	0.9412	0.0889		0.4798	0.6222	0.8966	0.1250		0.4799	0.6222	0.9310	0.0625
160	1.0000e-03		0.4860	0.6538	1.0000	0.0000		0.4797	0.6444	1.0000	0.0000		0.4798	0.6444	1.0000	0.0000
165	1.0000e-03		0.4788	0.6615	0.9882	0.0444		0.4798	0.6444	1.0000	0.0000		0.4798	0.6444	1.0000	0.0000
170	1.0000e-03		0.4856	0.6385	0.8941	0.1556		0.4799	0.5778	0.7586	0.2500		0.4800	0.6000	0.8966	0.0625
175	1.0000e-03		0.4794	0.5000	0.5529	0.4000		0.4800	0.5333	0.6552	0.3125		0.4803	0.5778	0.7931	0.1875
180	1.0000e-03		0.4852	0.6615	0.7765	0.4444		0.4799	0.5333	0.6897	0.2500		0.4800	0.5778	0.8621	0.0625
185	1.0000e-03		0.4939	0.4538	0.3765	0.6000		0.4798	0.5333	0.6207	0.3750		0.4801	0.4444	0.4483	0.4375
190	1.0000e-03		0.4901	0.5154	0.4353	0.6667		0.4799	0.5333	0.5862	0.4375		0.4806	0.4889	0.5862	0.3125
195	1.0000e-03		0.4846	0.5462	0.4824	0.6667		0.4807	0.5111	0.5172	0.5000		0.4803	0.4889	0.5517	0.3750
200	1.0000e-03		0.4893	0.6308	0.6471	0.6000		0.4797	0.5111	0.6552	0.2500		0.4812	0.6000	0.7931	0.2500
205	1.0000e-03		0.4779	0.5769	0.4941	0.7333		0.4784	0.5556	0.6207	0.4375		0.4800	0.5333	0.5862	0.4375
210	1.0000e-03		0.4759	0.5769	0.5176	0.6889		0.4863	0.4222	0.2759	0.6875		0.4854	0.4000	0.2759	0.6250
215	1.0000e-03		0.4874	0.5000	0.3529	0.7778		0.4867	0.4000	0.2759	0.6250		0.4896	0.3556	0.2759	0.5000
220	1.0000e-03		0.4704	0.5923	0.5294	0.7111		0.4829	0.4889	0.5517	0.3750		0.4843	0.5111	0.5517	0.4375
225	1.0000e-03		0.4738	0.5923	0.6118	0.5556		0.4911	0.4444	0.5172	0.3125		0.4809	0.5778	0.7586	0.2500
230	1.0000e-03		0.4697	0.5769	0.5412	0.6444		0.4902	0.4889	0.5862	0.3125		0.4950	0.4889	0.6552	0.1875
235	1.0000e-03		0.4505	0.6538	0.6471	0.6667		0.4931	0.4889	0.5517	0.3750		0.4873	0.5333	0.5862	0.4375
240	1.0000e-03		0.4429	0.6462	0.6235	0.6889		0.4879	0.5778	0.6897	0.3750		0.4895	0.5778	0.7586	0.2500
245	1.0000e-03		0.4506	0.6077	0.5294	0.7556		0.4922	0.5333	0.5517	0.5000		0.5049	0.4889	0.5172	0.4375
250	1.0000e-03		0.4313	0.6846	0.6000	0.8444		0.4893	0.5333	0.5172	0.5625		0.5058	0.5333	0.5172	0.5625
255	1.0000e-03		0.4241	0.7000	0.7647	0.5778		0.4915	0.5778	0.6897	0.3750		0.5061	0.5556	0.6552	0.3750
260	1.0000e-03		0.4470	0.6846	0.6824	0.6889		0.5112	0.5556	0.7931	0.1250		0.4985	0.5333	0.7586	0.1250
265	1.0000e-03		0.4465	0.6385	0.6235	0.6667		0.5055	0.5778	0.6552	0.4375		0.5112	0.5333	0.5172	0.5625
270	1.0000e-03		0.3491	0.7923	0.8118	0.7556		0.4965	0.5556	0.6897	0.3125		0.5441	0.4889	0.5862	0.3125
275	1.0000e-03		0.3764	0.7154	0.7059	0.7333		0.5272	0.5556	0.7241	0.2500		0.5325	0.6000	0.7931	0.2500
280	1.0000e-03		0.3232	0.8077	0.8235	0.7778		0.5366	0.6000	0.6897	0.4375		0.5788	0.5111	0.5862	0.3750
285	1.0000e-03		0.3229	0.7769	0.7059	0.9111		0.5856	0.5333	0.4483	0.6875		0.6709	0.4000	0.3448	0.5000
290	1.0000e-03		0.2936	0.7692	0.6941	0.9111		0.5860	0.5333	0.6897	0.2500		0.6021	0.4889	0.6207	0.2500
295	1.0000e-03		0.2208	0.9077	0.9176	0.8889		0.5908	0.5778	0.6552	0.4375		0.6463	0.4889	0.5172	0.4375
300	1.0000e-04		0.3523	0.7615	0.7647	0.7556		0.6117	0.5778	0.6897	0.3750		0.7389	0.4000	0.4138	0.3750
305	1.0000e-04		0.2547	0.8615	0.8824	0.8222		0.5862	0.6222	0.7931	0.3125		0.6622	0.4889	0.5862	0.3125
310	1.0000e-04		0.2338	0.8769	0.9059	0.8222		0.5907	0.5556	0.6207	0.4375		0.6807	0.4889	0.5862	0.3125
315	1.0000e-04		0.2677	0.8692	0.8941	0.8222		0.5990	0.5778	0.6897	0.3750		0.6651	0.4444	0.5172	0.3125
320	1.0000e-04		0.1832	0.8846	0.8706	0.9111		0.6251	0.6222	0.7931	0.3125		0.6684	0.5333	0.6552	0.3125
325	1.0000e-04		0.1967	0.9000	0.9176	0.8667		0.6388	0.5778	0.6897	0.3750		0.6866	0.4667	0.5517	0.3125
330	1.0000e-04		0.1449	0.9385	0.9529	0.9111		0.6471	0.5778	0.7586	0.2500		0.6887	0.5111	0.6552	0.2500
335	1.0000e-04		0.2091	0.8923	0.8941	0.8889		0.6690	0.6000	0.7931	0.2500		0.6608	0.6000	0.7931	0.2500
340	1.0000e-04		0.2271	0.8769	0.9059	0.8222		0.6141	0.6000	0.7586	0.3125		0.6806	0.4889	0.5862	0.3125
345	1.0000e-04		0.2135	0.8923	0.9176	0.8444		0.6289	0.6222	0.7586	0.3750		0.6795	0.4889	0.6207	0.2500
350	1.0000e-04		0.2095	0.9000	0.9059	0.8889		0.6613	0.5778	0.7586	0.2500		0.6608	0.4889	0.5862	0.3125
355	1.0000e-04		0.1766	0.9231	0.9176	0.9333		0.6533	0.5778	0.7586	0.2500		0.6828	0.4667	0.5862	0.2500
360	1.0000e-04		0.2062	0.8769	0.9176	0.8000		0.6571	0.6000	0.7586	0.3125		0.7028	0.4444	0.5517	0.2500
365	1.0000e-04		0.1463	0.9308	0.9176	0.9556		0.6553	0.5778	0.7241	0.3125		0.7058	0.5111	0.6207	0.3125
370	1.0000e-04		0.1585	0.8923	0.9059	0.8667		0.6790	0.5556	0.6897	0.3125		0.7230	0.4667	0.5172	0.3750
375	1.0000e-04		0.1826	0.9231	0.9294	0.9111		0.6995	0.5333	0.6897	0.2500		0.7057	0.5111	0.6207	0.3125
380	1.0000e-04		0.1712	0.9154	0.9294	0.8889		0.7094	0.6000	0.7241	0.3750		0.7175	0.4667	0.5862	0.2500
385	1.0000e-04		0.1493	0.9385	0.9294	0.9556		0.6847	0.6000	0.7586	0.3125		0.7167	0.4889	0.5862	0.3125
390	1.0000e-04		0.1446	0.9385	0.9412	0.9333		0.6760	0.6222	0.7931	0.3125		0.7076	0.5111	0.6207	0.3125
395	1.0000e-04		0.1533	0.9308	0.9176	0.9556		0.7133	0.6000	0.7931	0.2500		0.7114	0.4889	0.5862	0.3125
400	1.0000e-04		0.1112	0.9692	0.9529	1.0000		0.7474	0.5556	0.7241	0.2500		0.7262	0.4667	0.5862	0.2500
405	1.0000e-04		0.1742	0.9154	0.9294	0.8889		0.7511	0.5556	0.7241	0.2500		0.7351	0.5333	0.6552	0.3125
410	1.0000e-04		0.1256	0.9692	0.9765	0.9556		0.7418	0.5556	0.7241	0.2500		0.7225	0.5111	0.6207	0.3125
415	1.0000e-04		0.1364	0.9308	0.9412	0.9111		0.6820	0.6000	0.7586	0.3125		0.7266	0.5111	0.6207	0.3125
420	1.0000e-04		0.1262	0.9615	0.9529	0.9778		0.7213	0.5778	0.7241	0.3125		0.7540	0.5111	0.6207	0.3125
425	1.0000e-04		0.1472	0.9308	0.9412	0.9111		0.7478	0.6222	0.8276	0.2500		0.7431	0.5556	0.7241	0.2500
430	1.0000e-04		0.1452	0.9308	0.9412	0.9111		0.7543	0.5778	0.7241	0.3125		0.7476	0.4889	0.5862	0.3125
435	1.0000e-04		0.1218	0.9692	0.9647	0.9778		0.7230	0.5778	0.7241	0.3125		0.7420	0.5333	0.6207	0.3750
440	1.0000e-04		0.1153	0.9692	0.9765	0.9556		0.7505	0.5556	0.6897	0.3125		0.7303	0.4889	0.5517	0.3750
445	1.0000e-04		0.1293	0.9231	0.9294	0.9111		0.7545	0.5556	0.6897	0.3125		0.7591	0.4889	0.5517	0.3750
450	1.0000e-04		0.1392	0.9538	0.9765	0.9111		0.7969	0.5778	0.7241	0.3125		0.7245	0.4444	0.5517	0.2500
455	1.0000e-04		0.0935	0.9615	0.9529	0.9778		0.7911	0.5778	0.7241	0.3125		0.7566	0.5111	0.6207	0.3125
460	1.0000e-04		0.1127	0.9615	0.9647	0.9556		0.8043	0.5778	0.6897	0.3750		0.7666	0.5111	0.6207	0.3125
465	1.0000e-04		0.1288	0.9385	0.9412	0.9333		0.7927	0.6000	0.7241	0.3750		0.7640	0.5111	0.6207	0.3125
470	1.0000e-04		0.1334	0.9615	0.9765	0.9333		0.8092	0.5778	0.6897	0.3750		0.7484	0.4889	0.5862	0.3125
475	1.0000e-04		0.1443	0.9615	0.9647	0.9556		0.7823	0.5778	0.7241	0.3125		0.7533	0.5111	0.5862	0.3750
480	1.0000e-04		0.1135	0.9538	0.9529	0.9556		0.7907	0.5556	0.6897	0.3125		0.7718	0.5333	0.6207	0.3750
485	1.0000e-04		0.0841	0.9769	0.9765	0.9778		0.8237	0.5556	0.6207	0.4375		0.7938	0.5111	0.6207	0.3125
490	1.0000e-04		0.1145	0.9538	0.9765	0.9111		0.8088	0.5333	0.6207	0.3750		0.8126	0.4889	0.5517	0.3750
495	1.0000e-04		0.0727	0.9769	0.9765	0.9778		0.8135	0.5778	0.7241	0.3125		0.7996	0.5111	0.6207	0.3125
500	1.0000e-04		0.0734	0.9769	0.9882	0.9556		0.8553	0.5556	0.6897	0.3125		0.7759	0.4889	0.5862	0.3125
505	1.0000e-04		0.0870	0.9692	0.9647	0.9778		0.8383	0.5778	0.6897	0.3750		0.8098	0.4889	0.5517	0.3750
510	1.0000e-04		0.1273	0.9385	0.9412	0.9333		0.8228	0.5556	0.6897	0.3125		0.8227	0.5333	0.6207	0.3750
515	1.0000e-04		0.1299	0.9231	0.9412	0.8889		0.7951	0.5556	0.6897	0.3125		0.8124	0.5333	0.6207	0.3750
520	1.0000e-04		0.0981	0.9692	0.9765	0.9556		0.8700	0.5556	0.6897	0.3125		0.7351	0.5111	0.6552	0.2500
525	1.0000e-04		0.0839	0.9538	0.9294	1.0000		0.8634	0.5778	0.7241	0.3125		0.7939	0.5111	0.6207	0.3125
530	1.0000e-04		0.0701	0.9846	0.9765	1.0000		0.8784	0.5556	0.6897	0.3125		0.7588	0.5333	0.6207	0.3750
535	1.0000e-04		0.0443	1.0000	1.0000	1.0000		0.8899	0.5556	0.6897	0.3125		0.7400	0.5111	0.6207	0.3125
540	1.0000e-04		0.0948	0.9462	0.9765	0.8889		0.8865	0.5778	0.6897	0.3750		0.7519	0.5333	0.6207	0.3750
545	1.0000e-04		0.0965	0.9538	0.9412	0.9778		0.8999	0.5556	0.6897	0.3125		0.7698	0.5556	0.6552	0.3750
550	1.0000e-04		0.0876	0.9615	0.9647	0.9556		0.8979	0.5333	0.6552	0.3125		0.8029	0.5333	0.6207	0.3750
555	1.0000e-04		0.1444	0.9385	0.9647	0.8889		0.9098	0.5111	0.5862	0.3750		0.7261	0.5778	0.6897	0.3750
560	1.0000e-04		0.0904	0.9615	0.9529	0.9778		0.9178	0.5111	0.5862	0.3750		0.7482	0.5333	0.5862	0.4375
565	1.0000e-04		0.0790	0.9846	0.9765	1.0000		0.9021	0.5778	0.6897	0.3750		0.7666	0.5333	0.6207	0.3750
570	1.0000e-04		0.0898	0.9538	0.9529	0.9556		0.8739	0.5333	0.6207	0.3750		0.8536	0.5111	0.5862	0.3750
575	1.0000e-04		0.0475	0.9846	0.9882	0.9778		0.9003	0.5333	0.7241	0.1875		0.7738	0.6222	0.8276	0.2500
580	1.0000e-04		0.0713	0.9769	0.9765	0.9778		0.9137	0.5556	0.6897	0.3125		0.7716	0.5778	0.6897	0.3750
585	1.0000e-04		0.0491	0.9923	0.9882	1.0000		0.9065	0.5556	0.6897	0.3125		0.8037	0.5333	0.6207	0.3750
590	1.0000e-04		0.0705	0.9692	0.9765	0.9556		0.9396	0.5333	0.6552	0.3125		0.8237	0.5111	0.5862	0.3750
595	1.0000e-04		0.0848	0.9769	0.9882	0.9556		0.9373	0.5556	0.6897	0.3125		0.7880	0.5556	0.6552	0.3750
600	1.0000e-04		0.0448	0.9846	0.9882	0.9778		0.9464	0.5111	0.6207	0.3125		0.8119	0.5333	0.5862	0.4375
605	1.0000e-04		0.0419	1.0000	1.0000	1.0000		0.9668	0.4889	0.5517	0.3750		0.7489	0.5333	0.5172	0.5625
610	1.0000e-04		0.0511	0.9846	0.9882	0.9778		0.9215	0.5556	0.6552	0.3750		0.8086	0.5333	0.6207	0.3750
615	1.0000e-04		0.0463	0.9769	0.9882	0.9556		1.0155	0.5333	0.6207	0.3750		0.8173	0.5111	0.5172	0.5000
620	1.0000e-04		0.0484	0.9923	0.9882	1.0000		1.0313	0.5556	0.6552	0.3750		0.7593	0.5556	0.5862	0.5000
625	1.0000e-04		0.0584	0.9692	0.9765	0.9556		0.9591	0.5778	0.7241	0.3125		0.8212	0.5778	0.6897	0.3750
630	1.0000e-04		0.0493	0.9923	1.0000	0.9778		0.9724	0.5556	0.6897	0.3125		0.7695	0.5778	0.6897	0.3750
635	1.0000e-04		0.0704	0.9769	0.9882	0.9556		0.9831	0.5333	0.6552	0.3125		0.7842	0.5333	0.6207	0.3750
640	1.0000e-04		0.0573	0.9846	0.9882	0.9778		0.9970	0.5333	0.6552	0.3125		0.8195	0.5111	0.5862	0.3750
645	1.0000e-04		0.0685	0.9846	0.9882	0.9778		0.9582	0.5556	0.6897	0.3125		0.8603	0.5333	0.6207	0.3750
650	1.0000e-04		0.0775	0.9692	0.9647	0.9778		1.0352	0.5556	0.6552	0.3750		0.7933	0.4889	0.5862	0.3125
655	1.0000e-04		0.0852	0.9846	0.9882	0.9778		1.0068	0.5778	0.7241	0.3125		0.8077	0.5333	0.6207	0.3750
660	1.0000e-04		0.0562	0.9692	0.9647	0.9778		0.9954	0.5556	0.6552	0.3750		0.8802	0.5333	0.6207	0.3750
665	1.0000e-04		0.0462	0.9846	1.0000	0.9556		1.0408	0.5333	0.6552	0.3125		0.8267	0.5333	0.5862	0.4375
670	1.0000e-04		0.1133	0.9308	0.9412	0.9111		0.9971	0.5333	0.6552	0.3125		0.9328	0.5556	0.6207	0.4375
675	1.0000e-04		0.0930	0.9615	0.9647	0.9556		1.0021	0.5778	0.7586	0.2500		0.8091	0.5111	0.6207	0.3125
680	1.0000e-04		0.0472	0.9769	0.9765	0.9778		0.9705	0.6222	0.7586	0.3750		0.9362	0.5556	0.6207	0.4375
685	1.0000e-04		0.0438	0.9846	0.9882	0.9778		1.0507	0.5333	0.6897	0.2500		0.8272	0.5333	0.6552	0.3125
690	1.0000e-04		0.0494	0.9692	0.9529	1.0000		1.0655	0.4889	0.6552	0.1875		0.8130	0.6222	0.7931	0.3125
695	1.0000e-04		0.0718	0.9846	0.9882	0.9778		0.9642	0.6000	0.7241	0.3750		0.9385	0.5111	0.5862	0.3750
700	1.0000e-04		0.0414	0.9846	0.9882	0.9778		1.0476	0.5333	0.6552	0.3125		0.8886	0.5111	0.6207	0.3125
705	1.0000e-04		0.0540	0.9846	0.9882	0.9778		1.0942	0.4889	0.6552	0.1875		0.8636	0.5556	0.6897	0.3125
710	1.0000e-04		0.0503	0.9923	1.0000	0.9778		1.0141	0.4889	0.5517	0.3750		0.9890	0.4889	0.5172	0.4375
715	1.0000e-04		0.0450	0.9846	0.9882	0.9778		1.0874	0.5333	0.6552	0.3125		0.9023	0.5333	0.6207	0.3750
720	1.0000e-04		0.0706	0.9769	0.9882	0.9556		1.0532	0.5778	0.7586	0.2500		0.9507	0.5333	0.6207	0.3750
725	1.0000e-04		0.0689	0.9846	1.0000	0.9556		1.0761	0.5556	0.7241	0.2500		0.9255	0.5333	0.6552	0.3125
730	1.0000e-04		0.0342	0.9923	0.9882	1.0000		1.0962	0.5111	0.6897	0.1875		0.8798	0.5556	0.6552	0.3750
735	1.0000e-04		0.0710	0.9692	0.9647	0.9778		1.0985	0.4889	0.6552	0.1875		0.8870	0.5778	0.7241	0.3125
740	1.0000e-04		0.0440	0.9846	0.9882	0.9778		1.0682	0.5333	0.6897	0.2500		0.9619	0.5333	0.6207	0.3750
745	1.0000e-04		0.0277	0.9923	0.9882	1.0000		1.0708	0.5333	0.6552	0.3125		0.9480	0.5333	0.6207	0.3750
750	1.0000e-04		0.0372	0.9769	0.9765	0.9778		1.0976	0.5333	0.6552	0.3125		0.9022	0.5333	0.6552	0.3125
755	1.0000e-04		0.0751	0.9692	0.9765	0.9556		1.0681	0.5556	0.7241	0.2500		0.9421	0.5111	0.6207	0.3125
760	1.0000e-04		0.0340	0.9923	0.9882	1.0000		1.0886	0.6000	0.7931	0.2500		0.9302	0.5111	0.6207	0.3125
765	1.0000e-04		0.0390	0.9923	0.9882	1.0000		1.1436	0.4889	0.6552	0.1875		0.8273	0.6222	0.7931	0.3125
770	1.0000e-04		0.0286	0.9923	0.9882	1.0000		1.1082	0.5333	0.6552	0.3125		0.9006	0.5333	0.6207	0.3750
775	1.0000e-04		0.0606	0.9769	0.9882	0.9556		1.1155	0.5111	0.6207	0.3125		0.9210	0.5333	0.6207	0.3750
780	1.0000e-04		0.0305	0.9923	0.9882	1.0000		1.0871	0.5556	0.7241	0.2500		0.9730	0.5111	0.6207	0.3125
785	1.0000e-04		0.0448	0.9769	0.9765	0.9778		1.0972	0.5556	0.7241	0.2500		0.9655	0.5333	0.6207	0.3750
790	1.0000e-04		0.0444	0.9846	0.9882	0.9778		1.1181	0.5111	0.6207	0.3125		0.8729	0.5111	0.6207	0.3125
795	1.0000e-04		0.0595	0.9923	1.0000	0.9778		1.1206	0.5333	0.6897	0.2500		0.8980	0.5556	0.6897	0.3125
800	1.0000e-04		0.0542	0.9846	0.9882	0.9778		1.1181	0.5333	0.6207	0.3750		0.8538	0.5111	0.5517	0.4375
805	1.0000e-04		0.0428	0.9769	0.9765	0.9778		1.0944	0.5111	0.6552	0.2500		0.9009	0.5333	0.6207	0.3750
810	1.0000e-04		0.0244	1.0000	1.0000	1.0000		1.0944	0.6000	0.7931	0.2500		0.9322	0.6000	0.7586	0.3125
815	1.0000e-04		0.0255	1.0000	1.0000	1.0000		1.1089	0.5556	0.7241	0.2500		0.9296	0.5778	0.6897	0.3750
820	1.0000e-04		0.0792	0.9692	0.9647	0.9778		1.1053	0.5111	0.5862	0.3750		0.9939	0.4889	0.5172	0.4375
825	1.0000e-04		0.0377	0.9769	0.9765	0.9778		1.1345	0.5111	0.6552	0.2500		0.9047	0.5333	0.6207	0.3750
830	1.0000e-04		0.0621	0.9769	0.9765	0.9778		1.1547	0.4889	0.6207	0.2500		0.8930	0.5111	0.5517	0.4375
835	1.0000e-04		0.0710	0.9769	0.9882	0.9556		1.1574	0.4667	0.5517	0.3125		0.9633	0.5111	0.5862	0.3750
840	1.0000e-04		0.0204	1.0000	1.0000	1.0000		1.0979	0.5556	0.6552	0.3750		1.0312	0.5111	0.5862	0.3750
845	1.0000e-04		0.0473	0.9769	0.9765	0.9778		1.1635	0.5111	0.6207	0.3125		0.9298	0.5333	0.6207	0.3750
850	1.0000e-04		0.0186	1.0000	1.0000	1.0000		1.1514	0.5111	0.5862	0.3750		0.9876	0.5111	0.5862	0.3750


 training loss less than 0.02, Program exit.


=============END of Training of ResAttentionNet Predict Model =================

Program Ending Time: 2019-08-24 08:19:52.312524
