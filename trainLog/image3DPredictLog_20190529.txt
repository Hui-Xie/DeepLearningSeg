Program ID of Prdict Network training:28361

Program command: 
 ['TrainPredictNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/Predict', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainImages', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/testImages', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/latent/patientResponseDict.json', 'image3d']

Major program changes: 
                      the nunmber of filters in 1st layer in V model = 96
                      latent Vector size: 1536*51*49 (featureMap* slices * axisPlaneLatentVector)
                      PredictModel is convsDenseModule+FC network.
                      there total 162 patient data, in which, 130 with smaller patientId as training data, 
                                                          and 32 with bigger patientID as test data

Experiment setting for Latent to response:
Input: 1536*51*49 Tensor as latent vector,
       where 1536 is the  number of filter at the bottleneck of V model, 
             51 is the number of slices of ROI CT image with size 51*281*281 for input to V model, 
             49 =7*7 is the flatted feature map for each filter.

Predictive Model: 1,  first 4-layer dense conv block reducing feature space into 768 with tensor size 768*51*49 
                  2,  and 4 dense conv blocks each of which includes a stride 2 conv and 4-layers dense conv block; now the the tensor is with size 48*2*2
                  3,  and a simple conv-batchNorm-Relu layer with filter size(2,2) change the tensor with size of  48*1;
                  4,  and 2 fully connected layers  changes the tensor into size 2*1;
                  5  final a softmax for binary classification;
                  Total network learning parameters are 29 millions.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/LatentPredictModel.py

Loss Function:   Cross Entropy with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.

Data:            training data has 130 patients, and test data has 32 patients with training/test rate 80/20.

Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.
                                                          
                                                         
Experiment setting for Image3d to response:
Input: 73*141*141 sacled raw image 
       
Predictive Model: 1,  first 3-layer dense conv block with channel size 64.
                  2,  and 6 dense conv DownBB blocks,  each of which includes a stride 2 conv and 4-layers dense conv block; 
                  3,  and 2 fully connected layers  changes the tensor into size 2*1;
                  4,  final a softmax for binary classification;
                  Total network learning parameters are 29 millions.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/Image3dPredictModel.py

Loss Function:   Cross Entropy with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.

Data:            training data has 130 patients, and test data has 32 patients with training/test rate 80/20.

Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.                      

            

Program starting Time: 2019-05-29 17:08:35.424242
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/Predict

Now program get 130 input files.
Now program get 32 input files.
TrainData Input:  batchSize=4, depth=73, height=141, width=141, NumClassfication=2

TestData Input:  batchSize=4, depth=73, height=141, width=141, NumClassfication=2

Info: program uses Mixup with alpha=0.4, and mixupProb = 0.5.
Network trains from scratch.
Network has total 1,568,514 parameters.
Infor: Cross Entropy Weight: [3.3333333333333335, 1.4285714285714286] for label[0, 1]
Info: program will use 4 GPUs.
Loss Functions List: FocalCELoss with weight of 1; 
Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 


Epoch		TrLoss	TrainAccuracy	TsLoss	TestAccuracy
0		0.1731	0.672		0.1664	1.000
1		0.1723	0.694		0.1663	1.000
2		0.1732	0.706		0.1667	0.500
3		0.1743	0.661		0.1673	0.750
4		0.1734	0.679		0.1679	1.000
5		0.1724	0.762		0.1679	1.000
6		0.1725	0.696		0.1677	0.750
7		0.1732	0.691		0.1674	0.500
8		0.1727	0.633		0.1678	0.750
9		0.1737	0.671		0.1682	1.000
10		0.1724	0.724		0.1678	0.750
11		0.1720	0.721		0.1677	0.750
12		0.1746	0.625		0.1686	0.750
13		0.1733	0.609		0.1683	0.000
14		0.1730	0.654		0.1691	0.750
15		0.1720	0.647		0.1685	1.000
16		0.1733	0.645		0.1683	0.250
17		0.1740	0.667		0.1690	0.750
18		0.1729	0.694		0.1695	1.000
19		0.1728	0.667		0.1692	0.750
20		0.1715	0.734		0.1679	0.750
21		0.1738	0.625		0.1684	1.000
22		0.1737	0.661		0.1688	0.250
23		0.1721	0.650		0.1683	0.250
24		0.1731	0.662		0.1685	0.000
25		0.1733	0.618		0.1690	1.000
26		0.1730	0.675		0.1689	1.000
27		0.1730	0.658		0.1684	0.750
28		0.1739	0.632		0.1693	0.750
29		0.1722	0.705		0.1690	0.500
30		0.1724	0.750		0.1682	0.750
31		0.1712	0.721		0.1681	1.000
32		0.1728	0.645		0.1673	0.750
33		0.1749	0.569		0.1682	0.750
34		0.1726	0.733		0.1681	0.500
35		0.1719	0.656		0.1676	0.750
36		0.1735	0.683		0.1680	0.750
37		0.1732	0.672		0.1684	0.750
38		0.1724	0.750		0.1677	0.500
39		0.1723	0.708		0.1677	0.500
40		0.1722	0.683		0.1676	0.750
41		0.1721	0.706		0.1670	0.750
42		0.1751	0.617		0.1676	0.500
43		0.1714	0.662		0.1674	0.250
44		0.1725	0.719		0.1676	0.750
45		0.1741	0.667		0.1678	0.000
46		0.1722	0.700		0.1678	0.750
47		0.1725	0.697		0.1675	0.750
48		0.1733	0.719		0.1682	0.500
49		0.1732	0.700		0.1679	0.500
50		0.1727	0.750		0.1683	0.500
51		0.1730	0.706		0.1686	0.500
52		0.1737	0.643		0.1685	0.750
53		0.1732	0.625		0.1689	1.000
54		0.1728	0.662		0.1684	0.000
55		0.1734	0.717		0.1693	1.000
56		0.1724	0.708		0.1683	1.000
57		0.1736	0.589		0.1688	0.750
58		0.1734	0.647		0.1696	0.750
59		0.1732	0.681		0.1687	0.750
60		0.1735	0.639		0.1696	1.000
61		0.1730	0.647		0.1695	0.750
62		0.1725	0.700		0.1689	0.000
