=============training from sratch============
Program ID: 23773

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy', '4', '2', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
      Oct 23th, 2019
      1   change to MaxPool with 2*2*2 with stride 2;
      2   Loss use GeneralizedDiceLoss function;
        
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-10-24 11:12:30.410367
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191024_111230

Info: this is the 4th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191024_111230.
6-fold cross validation: the 4th fold is for test, the 5th fold is for validation, remaining folds are for training.

training dataset: total 24 image files.

validation dataset: total 5 image files.

test dataset: total 6 image files.
Total 24 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy
0 has 22520554 elements, with a rate of  0.8862108821500742 
1 has 2891630 elements, with a rate of  0.11378911784992585 
loss weight = tensor([1.0000, 7.7882])
Network has total 254,652,146 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		0.7867		0.21055		0.9096		0.00000		0.8513		0.00000
5	1.0000e-02		0.5800		0.38396		0.8275		0.16815		0.7547		0.25131
10	1.0000e-02		0.5155		0.44877		0.6708		0.31975		0.6268		0.30061
15	1.0000e-02		0.4528		0.51717		0.6825		0.28718		0.4847		0.42412
20	1.0000e-02		0.4182		0.54703		0.6539		0.30825		0.5650		0.31924
25	1.0000e-02		0.4893		0.44375		0.6155		0.37479		0.6147		0.29524
30	1.0000e-02		0.4183		0.55038		0.5641		0.42426		0.4454		0.52318
35	1.0000e-02		0.4287		0.52024		0.6086		0.37130		0.5302		0.37557
40	1.0000e-02		0.3526		0.62291		0.6458		0.32102		0.5056		0.38618
45	1.0000e-02		0.3543		0.61767		0.5150		0.47390		0.4478		0.46839
50	1.0000e-02		0.3774		0.59274		0.5317		0.45847		0.4968		0.43760
55	1.0000e-02		0.3207		0.65566		0.5342		0.45424		0.4488		0.45773
60	1.0000e-02		0.3250		0.64093		0.5727		0.41330		0.4233		0.48746
65	1.0000e-02		0.3540		0.61778		0.6032		0.38475		0.4671		0.45792
70	1.0000e-02		0.3153		0.66133		0.5966		0.38469		0.4833		0.41078
75	1.0000e-02		0.2845		0.69264		0.5537		0.43355		0.4175		0.52415
80	1.0000e-02		0.2555		0.72632		0.6115		0.37804		0.5110		0.44190
85	1.0000e-02		0.2619		0.71697		0.5799		0.40887		0.4544		0.48219
90	1.0000e-02		0.2608		0.71395		0.5616		0.42492		0.5036		0.39067
95	1.0000e-02		0.2740		0.70531		0.5263		0.46437		0.4756		0.43468
100	1.0000e-03		0.3020		0.66928		0.6094		0.38111		0.5018		0.47671
105	1.0000e-03		0.2182		0.76644		0.5457		0.44515		0.4552		0.47140
110	1.0000e-03		0.2215		0.76066		0.5940		0.39617		0.4716		0.48529
115	1.0000e-03		0.2073		0.77870		0.6205		0.36963		0.4754		0.49442
120	1.0000e-03		0.1891		0.79436		0.5965		0.39350		0.4597		0.49889
125	1.0000e-03		0.1956		0.79272		0.6072		0.38261		0.4790		0.48019
130	1.0000e-03		0.1807		0.80298		0.5516		0.43873		0.4666		0.44780
135	1.0000e-03		0.1886		0.79939		0.5970		0.39262		0.4910		0.44413
140	1.0000e-03		0.1906		0.79726		0.6140		0.37565		0.4811		0.48197
145	1.0000e-03		0.1771		0.81045		0.5805		0.40939		0.4640		0.48628
150	1.0000e-03		0.1592		0.82852		0.6017		0.38839		0.4617		0.50705
155	1.0000e-04		0.1732		0.81522		0.6021		0.38793		0.5166		0.42571
160	1.0000e-04		0.1724		0.81459		0.5265		0.46438		0.4549		0.46002
165	1.0000e-04		0.1592		0.83024		0.6292		0.36063		0.5118		0.46004
170	1.0000e-04		0.1594		0.82809		0.6237		0.36627		0.5126		0.45022
175	1.0000e-04		0.1669		0.81880		0.6010		0.38891		0.4983		0.45219
180	1.0000e-04		0.1833		0.80313		0.6198		0.37036		0.5125		0.44738
185	1.0000e-04		0.1601		0.82799		0.6061		0.38386		0.5062		0.44549
190	1.0000e-04		0.1718		0.81631		0.6204		0.36957		0.5018		0.46961
195	1.0000e-04		0.1639		0.82433		0.5917		0.39849		0.4774		0.47977
200	1.0000e-04		0.1715		0.81725		0.6315		0.35886		0.5170		0.45598
205	1.0000e-04		0.1492		0.83984		0.5923		0.39776		0.4825		0.47307
210	1.0000e-05		0.1745		0.81360		0.6167		0.37352		0.4924		0.48004
215	1.0000e-05		0.1671		0.82112		0.6238		0.36621		0.4901		0.48885
220	1.0000e-05		0.1766		0.80883		0.6382		0.35192		0.5161		0.46069
225	1.0000e-05		0.1562		0.83276		0.6033		0.38673		0.4847		0.47813
230	1.0000e-05		0.1578		0.83004		0.5920		0.39798		0.4819		0.47112
235	1.0000e-05		0.1721		0.81397		0.6296		0.36041		0.5012		0.47497
240	1.0000e-05		0.1532		0.83616		0.5928		0.39726		0.4903		0.46066
245	1.0000e-05		0.1595		0.82832		0.5719		0.41853		0.4630		0.48346
250	1.0000e-05		0.1789		0.80972		0.6259		0.36406		0.5002		0.47359
255	1.0000e-05		0.1487		0.83996		0.5777		0.41229		0.4669		0.48054
260	1.0000e-05		0.1752		0.81177		0.6197		0.37039		0.5032		0.46467
265	1.0000e-06		0.1545		0.83388		0.6052		0.38473		0.4858		0.47706
270	1.0000e-06		0.1837		0.80199		0.5954		0.39466		0.4728		0.48907
275	1.0000e-06		0.1523		0.83333		0.5809		0.40923		0.4772		0.47193
280	1.0000e-06		0.1655		0.82074		0.6026		0.38742		0.4890		0.46859
285	1.0000e-06		0.1658		0.82337		0.6041		0.38602		0.4782		0.48683
290	1.0000e-06		0.1636		0.82535		0.5818		0.40850		0.4709		0.47994
295	1.0000e-06		0.1654		0.82468		0.6416		0.34858		0.4998		0.48907
300	1.0000e-06		0.1397		0.84994		0.5610		0.42946		0.4654		0.47471
305	1.0000e-06		0.1832		0.80649		0.5934		0.39664		0.4787		0.47706
310	1.0000e-06		0.1590		0.82997		0.5218		0.46979		0.4981		0.42083
315	1.0000e-06		0.1690		0.81938		0.6013		0.38877		0.4876		0.47095
320	1.0000e-07		0.1668		0.82190		0.6060		0.38410		0.4828		0.48661
325	1.0000e-07		0.1625		0.82581		0.5570		0.43326		0.4403		0.50304
330	1.0000e-07		0.1605		0.82967		0.6266		0.36343		0.4916		0.48861
335	1.0000e-07		0.1725		0.81594		0.5745		0.41565		0.4614		0.48617
340	1.0000e-07		0.1605		0.82774		0.6072		0.38282		0.5017		0.45679
345	1.0000e-07		0.1659		0.82186		0.6055		0.38451		0.4890		0.47136
350	1.0000e-07		0.1618		0.82470		0.5907		0.39928		0.4681		0.48930
355	1.0000e-07		0.1642		0.82249		0.6178		0.37217		0.4892		0.48519
360	1.0000e-07		0.1575		0.83057		0.6023		0.38772		0.4889		0.47075
365	1.0000e-07		0.1557		0.83353		0.5388		0.45211		0.4454		0.48547
370	1.0000e-07		0.1516		0.83735		0.5923		0.39781		0.4669		0.49365
375	1.0000e-08		0.1692		0.81911		0.5913		0.39874		0.4755		0.48041
380	1.0000e-08		0.1520		0.83692		0.5962		0.39384		0.4735		0.48837
385	1.0000e-08		0.1579		0.83129		0.6179		0.37198		0.4848		0.49163
390	1.0000e-08		0.1558		0.83266		0.6376		0.35254		0.5037		0.47660
395	1.0000e-08		0.1851		0.80307		0.6221		0.36798		0.4968		0.47629
400	1.0000e-08		0.1632		0.82505		0.6238		0.36631		0.4997		0.47507
405	1.0000e-08		0.1598		0.82771		0.5919		0.39811		0.4800		0.47624
410	1.0000e-08		0.1534		0.83336		0.5848		0.40531		0.4714		0.48115
415	1.0000e-08		0.1830		0.80594		0.6448		0.34520		0.5131		0.47046
420	1.0000e-08		0.1648		0.82325		0.5842		0.40595		0.4722		0.48124
425	1.0000e-08		0.1750		0.81163		0.6374		0.35272		0.4979		0.48755
430	1.0000e-08		0.1710		0.81905		0.6150		0.37499		0.4973		0.46912
435	1.0000e-08		0.1515		0.83736		0.5868		0.40335		0.4793		0.47197
440	1.0000e-08		0.1651		0.82349		0.5939		0.39627		0.4881		0.46699
445	1.0000e-08		0.1737		0.81498		0.6110		0.37908		0.4847		0.48576
450	1.0000e-08		0.1708		0.81765		0.6179		0.37215		0.4879		0.48766
