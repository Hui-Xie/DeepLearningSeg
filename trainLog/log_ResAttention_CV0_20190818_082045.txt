=============training from sratch============
Program ID: 22679

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt', '1', '/home/hxie1/data/OvarianCancerCT/pixelSize223/numpy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '0', '0,1,2,3']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
            8    at July 30th 03:00, 2019:
                    add learning rate print;
                    use convStride =2;
                    add filter number by 2 times along deeper layers.
            9    at July 30th, 10:13, 2019:
                    add MaxPool2d in stage1;
                    add final filters to 2048.
            10   at July 30th, 15:23, 2019
                    final conv layer filter number: 1024
            11   at Aug 10th, 2019:
                    A. Add new patient data; and exclude non-standard patient data;
                    B. test the k-th fold,  validation on the (k+1)th fold;
                    C. new inputsize: 231*251*251 with pixels size 3*2*2 mm
                    D. window level shresthold [0,300]
                    E. put data padding in to converting from nrrd to numpy;
                    F. Add STN network as first laye of network
                    G. change input data into gaussian normalization in slice by slice; (x-mean)/std;
            12   Aug 12th, 2019
                    A. fix the bug that SITk reads int imape, and normalization into interger error;
                    B  test without SPN.           
            13   Aug 13th, 2019
                    A change input data into Gausssian distribution with non-zero mean,
                      it will make the padding zero do not occupy meaning of gaussian distribution.        
                    B add filters in the ResNeXt network to 128 at first stage, and reduce batchSize to 3. 
                    C Add STN at the beginning of the ResNeXt network
            14   Aug 14th, 2019
                    A. Fix the Bug the STN convert all image into 0 problem;
                    B. change DataTransform: change shear into [-30, 30], and add scale[0.6,1.2]
                    C  put STN at teh begginning of the network;
                    D  change optimizer from SGD to Adam; and change learning rate decay with gamma=0.5 per 20 steps.                                             
            15   Aug 16th, 2019
                    A The affine matrix in the STN is divided by its spectral norm;
                    B All conv layer add spectralNorm, while all FC layer do not add spectral Norm;
                    C reduce initial LR at 0.0001, and decay step to 30.
            16   Aug 17th, 2019
                    A  Cancel STN;
                    B  Add LeakyReLU;
                    C  before final FC layer, add ReLU and LocalResponseNorm   
            17   Aug 17th, 2019:10:44am
                    A enable STN;
                    B STN support LeakyReLU and localResponseNorm  
            18   Aug 17th, 2019 14:38 pm
                    A  del STN at beginining;
                    B  put STN at final 2 layer before FC. 
            19  Aug 17th 18:09 2019
                    A add surgical result file support;                    
            20  Aug 18th 08:00 am 2019
                    A  change all bias in Conv2d, and Linear into True.                     
            
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   total 220 patients, 5-fold cross validation, test 45, validation 45, and training 130.  

Training strategy: 

          

Program starting Time: 2019-08-18 08:20:45.710966
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/STNResNeXt/20190818_082045

Info: this is the 0th fold leave for test in the 5-fold cross-validation.

Infor: In all data of 220 files, label 0 has 77 files,
	  and label 1 has 143 files, where positive response rate = 0.65 in full data
5-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.
training dataset:	 total 130 files, where 1 has 85.0 with rate of 0.6538461538461539
validation dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
test dataset:	 total 45 files, where 1 has 29.0 with rate of 0.6444444444444445
Info: batchSize = 12

=== Network trains from scratch ====
Network has total 331,776,449 parameters.
This predict optimal response.
Info: program will use GPU [0, 1, 2, 3] from all 4 GPUs.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	LearningRate		TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	1.0000e-04		5.1083	0.5000	0.6471	0.2222		0.5085	0.3556	0.0000	1.0000		0.5085	0.3556	0.0000	1.0000
5	1.0000e-04		0.4918	0.5692	0.7176	0.2889		0.4759	0.6667	0.9310	0.1875		0.4833	0.6444	1.0000	0.0000
10	1.0000e-04		0.5007	0.4615	0.4588	0.4667		0.5173	0.4444	0.3103	0.6875		0.5103	0.5333	0.5517	0.5000
15	1.0000e-04		0.4877	0.6308	0.9176	0.0889		0.4948	0.4889	0.3448	0.7500		0.5059	0.5556	0.5862	0.5000
20	1.0000e-04		0.4733	0.5462	0.3882	0.8444		0.4745	0.6000	0.6897	0.4375		0.5203	0.6000	0.6897	0.4375
25	1.0000e-04		0.4901	0.6615	0.9529	0.1111		0.4686	0.6000	0.7586	0.3125		0.4920	0.5333	0.6207	0.3750
30	1.0000e-04		0.4988	0.5308	0.4235	0.7333		0.5654	0.6444	0.9655	0.0625		0.5404	0.6222	0.9655	0.0000
35	1.0000e-04		0.4479	0.5923	0.6118	0.5556		0.6492	0.4222	0.3103	0.6250		0.7324	0.4000	0.2069	0.7500
40	5.0000e-05		0.4264	0.6462	0.6000	0.7333		0.6481	0.4889	0.4828	0.5000		0.7596	0.3333	0.1724	0.6250
45	5.0000e-05		0.4208	0.7154	0.6941	0.7556		0.7582	0.6667	0.8276	0.3750		0.7956	0.4667	0.6207	0.1875
50	5.0000e-05		0.3985	0.7308	0.7529	0.6889		0.6829	0.4889	0.5517	0.3750		0.8277	0.4444	0.3793	0.5625
55	5.0000e-05		0.4142	0.6923	0.7294	0.6222		1.7950	0.3778	0.0345	1.0000		2.1287	0.3778	0.0345	1.0000
60	5.0000e-05		0.3300	0.7846	0.7412	0.8667		2.1379	0.6222	0.9655	0.0000		2.0219	0.6222	0.9655	0.0000
65	5.0000e-05		0.3379	0.7385	0.7176	0.7778		1.4879	0.3556	0.0345	0.9375		1.5369	0.3556	0.0000	1.0000
70	5.0000e-05		0.3597	0.7846	0.8353	0.6889		3.2730	0.3556	0.0000	1.0000		3.5451	0.3778	0.0345	1.0000
75	5.0000e-05		0.4594	0.6077	0.6706	0.4889		0.5286	0.5333	0.6207	0.3750		0.6258	0.4889	0.6897	0.1250
80	2.5000e-05		0.4270	0.6923	0.7882	0.5111		0.5247	0.6222	0.5172	0.8125		0.7929	0.3778	0.3103	0.5000
85	2.5000e-05		0.3044	0.7769	0.7529	0.8222		0.9740	0.4889	0.4483	0.5625		1.1358	0.3778	0.2414	0.6250
90	2.5000e-05		0.3021	0.8077	0.8706	0.6889		1.0203	0.5333	0.4138	0.7500		1.3651	0.3556	0.2069	0.6250
95	2.5000e-05		0.3444	0.7692	0.7529	0.8000		0.8689	0.5111	0.2759	0.9375		1.4047	0.3778	0.1034	0.8750
100	2.5000e-05		0.2565	0.8154	0.7882	0.8667		0.9327	0.5111	0.6207	0.3125		1.3259	0.5111	0.5517	0.4375
105	2.5000e-05		0.3193	0.7538	0.7294	0.8000		0.7737	0.6222	0.7241	0.4375		1.3190	0.5333	0.4828	0.6250
110	2.5000e-05		0.2326	0.8692	0.8353	0.9333		0.8824	0.5556	0.5172	0.6250		0.9806	0.4444	0.2069	0.8750
115	2.5000e-05		0.2535	0.8385	0.8471	0.8222		0.8231	0.5333	0.4828	0.6250		1.1791	0.4222	0.2069	0.8125
120	1.2500e-05		0.2862	0.8077	0.8471	0.7333		0.8732	0.4889	0.3448	0.7500		0.9579	0.4444	0.1724	0.9375
125	1.2500e-05		0.1991	0.8615	0.8588	0.8667		0.9944	0.5333	0.5172	0.5625		0.9451	0.5111	0.4828	0.5625
130	1.2500e-05		0.2704	0.8692	0.8824	0.8444		0.9015	0.5111	0.5517	0.4375		0.8161	0.5111	0.5172	0.5000
135	1.2500e-05		0.1898	0.8846	0.8824	0.8889		0.9445	0.5556	0.4828	0.6875		1.0676	0.4667	0.2759	0.8125
140	1.2500e-05		0.2051	0.8462	0.8353	0.8667		0.9733	0.6000	0.4828	0.8125		1.2576	0.4444	0.2069	0.8750
145	1.2500e-05		0.2157	0.9000	0.9412	0.8222		0.7640	0.5778	0.5862	0.5625		1.3113	0.5111	0.5172	0.5000
150	1.2500e-05		0.1401	0.9231	0.9059	0.9556		1.3331	0.5333	0.3448	0.8750		1.7732	0.4444	0.2069	0.8750
155	1.2500e-05		0.2180	0.8923	0.9176	0.8444		0.8133	0.5556	0.4483	0.7500		1.0599	0.4222	0.2759	0.6875
160	6.2500e-06		0.2225	0.8615	0.8941	0.8000		0.7962	0.6000	0.5862	0.6250		1.3142	0.5333	0.5172	0.5625
165	6.2500e-06		0.1503	0.8923	0.8824	0.9111		0.9800	0.5333	0.4138	0.7500		1.3187	0.4667	0.2759	0.8125
170	6.2500e-06		0.1593	0.9231	0.9647	0.8444		1.0504	0.4889	0.3448	0.7500		1.2890	0.5111	0.3448	0.8125
175	6.2500e-06		0.1790	0.9000	0.9176	0.8667		1.1067	0.5778	0.4138	0.8750		1.3251	0.4667	0.2414	0.8750
180	6.2500e-06		0.1551	0.9154	0.9412	0.8667		1.0256	0.5556	0.4138	0.8125		1.1544	0.4444	0.3103	0.6875
185	6.2500e-06		0.1457	0.9308	0.9412	0.9111		1.0771	0.5333	0.3448	0.8750		1.3545	0.4667	0.3103	0.7500
190	6.2500e-06		0.1220	0.9308	0.9059	0.9778		0.8747	0.5333	0.4828	0.6250		1.0001	0.4889	0.4138	0.6250
195	6.2500e-06		0.1064	0.9615	0.9647	0.9556		1.2509	0.5111	0.3103	0.8750		1.3567	0.4222	0.1724	0.8750
200	3.1250e-06		0.1850	0.8923	0.9412	0.8000		0.8031	0.5778	0.5862	0.5625		1.0261	0.5333	0.5862	0.4375
205	3.1250e-06		0.1596	0.9385	0.9529	0.9111		0.9247	0.5333	0.4828	0.6250		1.0066	0.4889	0.4483	0.5625
210	3.1250e-06		0.1621	0.9077	0.9059	0.9111		0.9557	0.5556	0.4138	0.8125		1.1385	0.4889	0.3448	0.7500
215	3.1250e-06		0.1562	0.9154	0.9412	0.8667		1.0528	0.5111	0.4138	0.6875		1.1391	0.4889	0.3448	0.7500
220	3.1250e-06		0.0766	0.9538	0.9529	0.9556		1.0544	0.5778	0.4138	0.8750		1.1094	0.4667	0.2759	0.8125
225	3.1250e-06		0.1105	0.9385	0.9765	0.8667		1.1762	0.5556	0.3793	0.8750		1.3340	0.4667	0.2759	0.8125
230	3.1250e-06		0.1311	0.9231	0.9412	0.8889		1.1221	0.5778	0.4138	0.8750		1.2080	0.4889	0.3103	0.8125
235	3.1250e-06		0.1649	0.9308	0.9765	0.8444		1.2307	0.5111	0.3448	0.8125		1.3279	0.5111	0.3448	0.8125
240	1.5625e-06		0.0572	0.9769	0.9647	1.0000		1.1164	0.5333	0.4138	0.7500		1.1330	0.5111	0.4138	0.6875
245	1.5625e-06		0.1041	0.9385	0.9294	0.9556		1.1573	0.5778	0.4138	0.8750		1.1351	0.4444	0.2759	0.7500
250	1.5625e-06		0.1477	0.9154	0.9176	0.9111		1.3513	0.4889	0.2759	0.8750		1.4084	0.4222	0.1724	0.8750
255	1.5625e-06		0.0859	0.9692	0.9647	0.9778		1.1116	0.5333	0.3448	0.8750		1.2569	0.4222	0.1724	0.8750
260	1.5625e-06		0.1491	0.9231	0.9647	0.8444		0.8969	0.5778	0.5517	0.6250		1.0164	0.4889	0.4828	0.5000
265	1.5625e-06		0.1980	0.9000	0.9529	0.8000		1.0128	0.5333	0.3448	0.8750		1.1683	0.4222	0.2069	0.8125
270	1.5625e-06		0.1066	0.9769	1.0000	0.9333		0.9720	0.5778	0.4483	0.8125		1.1355	0.5333	0.3793	0.8125
275	1.5625e-06		0.1265	0.9231	0.9176	0.9333		0.8880	0.5556	0.4828	0.6875		1.0131	0.5333	0.4828	0.6250
280	7.8125e-07		0.0664	0.9615	0.9647	0.9556		1.0810	0.5778	0.4483	0.8125		1.1600	0.4667	0.2759	0.8125
285	7.8125e-07		0.1429	0.9385	0.9529	0.9111		1.1731	0.5778	0.4138	0.8750		1.3058	0.4889	0.2759	0.8750
290	7.8125e-07		0.0380	0.9769	0.9647	1.0000		1.0294	0.5778	0.4483	0.8125		1.1734	0.4444	0.3103	0.6875
295	7.8125e-07		0.1342	0.9538	0.9882	0.8889		1.0627	0.5556	0.4138	0.8125		1.2038	0.4889	0.2759	0.8750
300	7.8125e-07		0.1087	0.9615	0.9765	0.9333		1.0211	0.5778	0.4483	0.8125		1.1807	0.4889	0.3448	0.7500
305	7.8125e-07		0.0757	0.9769	0.9882	0.9556		1.0191	0.5333	0.3793	0.8125		1.1652	0.5111	0.3448	0.8125
310	7.8125e-07		0.1276	0.9462	0.9882	0.8667		1.0290	0.5556	0.4138	0.8125		1.1070	0.4889	0.3103	0.8125
315	7.8125e-07		0.0740	0.9462	0.9412	0.9556		1.2002	0.5556	0.3793	0.8750		1.3081	0.4667	0.2414	0.8750
320	3.9063e-07		0.2269	0.8692	0.8941	0.8222		1.0388	0.5556	0.4828	0.6875		1.1948	0.5333	0.4828	0.6250
325	3.9063e-07		0.0766	0.9769	0.9882	0.9556		1.2647	0.5333	0.3448	0.8750		1.4009	0.4444	0.2069	0.8750
330	3.9063e-07		0.0613	0.9615	0.9529	0.9778		1.2388	0.5333	0.3793	0.8125		1.4145	0.5333	0.3448	0.8750
335	3.9063e-07		0.1517	0.9231	0.9176	0.9333		1.2667	0.5111	0.3448	0.8125		1.3539	0.4889	0.2759	0.8750
340	3.9063e-07		0.1553	0.9077	0.9412	0.8444		1.1114	0.5111	0.3448	0.8125		1.2090	0.4889	0.3103	0.8125
345	3.9063e-07		0.1115	0.9385	0.9882	0.8444		1.2339	0.5111	0.3448	0.8125		1.2673	0.4444	0.2069	0.8750
350	3.9063e-07		0.0772	0.9462	0.9529	0.9333		1.2441	0.5111	0.3448	0.8125		1.3144	0.4667	0.2414	0.8750
355	3.9063e-07		0.0861	0.9692	0.9765	0.9556		1.2335	0.5333	0.3448	0.8750		1.2593	0.4667	0.2414	0.8750
360	1.9531e-07		0.0528	0.9769	0.9647	1.0000		1.2741	0.4889	0.2759	0.8750		1.3503	0.4000	0.1379	0.8750
