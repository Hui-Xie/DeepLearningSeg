=============training from sratch============
Program ID: 19702

Program command: 
 ['TrainResAttentionNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/SurgicalResNeXt', '1', '/home/hxie1/data/OvarianCancerCT/pixelSize223/numpy', '/home/hxie1/data/OvarianCancerCT/patientSurgicalResults.json', '0', '2,3']

Major program changes: 
            ResNeXt-based Attention Net: use 2D network to implement 3D convolution without losing 3D context information. 
            0   the input is a 3D full volume without any cropping; 
            1   Use slices as features channels in convolutions,  and use 1*1 convolution along slices direction to implement z direction convolution followed by 3*3 convolutino inside slice planes;
                It just uses three cascading 2D convolutions (first z, then xy, and z direction again) to implement 3D convolution, like in the paper of ResNeXt below.
                The benefits of this design:
                A   reduce network parameters, hoping to reducing overfitting;
                B   speed up training;
                C   this implemented 3D convolutions are all in full slices space;
            2   use group convolution to implement thick slice convolution to increase the network representation capability;
            3   Use ResNeXt-based module like Paper "Aggregated Residual Transformations for Deep Neural Networks " 
                (Link: http://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper.html);
            4   use rich 2D affine transforms slice by slice and concatenate them to implement 3D data augmentation;
            5   20% data for independent test, remaining 80% data for 4-fold cross validation;
            6   add lossweight to adjust positive samples to 3/7 posweight in BCEWithLogitsLoss;
            
            Update:
            1    reduced network parameters to 3.14 million in July 27th, 2019, 0840am
            2    at 15:00 of July 27th, 2019, reduce network parameter again. Now each stage has 160 filters, with 1.235 million parameters
            3    keep 2) parameter, change all maxpooling into average pooling.
            4    At July 29th 09:37am, 2019, reduce filters to 96 to further reduce parameters, keep avgPool.
            5    at July 29th 11:25am, 2019,  reduce filter number to 48, and redue one stage
            6    at July 29th 12:41, 2019:
                    add GPUID in command line;
                    use SGD optimizer, instead of Adam
                    add numbers of filters along deeper layer with step 12.
                    add saveDir's tims stamp;
            7    at July 29th 15:18, 2019,
                    change learning rate step_size = 5 from 10;
                    before FC, we use conv2d
                    learning rate start at 0.5.
            8    at July 30th 03:00, 2019:
                    add learning rate print;
                    use convStride =2;
                    add filter number by 2 times along deeper layers.
            9    at July 30th, 10:13, 2019:
                    add MaxPool2d in stage1;
                    add final filters to 2048.
            10   at July 30th, 15:23, 2019
                    final conv layer filter number: 1024
            11   at Aug 10th, 2019:
                    A. Add new patient data; and exclude non-standard patient data;
                    B. test the k-th fold,  validation on the (k+1)th fold;
                    C. new inputsize: 231*251*251 with pixels size 3*2*2 mm
                    D. window level shresthold [0,300]
                    E. put data padding in to converting from nrrd to numpy;
                    F. Add STN network as first laye of network
                    G. change input data into gaussian normalization in slice by slice; (x-mean)/std;
            12   Aug 12th, 2019
                    A. fix the bug that SITk reads int imape, and normalization into interger error;
                    B  test without SPN.           
            13   Aug 13th, 2019
                    A change input data into Gausssian distribution with non-zero mean,
                      it will make the padding zero do not occupy meaning of gaussian distribution.        
                    B add filters in the ResNeXt network to 128 at first stage, and reduce batchSize to 3. 
                    C Add STN at the beginning of the ResNeXt network
            14   Aug 14th, 2019
                    A. Fix the Bug the STN convert all image into 0 problem;
                    B. change DataTransform: change shear into [-30, 30], and add scale[0.6,1.2]
                    C  put STN at teh begginning of the network;
                    D  change optimizer from SGD to Adam; and change learning rate decay with gamma=0.5 per 20 steps.                                             
            15   Aug 16th, 2019
                    A The affine matrix in the STN is divided by its spectral norm;
                    B All conv layer add spectralNorm, while all FC layer do not add spectral Norm;
                    C reduce initial LR at 0.0001, and decay step to 30.
            16   Aug 17th, 2019
                    A  Cancel STN;
                    B  Add LeakyReLU;
                    C  before final FC layer, add ReLU and LocalResponseNorm   
            17   Aug 17th, 2019:10:44am
                    A enable STN;
                    B STN support LeakyReLU and localResponseNorm  
            18   Aug 17th, 2019 14:38 pm
                    A  del STN at beginining;
                    B  put STN at final 2 layer before FC. 
             19  Aug 17th 18:09 2019
                    A add surgical result file support;                    
                                
            
            
Discarded changes:                  
                  

Experiment setting:
Input CT data: maximum size 140*251*251 (zyx) of 3D numpy array with spacing size(5*2*2)
Ground truth: response binary label

Predictive Model: 

response Loss Function:  BCELogitLoss

Data:   total 220 patients, 5-fold cross validation, test 45, validation 45, and training 130.  

Training strategy: 

          

Program starting Time: 2019-08-17 18:16:50.832649
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SurgicalResNeXt/20190817_181650

Info: this is the 0th fold leave for test in the 5-fold cross-validation.

Infor: In all data of 220 files, label 0 has 50 files,
	  and label 1 has 170 files, where positive response rate = 0.7727272727272727 in full data
5-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.
training dataset:	 total 132 files, where 1 has 102.0 with rate of 0.7727272727272727
validation dataset:	 total 44 files, where 1 has 34.0 with rate of 0.7727272727272727
test dataset:	 total 44 files, where 1 has 34.0 with rate of 0.7727272727272727
Info: batchSize = 6

=== Network trains from scratch ====
Network has total 331,724,274 parameters.
This predict surgical results.
Info: program will use GPU [2, 3] from all 4 GPUs.
Loss Functions List: BCEWithLogitsLoss with weight of 1; 

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	LearningRate		TrLoss	Accura	TPR_r	TNR_r		VaLoss	Accura	TPR_r	TNR_r		TeLoss	Accura	TPR_r	TNR_r
0	1.0000e-04		4.2804	0.5000	0.5098	0.4667		2.1250	0.2273	0.0000	1.0000		2.1250	0.2273	0.0000	1.0000
5	1.0000e-04		0.9544	0.5227	0.5098	0.5667		1.4132	0.7727	1.0000	0.0000		1.1214	0.7727	1.0000	0.0000
10	1.0000e-04		0.4497	0.4848	0.5000	0.4333		0.4025	0.2955	0.2059	0.6000		0.3334	0.4545	0.2941	1.0000
15	1.0000e-04		0.4148	0.3788	0.3039	0.6333		0.3124	0.5227	0.6176	0.2000		0.3117	0.4545	0.4412	0.5000
20	1.0000e-04		0.3507	0.5909	0.6765	0.3000		0.3613	0.4773	0.4412	0.6000		0.3907	0.3182	0.2647	0.5000
25	1.0000e-04		0.2922	0.7500	0.8039	0.5667		0.4131	0.5227	0.5000	0.6000		0.4049	0.4545	0.4412	0.5000
30	1.0000e-04		0.3162	0.6136	0.6961	0.3333		0.3399	0.5455	0.6765	0.1000		0.3183	0.5682	0.6176	0.4000
35	1.0000e-04		0.3235	0.5455	0.5196	0.6333		0.4290	0.7273	0.9412	0.0000		0.3828	0.7273	0.9412	0.0000
40	5.0000e-05		0.3106	0.5682	0.6078	0.4333		0.3832	0.4773	0.5588	0.2000		0.3123	0.6136	0.6765	0.4000
45	5.0000e-05		0.2769	0.7197	0.7843	0.5000		0.6086	0.6364	0.7647	0.2000		0.4670	0.6591	0.7647	0.3000
50	5.0000e-05		0.2964	0.6894	0.7059	0.6333		0.5587	0.5455	0.5882	0.4000		0.4111	0.5909	0.6176	0.5000
55	5.0000e-05		0.2933	0.6591	0.6765	0.6000		0.5003	0.5455	0.6765	0.1000		0.4424	0.5909	0.6471	0.4000
60	5.0000e-05		0.2290	0.7652	0.7451	0.8333		0.7508	0.5909	0.7353	0.1000		0.6343	0.6364	0.7353	0.3000
65	5.0000e-05		0.2642	0.6894	0.7157	0.6000		0.6045	0.6364	0.7941	0.1000		0.4553	0.7500	0.8529	0.4000
70	5.0000e-05		0.2263	0.7727	0.7941	0.7000		0.9370	0.6818	0.8529	0.1000		0.6197	0.6818	0.8235	0.2000
75	5.0000e-05		0.2479	0.6667	0.6765	0.6333		0.7286	0.6818	0.8529	0.1000		0.6377	0.7273	0.8529	0.3000
80	2.5000e-05		0.1927	0.8030	0.7745	0.9000		0.8003	0.4773	0.5294	0.3000		0.5258	0.5227	0.5294	0.5000
85	2.5000e-05		0.1674	0.8485	0.8431	0.8667		1.0966	0.5455	0.6471	0.2000		0.8103	0.6591	0.7647	0.3000
90	2.5000e-05		0.1769	0.8030	0.7843	0.8667		0.8004	0.5682	0.7059	0.1000		0.7877	0.6136	0.6765	0.4000
95	2.5000e-05		0.1508	0.8864	0.8824	0.9000		0.7513	0.5909	0.7353	0.1000		0.7446	0.5682	0.6176	0.4000
100	2.5000e-05		0.1589	0.8485	0.8529	0.8333		0.6890	0.4545	0.5294	0.2000		0.6441	0.4773	0.4412	0.6000
105	2.5000e-05		0.2022	0.8258	0.8235	0.8333		0.5913	0.4545	0.5294	0.2000		0.5368	0.5909	0.5588	0.7000
110	2.5000e-05		0.1285	0.8864	0.8922	0.8667		0.6648	0.4773	0.5294	0.3000		0.6216	0.5227	0.5294	0.5000
115	2.5000e-05		0.1964	0.8333	0.8431	0.8000		0.6845	0.5682	0.6765	0.2000		0.5538	0.6136	0.6765	0.4000
120	1.2500e-05		0.1601	0.8258	0.8235	0.8333		0.7580	0.6364	0.7647	0.2000		0.8222	0.6364	0.7647	0.2000
125	1.2500e-05		0.1310	0.8788	0.8529	0.9667		0.9551	0.6136	0.7647	0.1000		0.6936	0.6364	0.7353	0.3000
130	1.2500e-05		0.1589	0.8712	0.8922	0.8000		2.3022	0.7500	0.9412	0.1000		1.1597	0.7727	0.9412	0.2000
135	1.2500e-05		0.1047	0.9167	0.9314	0.8667		0.7929	0.5682	0.6765	0.2000		0.8646	0.6136	0.6765	0.4000
140	1.2500e-05		0.1365	0.8561	0.8333	0.9333		0.5775	0.3636	0.3529	0.4000		0.5664	0.4773	0.3824	0.8000
145	1.2500e-05		0.1718	0.8864	0.9118	0.8000		0.5901	0.6818	0.7941	0.3000		0.6330	0.6136	0.7353	0.2000
150	1.2500e-05		0.0932	0.9545	0.9706	0.9000		0.6939	0.6364	0.7647	0.2000		0.5329	0.6136	0.7059	0.3000
155	1.2500e-05		0.0795	0.9242	0.9412	0.8667		0.6682	0.5455	0.6176	0.3000		0.7560	0.5682	0.5882	0.5000
160	6.2500e-06		0.1211	0.9394	0.9706	0.8333		0.9056	0.6591	0.8235	0.1000		0.8203	0.6591	0.7647	0.3000
165	6.2500e-06		0.1156	0.9091	0.9118	0.9000		0.5397	0.5000	0.5000	0.5000		0.6207	0.4545	0.4706	0.4000
170	6.2500e-06		0.1630	0.9167	0.9510	0.8000		0.7820	0.6136	0.7353	0.2000		0.7419	0.5682	0.6765	0.2000
175	6.2500e-06		0.1031	0.9091	0.9118	0.9000		0.7523	0.7273	0.9118	0.1000		0.6369	0.6818	0.8235	0.2000
180	6.2500e-06		0.0478	0.9697	0.9804	0.9333		0.6226	0.5000	0.5294	0.4000		0.6429	0.5000	0.5000	0.5000
185	6.2500e-06		0.0996	0.9242	0.9314	0.9000		0.7257	0.5909	0.6765	0.3000		0.5905	0.5909	0.6471	0.4000
190	6.2500e-06		0.0587	0.9621	0.9706	0.9333		0.5807	0.5682	0.6471	0.3000		0.6028	0.5000	0.5588	0.3000
195	6.2500e-06		0.0953	0.9318	0.9412	0.9000		1.2882	0.7500	0.9412	0.1000		1.0201	0.7045	0.8529	0.2000
200	3.1250e-06		0.0552	0.9697	0.9706	0.9667		0.7503	0.7273	0.8824	0.2000		0.6763	0.6818	0.8235	0.2000
205	3.1250e-06		0.0340	0.9773	0.9706	1.0000		0.7037	0.7045	0.8824	0.1000		0.6431	0.6818	0.7941	0.3000
210	3.1250e-06		0.0470	0.9773	0.9706	1.0000		0.7666	0.6591	0.8235	0.1000		0.7051	0.5909	0.7059	0.2000
215	3.1250e-06		0.0481	0.9394	0.9314	0.9667		0.6403	0.7045	0.8235	0.3000		0.6671	0.6136	0.7353	0.2000
220	3.1250e-06		0.0792	0.9545	0.9510	0.9667		0.6536	0.4773	0.5588	0.2000		0.6146	0.4545	0.5000	0.3000
225	3.1250e-06		0.0386	0.9545	0.9510	0.9667		0.7519	0.6818	0.8235	0.2000		0.7116	0.5909	0.7059	0.2000
230	3.1250e-06		0.1107	0.9470	0.9902	0.8000		0.6480	0.5909	0.7059	0.2000		0.6589	0.5455	0.6176	0.3000
235	3.1250e-06		0.0390	0.9697	0.9608	1.0000		0.7160	0.5909	0.6765	0.3000		0.7044	0.5455	0.6176	0.3000
240	1.5625e-06		0.0306	0.9848	0.9902	0.9667		0.7655	0.6136	0.7059	0.3000		0.6939	0.5682	0.6471	0.3000
245	1.5625e-06		0.0354	0.9773	0.9804	0.9667		0.6766	0.5909	0.6765	0.3000		0.7039	0.5227	0.6176	0.2000
250	1.5625e-06		0.0205	0.9697	0.9608	1.0000		0.6936	0.6818	0.8235	0.2000		0.6813	0.5909	0.7059	0.2000
255	1.5625e-06		0.1144	0.9621	0.9902	0.8667		0.7589	0.6591	0.7941	0.2000		0.7370	0.6136	0.7353	0.2000
260	1.5625e-06		0.0784	0.9621	0.9902	0.8667		0.6822	0.6364	0.7647	0.2000		0.7007	0.5682	0.6765	0.2000
265	1.5625e-06		0.0236	0.9848	0.9804	1.0000		0.6876	0.7045	0.8529	0.2000		0.7138	0.6591	0.7941	0.2000
270	1.5625e-06		0.0952	0.9545	0.9706	0.9000		0.6044	0.6818	0.8235	0.2000		0.6456	0.5909	0.7059	0.2000
275	1.5625e-06		0.0213	0.9848	0.9804	1.0000		0.6333	0.6818	0.8235	0.2000		0.6494	0.6136	0.7353	0.2000
280	7.8125e-07		0.0331	0.9848	0.9804	1.0000		0.6654	0.7273	0.8824	0.2000		0.6794	0.6136	0.7353	0.2000
285	7.8125e-07		0.0422	0.9545	0.9510	0.9667		0.6369	0.6364	0.7647	0.2000		0.6750	0.5909	0.6765	0.3000
290	7.8125e-07		0.0422	0.9545	0.9510	0.9667		0.6676	0.6591	0.7941	0.2000		0.6539	0.5909	0.6765	0.3000
295	7.8125e-07		0.0319	0.9773	0.9706	1.0000		0.7219	0.6591	0.7941	0.2000		0.7236	0.6136	0.7353	0.2000
300	7.8125e-07		0.0844	0.9621	0.9804	0.9000		0.6190	0.5227	0.5882	0.3000		0.7148	0.4545	0.5000	0.3000
305	7.8125e-07		0.0144	1.0000	1.0000	1.0000		0.6550	0.6364	0.7353	0.3000		0.7028	0.5227	0.5882	0.3000


 training loss less than 0.02, Program exit.


=============END of Training of ResAttentionNet Predict Model =================

Program Ending Time: 2019-08-17 20:57:36.705497
