Program ID of Prdict Network training:27780

Program command: 
 ['TrainPredictNet.py', '/home/hxie1/temp_netParameters/OvarianCancer/Predict', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainImages_ROI', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/testImages_ROI', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/patientResponseDict.json', 'image3dROI']

Major program changes: 
                      the nunmber of filters in 1st layer in V model = 96
                      latent Vector size: 1536*51*49 (featureMap* slices * axisPlaneLatentVector)
                      PredictModel is convsDenseModule+FC network.
                      there total 162 patient data, in which, 130 with smaller patientId as training data, 
                                                          and 32 with bigger patientID as test data

Experiment setting for Latent to response:
Input: 1536*51*49 Tensor as latent vector,
       where 1536 is the  number of filter at the bottleneck of V model, 
             51 is the number of slices of ROI CT image with size 51*281*281 for input to V model, 
             49 =7*7 is the flatted feature map for each filter.

Predictive Model: 1,  first 4-layer dense conv block reducing feature space into 768 with tensor size 768*51*49 
                  2,  and 4 dense conv blocks each of which includes a stride 2 conv and 4-layers dense conv block; now the the tensor is with size 48*2*2
                  3,  and a simple conv-batchNorm-Relu layer with filter size(2,2) change the tensor with size of  48*1;
                  4,  and 2 fully connected layers  changes the tensor into size 2*1;
                  5  final a softmax for binary classification;
                  Total network learning parameters are 29 millions.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/LatentPredictModel.py

Loss Function:   Cross Entropy with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.

Data:            training data has 130 patients, and test data has 32 patients with training/test rate 80/20.

Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.
                                                          
                                                         
Experiment setting for Image3d Zoom to response:
Input: 73*141*141 scaled 3D CT raw image as numpy array 
       
Predictive Model: 1,  first 3-layer dense conv block with channel size 64.
                  2,  and 6 dense conv DownBB blocks,  each of which includes a stride 2 conv and 4-layers dense conv block; 
                  3,  and 3 fully connected layers  changes the tensor into size 2*1;
                  4,  final a softmax for binary classification;
                  Total network learning parameters are 1.56 millions.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/Image3dPredictModel.py

Loss Function:   Cross Entropy with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.

Data:            training data has 130 patients, and test data has 32 patients with training/test rate 80/20.

Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.  
                    
                    change patience of learningRate scheduler to 30.

Experiment setting for Image3d ROI to response:
Input: 51*281*281  3D CT raw image ROI as numpy array 
       
Predictive Model: 1,  first 3-layer dense conv block with channel size 24.
                  2,  and 6 dense conv DownBB blocks,  each of which includes a stride 2 conv and 4-layers dense conv block; 
                  3,  and 3 fully connected layers  changes the tensor into size 2*1;
                  4,  final a softmax for binary classification;
                  Total network learning parameters are 94.5 millions.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/Image3dPredictModel.py

Loss Function:   Cross Entropy with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.

Data:            training data has 130 patients, and test data has 32 patients with training/test rate 80/20.

Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.  
                    
                    change patience of learningRate scheduler to 30.                                         

            

Program starting Time: 2019-05-31 15:09:56.719595
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/Predict

Now program get 130 input files.
Now program get 32 input files.
TrainData Input:  batchSize=4, depth=51, height=281, width=281, NumClassfication=2

TestData Input:  batchSize=4, depth=51, height=281, width=281, NumClassfication=2

Info: the size of bottle neck in the net = 24* (2, 16, 16)

Info: program uses Mixup with alpha=0.4, and mixupProb = 0.5.
Network trains from scratch.
Network has total 94,551,002 parameters.
Infor: Cross Entropy Weight: [3.3333333333333335, 1.4285714285714286] for label[0, 1]
Info: program will use 4 GPUs.
Loss Functions List: FocalCELoss with weight of 1; 
Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 


Epoch		TrLoss	TrainAccuracy	TsLoss	TestAccuracy
0		0.1738	0.328		0.1748	0.214
1		0.1736	0.278		0.1733	0.214
2		0.1735	0.639		0.1731	0.786
3		0.1734	0.531		0.1728	0.786
4		0.1731	0.667		0.1723	0.786
5		0.1723	0.768		0.1704	0.786
6		0.1722	0.765		0.1688	0.786
7		0.1717	0.714		0.1681	0.786
8		0.1736	0.681		0.1677	0.786
9		0.1734	0.647		0.1685	0.786
10		0.1735	0.719		0.1688	0.786
11		0.1741	0.625		0.1693	0.786
12		0.1733	0.696		0.1699	0.786
13		0.1729	0.604		0.1701	0.786
14		0.1730	0.691		0.1696	0.786
15		0.1730	0.645		0.1699	0.786
16		0.1733	0.655		0.1698	0.786
17		0.1734	0.650		0.1699	0.786
18		0.1728	0.617		0.1699	0.786
19		0.1722	0.719		0.1689	0.786
20		0.1717	0.714		0.1679	0.786
21		0.1721	0.673		0.1675	0.786
22		0.1730	0.625		0.1674	0.786
23		0.1732	0.700		0.1682	0.786
24		0.1721	0.700		0.1674	0.786
25		0.1723	0.647		0.1670	0.786
26		0.1728	0.708		0.1665	0.786
27		0.1731	0.667		0.1672	0.786
28		0.1726	0.658		0.1675	0.786
29		0.1733	0.733		0.1672	0.786
30		0.1727	0.688		0.1680	0.786
31		0.1715	0.779		0.1675	0.786
32		0.1728	0.672		0.1671	0.786
33		0.1740	0.656		0.1672	0.786
34		0.1731	0.650		0.1676	0.786
35		0.1724	0.706		0.1681	0.786
36		0.1719	0.732		0.1673	0.786
37		0.1732	0.692		0.1673	0.786
38		0.1720	0.705		0.1676	0.786
39		0.1732	0.706		0.1673	0.786
40		0.1740	0.667		0.1682	0.786
41		0.1726	0.673		0.1684	0.786
42		0.1732	0.719		0.1683	0.786
43		0.1736	0.703		0.1686	0.786
44		0.1729	0.662		0.1689	0.786
45		0.1732	0.654		0.1688	0.786
46		0.1723	0.750		0.1685	0.786
47		0.1723	0.656		0.1680	0.786
48		0.1728	0.681		0.1680	0.786
49		0.1735	0.769		0.1685	0.786
50		0.1726	0.681		0.1681	0.786
51		0.1732	0.654		0.1681	0.786
52		0.1736	0.711		0.1684	0.786
53		0.1734	0.679		0.1690	0.786
54		0.1725	0.643		0.1689	0.786
55		0.1738	0.659		0.1692	0.786
56		0.1739	0.692		0.1704	0.786
57		0.1731	0.683		0.1701	0.786
58		0.1733	0.650		0.1702	0.786
59		0.1733	0.632		0.1700	0.786
60		0.1727	0.700		0.1699	0.786
61		0.1722	0.692		0.1696	0.786
62		0.1733	0.618		0.1696	0.786
63		0.1728	0.700		0.1695	0.786
64		0.1721	0.734		0.1693	0.786
65		0.1728	0.688		0.1688	0.786
66		0.1737	0.681		0.1692	0.786
67		0.1717	0.646		0.1687	0.786
68		0.1717	0.750		0.1684	0.786
69		0.1737	0.683		0.1682	0.786
70		0.1725	0.633		0.1686	0.786
71		0.1733	0.671		0.1683	0.786
72		0.1717	0.724		0.1681	0.786
73		0.1734	0.653		0.1683	0.786
74		0.1737	0.645		0.1687	0.786
75		0.1729	0.703		0.1686	0.786
76		0.1731	0.659		0.1683	0.786
77		0.1740	0.609		0.1687	0.786
78		0.1723	0.722		0.1689	0.786
79		0.1726	0.708		0.1686	0.786
80		0.1730	0.691		0.1688	0.786
81		0.1736	0.721		0.1689	0.786
82		0.1730	0.667		0.1691	0.786
83		0.1740	0.635		0.1694	0.786
84		0.1726	0.645		0.1692	0.786
85		0.1725	0.671		0.1689	0.786
86		0.1728	0.636		0.1690	0.786
87		0.1725	0.607		0.1688	0.786
88		0.1730	0.656		0.1688	0.786
89		0.1728	0.750		0.1687	0.786
90		0.1733	0.719		0.1688	0.786
91		0.1727	0.733		0.1687	0.786
92		0.1734	0.732		0.1688	0.786
93		0.1731	0.700		0.1690	0.786
94		0.1728	0.690		0.1689	0.786
95		0.1735	0.672		0.1688	0.786
96		0.1735	0.717		0.1692	0.786
97		0.1731	0.675		0.1690	0.786
98		0.1731	0.662		0.1691	0.786
99		0.1724	0.690		0.1689	0.786
