=============training from sratch============
Program ID: 28988

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy', '2', '3,2,1']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 209
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss;
          
         

Discarded changes:                  
          

Program starting Time: 2019-10-10 09:22:23.730227
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191010_092223

Info: this is the 2th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 6

Info: useLabelConsistencyLoss = False

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191010_092223.
6-fold cross validation: the 2th fold is for test, the 3th fold is for validation, remaining folds are for training.

training dataset: total 19 image files.

validation dataset: total 5 image files.

test dataset: total 5 image files.
Total 19 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy
0 has 18801751 elements, with a rate of  0.9345745415083692 
1 has 1316228 elements, with a rate of  0.0654254584916308 
loss weight = tensor([ 1.0000, 14.2846])
Network has total 113,191,074 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		15.4288		0.09045		21.7611		0.00725		25.2064		0.03034
5	1.0000e-02		6.4082		0.29786		8.4727		0.25102		16.2731		0.25821
10	1.0000e-02		4.3704		0.36588		16.2220		0.22270		43.2504		0.32987
15	1.0000e-02		5.3508		0.35585		15.0785		0.15327		12.9944		0.42580
20	1.0000e-02		4.6888		0.36779		7.7683		0.27671		7.5706		0.40405
25	1.0000e-02		6.1793		0.36988		6.2392		0.36962		5.6863		0.50286
30	1.0000e-02		4.6896		0.39752		6.1786		0.34048		6.5205		0.45645
35	1.0000e-02		3.3201		0.42573		4.8238		0.45432		9.8144		0.48981
40	1.0000e-02		4.8746		0.34168		23.2609		0.26928		9.2540		0.50857
45	1.0000e-02		3.4371		0.39809		8.9306		0.28986		5.1006		0.46180
50	1.0000e-02		4.4115		0.37886		10.7067		0.26499		5.8504		0.45080
55	1.0000e-02		3.9698		0.37175		6.9719		0.41708		8.6482		0.54591
60	1.0000e-02		3.4524		0.40367		5.0727		0.40598		6.0739		0.48645
65	1.0000e-02		3.2059		0.43378		4.9589		0.38138		5.3178		0.48080
70	1.0000e-02		3.4713		0.38882		10.3380		0.27550		6.6407		0.42525
75	1.0000e-02		3.7248		0.32886		5.8653		0.31800		6.1486		0.44918
80	1.0000e-02		3.6699		0.39421		5.3662		0.35794		5.4478		0.48387
85	1.0000e-02		3.5131		0.38050		14.8682		0.22467		23.7524		0.40966
90	1.0000e-03		2.6833		0.46789		4.9591		0.43276		8.1844		0.49602
95	1.0000e-03		3.2159		0.43920		5.3497		0.43148		5.4659		0.54018
100	1.0000e-03		2.6151		0.44639		5.2211		0.42174		6.7563		0.54943
105	1.0000e-03		3.5596		0.43481		5.7651		0.38023		7.8755		0.55301
110	1.0000e-03		2.8705		0.45172		5.8476		0.37728		8.4224		0.54288
115	1.0000e-03		2.6226		0.45710		6.3627		0.34870		9.8191		0.55076
120	1.0000e-03		2.8726		0.44577		5.5000		0.39711		10.7608		0.53976
125	1.0000e-03		3.6761		0.45968		5.5743		0.40137		9.5863		0.54194
130	1.0000e-03		2.5742		0.44491		6.4723		0.33343		5.3867		0.54694
135	1.0000e-03		2.7481		0.42516		6.5613		0.34610		5.8056		0.57638
140	1.0000e-03		2.2611		0.46257		5.8370		0.34522		4.7823		0.56783
145	1.0000e-04		2.6284		0.43308		6.0551		0.35597		6.4646		0.58654
150	1.0000e-04		2.8347		0.43182		5.7455		0.35418		5.8995		0.56413
155	1.0000e-04		2.2441		0.42233		5.3378		0.39390		4.8926		0.53804
160	1.0000e-04		1.9842		0.47612		5.4961		0.39369		5.0144		0.54195
165	1.0000e-04		2.7134		0.45869		5.6026		0.38462		5.2470		0.55829
170	1.0000e-04		2.8427		0.43752		5.7795		0.36023		5.9889		0.55996
175	1.0000e-04		2.6637		0.45121		6.1310		0.35153		6.3073		0.56818
180	1.0000e-04		2.7146		0.46687		6.4560		0.34495		6.4752		0.55507
185	1.0000e-04		2.2481		0.46429		5.9215		0.35541		7.4332		0.56847
190	1.0000e-04		2.7699		0.46196		5.5874		0.36572		7.2813		0.56471
195	1.0000e-04		2.7320		0.48284		5.4149		0.38950		5.6194		0.54580
200	1.0000e-05		2.1310		0.46228		5.5629		0.38203		5.3072		0.54713
205	1.0000e-05		2.3931		0.46873		6.8132		0.33011		6.5857		0.57139
210	1.0000e-05		2.5761		0.46394		5.7339		0.38176		5.7871		0.56047
215	1.0000e-05		2.4940		0.45515		5.8838		0.36886		5.1615		0.54325
220	1.0000e-05		2.6995		0.43847		6.0181		0.34957		6.1187		0.56565
225	1.0000e-05		2.1309		0.46229		5.6204		0.40528		5.1601		0.54418
230	1.0000e-05		3.4091		0.43929		6.2567		0.35235		6.7264		0.57085
235	1.0000e-05		2.8661		0.44228		5.8124		0.35609		5.9463		0.56148
240	1.0000e-05		2.5334		0.44962		5.9396		0.35390		7.2372		0.56767
245	1.0000e-05		2.1172		0.46719		5.7735		0.39044		5.2882		0.54974
250	1.0000e-05		3.5458		0.45092		5.8424		0.35806		6.6631		0.56770
255	1.0000e-06		2.4209		0.45773		6.9132		0.34833		6.0214		0.56659
260	1.0000e-06		2.4279		0.44813		6.1675		0.38382		5.0599		0.54561
265	1.0000e-06		2.3241		0.44507		5.3962		0.36910		5.6270		0.54948
270	1.0000e-06		2.2538		0.45953		5.5489		0.36589		5.9230		0.55503
275	1.0000e-06		2.1796		0.44881		5.6573		0.36873		6.1558		0.56203
280	1.0000e-06		2.3503		0.46912		5.6243		0.37015		5.9071		0.55468
285	1.0000e-06		2.4882		0.46974		5.8013		0.39205		5.9727		0.56401
290	1.0000e-06		2.4247		0.46043		6.8355		0.34713		7.6031		0.56794
295	1.0000e-06		2.0835		0.46866		6.5361		0.36036		5.3894		0.55332
300	1.0000e-06		2.2167		0.46947		5.5819		0.37864		5.3816		0.54998
305	1.0000e-06		2.7183		0.45480		5.7463		0.37243		6.1323		0.56712
310	1.0000e-07		3.0351		0.46404		5.3657		0.40531		5.3806		0.55356
315	1.0000e-07		2.5238		0.45843		6.1802		0.38497		5.2722		0.55264
320	1.0000e-07		2.6989		0.46400		5.5607		0.36379		6.3772		0.56273
325	1.0000e-07		3.1072		0.45417		5.5503		0.39935		6.1106		0.56073
330	1.0000e-07		2.3647		0.46666		5.7794		0.38356		5.8988		0.56152
335	1.0000e-07		2.8253		0.45692		5.7659		0.34926		6.0680		0.55269
340	1.0000e-07		2.1283		0.47819		6.0361		0.35481		6.1659		0.55505
345	1.0000e-07		2.1986		0.45417		5.6777		0.35943		6.7153		0.56410
350	1.0000e-07		2.0440		0.46895		5.6511		0.37043		5.9210		0.55527
355	1.0000e-07		2.6656		0.46553		5.7905		0.36777		7.0202		0.56452
360	1.0000e-07		2.6361		0.45711		5.7552		0.35345		6.3550		0.56225
365	1.0000e-08		2.7992		0.47363		5.6341		0.35403		6.3277		0.55685
370	1.0000e-08		2.2714		0.45505		6.7545		0.33658		5.6043		0.55821
375	1.0000e-08		2.5325		0.46769		5.7502		0.40346		5.3168		0.55009
380	1.0000e-08		2.5641		0.47662		6.1765		0.34964		7.1134		0.56035
385	1.0000e-08		2.8763		0.46031		5.8068		0.36396		5.7286		0.54978
390	1.0000e-08		2.4490		0.46722		6.5574		0.34395		6.3413		0.55523
395	1.0000e-08		3.1449		0.47735		6.4273		0.35356		7.5024		0.57737
400	1.0000e-08		2.1259		0.45468		6.5053		0.34249		7.6021		0.56365
405	1.0000e-08		2.5277		0.44238		5.9622		0.36490		5.8348		0.56262
410	1.0000e-08		2.4340		0.45128		5.6711		0.35172		6.0963		0.55209
415	1.0000e-08		2.3352		0.46271		6.4254		0.34488		6.8187		0.55804
420	1.0000e-08		2.6233		0.44125		5.5952		0.35280		5.8269		0.55024
425	1.0000e-08		2.7027		0.47359		5.6856		0.36285		6.1654		0.55083
430	1.0000e-08		2.4466		0.46725		5.8419		0.35696		6.1254		0.56052
