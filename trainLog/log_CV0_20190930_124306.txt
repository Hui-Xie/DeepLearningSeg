=============training from sratch============
Program ID: 23693

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy', '0', '3,2,1']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      
         

Discarded changes:                  

Experiment setting:
Input CT data: 51*171*171 ROI around primary cancer

Loss Function:  SoftMax

Data:   total 36 patients with 50-80% label, 6-fold cross validation, test 6, validation 6, and training 24.  
    script: python3.7 statisticsLabelFiles.py 
    Total 36 in /home/hxie1/data/OvarianCancerCT/primaryROI/labels_npy
    0 has 48159408 elements, with a rate of  0.8970491562903105 
    1 has 5527068 elements, with a rate of  0.10295084370968957

Training strategy: 

          

Program starting Time: 2019-09-30 12:43:06.958927
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20190930_124306

Info: this is the 0th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 6

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20190930_124306.
program re-initializes all input files list, which will lead previous all K_fold cross validation invalid.
6-fold cross validation: the 0th fold is for test, the 1th fold is for validation, remaining folds are for training.

training dataset: total 23 image files.

validation dataset: total 6 image files.

test dataset: total 6 image files.
Total 23 training files  extracted from /home/hxie1/data/OvarianCancerCT/primaryROISmall/labels_npy
0 has 20483196 elements, with a rate of  0.786551514752855 
1 has 5558577 elements, with a rate of  0.21344848524714505 
loss weight = tensor([1.0000, 3.6850])
Network has total 73,047,746 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		29.8776		0.18939		5700.4971		0.00000		2719.8948		0.00000
5	1.0000e-02		6.9066		0.60991		9.9393		0.59085		9.8877		0.53572
10	1.0000e-02		5.9259		0.65910		11.4485		0.73227		9.0259		0.70291
15	1.0000e-02		4.5820		0.68013		3.8882		0.77840		5.2487		0.70651
20	1.0000e-02		5.7073		0.64690		4.1125		0.75925		5.2559		0.69336
25	1.0000e-02		5.8250		0.67366		4.2474		0.76400		5.3987		0.68822
30	1.0000e-02		4.5686		0.70163		4.4526		0.77138		5.4036		0.68029
35	1.0000e-02		5.0694		0.69565		5.6282		0.76713		5.4424		0.71914
40	1.0000e-02		4.0485		0.72614		3.3722		0.81091		4.9304		0.71811
45	1.0000e-02		4.4987		0.70297		3.7273		0.79413		4.9654		0.71364
50	1.0000e-02		5.2193		0.74675		3.7806		0.80607		4.8507		0.71464
55	1.0000e-02		5.6665		0.70685		3.3355		0.83091		4.9443		0.73133
60	1.0000e-02		3.6324		0.74660		4.1067		0.78732		4.9397		0.72777
65	1.0000e-02		3.6995		0.76370		3.7400		0.80798		5.7251		0.70980
70	1.0000e-02		3.7355		0.73805		3.3894		0.82174		5.1086		0.73319
75	1.0000e-02		3.5401		0.74013		4.1828		0.78120		5.2584		0.70122
80	1.0000e-02		4.2701		0.73779		4.5832		0.76165		5.7421		0.71839
85	1.0000e-02		4.2536		0.74606		3.7004		0.79195		5.1752		0.72116
90	1.0000e-02		4.5020		0.75087		4.4541		0.77615		5.6364		0.68135
95	1.0000e-02		3.4576		0.75242		4.1623		0.78873		5.4055		0.69644
100	1.0000e-02		3.5388		0.76685		3.7424		0.80091		4.8403		0.72656
105	1.0000e-02		3.6431		0.78933		3.7535		0.82317		4.6041		0.72275
110	1.0000e-03		3.6322		0.75806		3.5678		0.79777		4.6941		0.74465
115	1.0000e-03		3.1871		0.74460		3.3134		0.80826		4.5840		0.73471
120	1.0000e-03		3.0673		0.78052		3.5940		0.79994		4.5391		0.72994
125	1.0000e-03		3.2568		0.77303		3.1993		0.81522		4.3918		0.73441
130	1.0000e-03		2.9479		0.78676		3.4830		0.80570		4.2419		0.73553
135	1.0000e-03		2.7367		0.80561		3.3522		0.80428		4.4390		0.73743
140	1.0000e-03		2.6186		0.77714		3.4740		0.80267		4.7054		0.72625
145	1.0000e-03		2.3936		0.79752		3.1416		0.81099		4.6821		0.73116
150	1.0000e-03		3.3872		0.78971		3.4714		0.79902		4.8698		0.72605
155	1.0000e-03		2.6218		0.80845		3.4993		0.79758		4.5314		0.73432
160	1.0000e-03		2.4792		0.82050		3.4453		0.80395		4.4742		0.73585
165	1.0000e-03		2.9571		0.79139		3.4565		0.80050		4.7201		0.73004
170	1.0000e-03		2.7586		0.80254		3.8962		0.79270		4.6416		0.72344
175	1.0000e-03		2.2920		0.82545		3.4019		0.80187		4.6464		0.73373
180	1.0000e-03		2.8032		0.81718		3.3025		0.80334		4.8726		0.72929
185	1.0000e-03		2.6318		0.79549		3.5087		0.79860		4.5348		0.73132
190	1.0000e-03		2.4704		0.80456		3.4969		0.80215		4.6424		0.72683
195	1.0000e-03		2.6527		0.82618		4.0321		0.79002		4.5726		0.72357
200	1.0000e-04		2.2794		0.80033		4.0331		0.77866		4.6546		0.72012
205	1.0000e-04		2.2845		0.80827		3.6675		0.78959		4.7912		0.73097
210	1.0000e-04		2.3710		0.80535		3.6831		0.79202		4.6091		0.72829
215	1.0000e-04		2.4424		0.80094		3.7288		0.78920		4.6699		0.73217
220	1.0000e-04		2.1795		0.81601		3.6812		0.79312		4.7414		0.72388
225	1.0000e-04		2.3890		0.80688		3.5968		0.79784		4.5996		0.72687
230	1.0000e-04		2.2692		0.81564		3.6733		0.79217		4.6735		0.72476
235	1.0000e-04		2.2113		0.80792		3.6425		0.79242		4.9968		0.72339
240	1.0000e-04		2.6176		0.82094		3.5577		0.79394		4.7197		0.72904
245	1.0000e-04		2.7152		0.79986		3.5757		0.79299		4.7384		0.72951
250	1.0000e-04		2.3262		0.79769		3.7316		0.78977		4.7999		0.72784
255	1.0000e-05		2.2867		0.82871		3.7388		0.78799		4.7636		0.72717
260	1.0000e-05		2.1801		0.82539		3.6013		0.79575		4.7147		0.72999
265	1.0000e-05		2.5750		0.80780		3.6934		0.79091		4.8612		0.72879
270	1.0000e-05		2.0609		0.80740		3.7375		0.78824		4.7776		0.72888
275	1.0000e-05		2.2916		0.81808		3.7024		0.78855		4.7634		0.72835
280	1.0000e-05		2.4644		0.81888		3.6998		0.79074		4.7719		0.72829
285	1.0000e-05		2.3662		0.81766		3.6720		0.78916		4.8012		0.72839
290	1.0000e-05		2.3311		0.80981		3.7566		0.79158		4.7376		0.73093
295	1.0000e-05		2.1757		0.83111		3.6940		0.78894		4.7510		0.72886
300	1.0000e-05		2.1333		0.81571		3.6070		0.79353		4.9124		0.72473
305	1.0000e-05		2.0790		0.81559		3.7021		0.78914		4.8600		0.72605
310	1.0000e-06		2.5249		0.81775		3.9540		0.78282		4.8406		0.72444
315	1.0000e-06		2.5374		0.81135		3.7378		0.79249		4.8270		0.72532
320	1.0000e-06		2.6422		0.82082		3.7040		0.79217		4.6539		0.72558
325	1.0000e-06		2.6639		0.80453		3.5308		0.79903		4.7518		0.73042
330	1.0000e-06		2.6313		0.82338		3.6909		0.79450		4.6680		0.72787
335	1.0000e-06		2.7093		0.81149		3.7881		0.78608		4.8483		0.72781
340	1.0000e-06		2.8101		0.79741		3.8429		0.78511		4.7786		0.72681
345	1.0000e-06		1.9141		0.82004		3.7164		0.79360		4.6201		0.72480
350	1.0000e-06		2.4318		0.81854		3.7452		0.79006		4.7721		0.72631
355	1.0000e-06		2.3229		0.82497		3.6292		0.79630		4.6379		0.72827
360	1.0000e-06		2.4394		0.81181		3.7711		0.79001		4.7962		0.72363
365	1.0000e-07		2.5643		0.81631		3.6599		0.78997		4.8821		0.72961
370	1.0000e-07		2.2902		0.80787		4.0071		0.77631		5.1610		0.71885
375	1.0000e-07		3.0599		0.81175		3.6576		0.79163		4.8144		0.73295
380	1.0000e-07		2.3341		0.81696		3.7486		0.79430		4.8787		0.72467
385	1.0000e-07		2.4726		0.80110		3.6744		0.79267		4.8744		0.72326
390	1.0000e-07		2.5489		0.81963		3.7370		0.79161		4.5656		0.72856
395	1.0000e-07		2.2685		0.82278		3.7119		0.79431		4.8230		0.72482
400	1.0000e-07		2.6222		0.79373		3.7212		0.79219		4.7299		0.72403
405	1.0000e-07		2.3589		0.82916		3.6524		0.79468		4.8054		0.72932
410	1.0000e-07		2.0465		0.80685		3.8084		0.78707		4.8446		0.72818
415	1.0000e-07		2.2162		0.82649		3.6601		0.79304		4.7265		0.73080
420	1.0000e-08		2.5794		0.80442		3.8011		0.78735		4.8354		0.72727
425	1.0000e-08		2.3432		0.83136		3.7204		0.79465		4.5684		0.72814
430	1.0000e-08		2.4285		0.81457		3.7391		0.79070		4.7905		0.72465
435	1.0000e-08		2.3319		0.81676		3.7084		0.79442		4.6864		0.72534
440	1.0000e-08		1.8819		0.81806		3.7570		0.78956		4.9432		0.72183
445	1.0000e-08		2.7587		0.80406		3.6441		0.79091		4.9538		0.72947
450	1.0000e-08		2.1186		0.79266		3.8812		0.78191		4.9253		0.72059
455	1.0000e-08		2.3055		0.81700		3.8613		0.78410		4.7711		0.72492
460	1.0000e-08		2.3465		0.81828		3.7170		0.78930		4.7393		0.72715
465	1.0000e-08		2.0525		0.82023		3.6574		0.79282		4.8323		0.72548
470	1.0000e-08		2.2984		0.81922		3.5745		0.79999		4.5568		0.73015
475	1.0000e-08		2.1349		0.82797		3.7387		0.78892		4.8142		0.72733
480	1.0000e-08		2.0829		0.83317		3.6821		0.79360		4.7562		0.73077
485	1.0000e-08		2.1923		0.81150		3.8634		0.78780		4.7705		0.72703
490	1.0000e-08		2.2233		0.81299		3.6176		0.79541		4.6131		0.72835
495	1.0000e-08		2.1948		0.82019		3.7088		0.78784		4.9624		0.72896
500	1.0000e-08		2.7570		0.82792		3.7942		0.78970		4.8249		0.72348
505	1.0000e-08		2.4108		0.83069		3.6229		0.79558		4.6256		0.73040
510	1.0000e-08		1.9654		0.82498		3.8134		0.78664		4.7972		0.72235
515	1.0000e-08		2.1486		0.79386		3.7493		0.78842		4.7706		0.72579
520	1.0000e-08		2.4395		0.81899		3.7924		0.78715		4.7407		0.72915
525	1.0000e-08		2.3347		0.79511		3.6797		0.79552		4.6285		0.72601
530	1.0000e-08		2.2378		0.83606		3.6462		0.79732		4.5742		0.73080
535	1.0000e-08		2.0939		0.80196		3.6794		0.79146		4.6605		0.73341
540	1.0000e-08		2.2648		0.82070		3.6922		0.78928		4.7814		0.72722
545	1.0000e-08		2.2920		0.82405		3.6818		0.79491		4.5879		0.72845
550	1.0000e-08		2.4167		0.83172		3.6896		0.79364		4.6823		0.72777
555	1.0000e-08		2.2022		0.82509		3.6480		0.79442		4.8033		0.72875
560	1.0000e-08		2.2342		0.82347		3.6331		0.79442		4.8121		0.72729
565	1.0000e-08		2.1685		0.82327		3.6905		0.79135		4.8145		0.72201
570	1.0000e-08		2.1280		0.83188		3.7129		0.79140		4.7581		0.72367
575	1.0000e-08		2.4613		0.80865		3.7324		0.78874		5.0757		0.72049
580	1.0000e-08		2.2967		0.80995		3.6444		0.79375		4.7868		0.72533
