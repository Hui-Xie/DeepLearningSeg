Program ID 17135

Program command: 
 ['/home/hxie1/Projects/OvarianCancer/TrainSegV.py', '/home/hxie1/temp_netParameters/OvarianCancer/Label0_1', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainImages', '/home/hxie1/data/OvarianCancerCT/Extract_uniform/trainLabels', '2D', '0,1']

Major program changes: ConvSeqential use BatchNorm-reLU-Conv structure; 
                       ConsDense also use BatchNorm-reLU-Conv structure.
                       Add ConvSegDecreaseChannels
                       and each block has 5 layers, 
                       Residual connect to each Conv, 
                       skip at least 2 layers.
                       output layer use conv with 3*3 fiter instead of 1*1 filter. 
                       use boundary loss with weight 0 at beginning, and pretrain CE loss. 
                       special convInput Module
                       convOutput moudel uses 1*1 conv to tranparent gradident 
            

Program starting Time: 2019-05-08 10:08:44.157081
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/Label0_1

Infor: program test labels: [0, 1]
Infor: program suppressed labels: [2, 3]
Infor: program test labels: [0, 1]
Infor: program suppressed labels: [2, 3]
Building the Segmented Slice Tuple list, which may need 8 mins, please waiting......
Directory of /home/hxie1/data/OvarianCancerCT/Extract_uniform/trainLabels has 164 segmented slices for remained labels [0, 1].
Building the Segmented Slice Tuple list, which may need 8 mins, please waiting......
Directory of /home/hxie1/data/OvarianCancerCT/Extract_uniform/testLabels has 31 segmented slices for remained labels [0, 1].
Info: program uses 2D input.
TrainData Input:  batchSize=8, depth=1, height=281, width=281, NumClassfication=2

TestData Input:  batchSize=8, depth=1, height=281, width=281, NumClassfication=2

Network trains from scratch.
Network has total 404686466 parameters.
Info: network dropout rate = 0.3
Infor: Cross Entropy Weight: [1.0260619741432382, 39.37007874015748]

====================Net Architecture===========================
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1        [-1, 128, 281, 281]           1,280
       BatchNorm2d-2        [-1, 128, 281, 281]             256
              ReLU-3        [-1, 128, 281, 281]               0
            Conv2d-4        [-1, 128, 281, 281]         147,584
       BatchNorm2d-5        [-1, 128, 281, 281]             256
              ReLU-6        [-1, 128, 281, 281]               0
            Conv2d-7        [-1, 128, 281, 281]         147,584
       BatchNorm2d-8        [-1, 128, 281, 281]             256
              ReLU-9        [-1, 128, 281, 281]               0
           Conv2d-10        [-1, 128, 281, 281]         147,584
      BatchNorm2d-11        [-1, 128, 281, 281]             256
             ReLU-12        [-1, 128, 281, 281]               0
           Conv2d-13        [-1, 128, 281, 281]         147,584
      BatchNorm2d-14        [-1, 128, 281, 281]             256
             ReLU-15        [-1, 128, 281, 281]               0
           Conv2d-16        [-1, 128, 281, 281]         147,584
   ConvSequential-17        [-1, 128, 281, 281]               0
        ConvInput-18        [-1, 128, 281, 281]               0
      BatchNorm2d-19        [-1, 128, 281, 281]             256
           Conv2d-20        [-1, 128, 139, 139]         409,728
      BatchNorm2d-21        [-1, 128, 139, 139]             256
             ReLU-22        [-1, 128, 139, 139]               0
           Conv2d-23        [-1, 128, 139, 139]         147,584
      BatchNorm2d-24        [-1, 128, 139, 139]             256
             ReLU-25        [-1, 128, 139, 139]               0
           Conv2d-26        [-1, 128, 139, 139]         147,584
      BatchNorm2d-27        [-1, 128, 139, 139]             256
             ReLU-28        [-1, 128, 139, 139]               0
           Conv2d-29        [-1, 128, 139, 139]         147,584
      BatchNorm2d-30        [-1, 128, 139, 139]             256
             ReLU-31        [-1, 128, 139, 139]               0
           Conv2d-32        [-1, 128, 139, 139]         147,584
      BatchNorm2d-33        [-1, 128, 139, 139]             256
             ReLU-34        [-1, 128, 139, 139]               0
           Conv2d-35        [-1, 128, 139, 139]         147,584
   ConvSequential-36        [-1, 128, 139, 139]               0
         Down2dBB-37        [-1, 128, 139, 139]               0
        Dropout2d-38        [-1, 128, 139, 139]               0
      BatchNorm2d-39        [-1, 128, 139, 139]             256
           Conv2d-40          [-1, 256, 69, 69]         295,168
      BatchNorm2d-41          [-1, 256, 69, 69]             512
             ReLU-42          [-1, 256, 69, 69]               0
           Conv2d-43          [-1, 256, 69, 69]         590,080
      BatchNorm2d-44          [-1, 256, 69, 69]             512
             ReLU-45          [-1, 256, 69, 69]               0
           Conv2d-46          [-1, 256, 69, 69]         590,080
      BatchNorm2d-47          [-1, 256, 69, 69]             512
             ReLU-48          [-1, 256, 69, 69]               0
           Conv2d-49          [-1, 256, 69, 69]         590,080
      BatchNorm2d-50          [-1, 256, 69, 69]             512
             ReLU-51          [-1, 256, 69, 69]               0
           Conv2d-52          [-1, 256, 69, 69]         590,080
      BatchNorm2d-53          [-1, 256, 69, 69]             512
             ReLU-54          [-1, 256, 69, 69]               0
           Conv2d-55          [-1, 256, 69, 69]         590,080
   ConvSequential-56          [-1, 256, 69, 69]               0
         Down2dBB-57          [-1, 256, 69, 69]               0
        Dropout2d-58          [-1, 256, 69, 69]               0
      BatchNorm2d-59          [-1, 256, 69, 69]             512
           Conv2d-60          [-1, 512, 33, 33]       3,277,312
      BatchNorm2d-61          [-1, 512, 33, 33]           1,024
             ReLU-62          [-1, 512, 33, 33]               0
           Conv2d-63          [-1, 512, 33, 33]       2,359,808
      BatchNorm2d-64          [-1, 512, 33, 33]           1,024
             ReLU-65          [-1, 512, 33, 33]               0
           Conv2d-66          [-1, 512, 33, 33]       2,359,808
      BatchNorm2d-67          [-1, 512, 33, 33]           1,024
             ReLU-68          [-1, 512, 33, 33]               0
           Conv2d-69          [-1, 512, 33, 33]       2,359,808
      BatchNorm2d-70          [-1, 512, 33, 33]           1,024
             ReLU-71          [-1, 512, 33, 33]               0
           Conv2d-72          [-1, 512, 33, 33]       2,359,808
      BatchNorm2d-73          [-1, 512, 33, 33]           1,024
             ReLU-74          [-1, 512, 33, 33]               0
           Conv2d-75          [-1, 512, 33, 33]       2,359,808
   ConvSequential-76          [-1, 512, 33, 33]               0
         Down2dBB-77          [-1, 512, 33, 33]               0
        Dropout2d-78          [-1, 512, 33, 33]               0
      BatchNorm2d-79          [-1, 512, 33, 33]           1,024
           Conv2d-80         [-1, 1024, 15, 15]      13,108,224
      BatchNorm2d-81         [-1, 1024, 15, 15]           2,048
             ReLU-82         [-1, 1024, 15, 15]               0
           Conv2d-83         [-1, 1024, 15, 15]       9,438,208
      BatchNorm2d-84         [-1, 1024, 15, 15]           2,048
             ReLU-85         [-1, 1024, 15, 15]               0
           Conv2d-86         [-1, 1024, 15, 15]       9,438,208
      BatchNorm2d-87         [-1, 1024, 15, 15]           2,048
             ReLU-88         [-1, 1024, 15, 15]               0
           Conv2d-89         [-1, 1024, 15, 15]       9,438,208
      BatchNorm2d-90         [-1, 1024, 15, 15]           2,048
             ReLU-91         [-1, 1024, 15, 15]               0
           Conv2d-92         [-1, 1024, 15, 15]       9,438,208
      BatchNorm2d-93         [-1, 1024, 15, 15]           2,048
             ReLU-94         [-1, 1024, 15, 15]               0
           Conv2d-95         [-1, 1024, 15, 15]       9,438,208
   ConvSequential-96         [-1, 1024, 15, 15]               0
         Down2dBB-97         [-1, 1024, 15, 15]               0
        Dropout2d-98         [-1, 1024, 15, 15]               0
      BatchNorm2d-99         [-1, 1024, 15, 15]           2,048
          Conv2d-100           [-1, 2048, 7, 7]      18,876,416
     BatchNorm2d-101           [-1, 2048, 7, 7]           4,096
            ReLU-102           [-1, 2048, 7, 7]               0
          Conv2d-103           [-1, 2048, 7, 7]      37,750,784
     BatchNorm2d-104           [-1, 2048, 7, 7]           4,096
            ReLU-105           [-1, 2048, 7, 7]               0
          Conv2d-106           [-1, 2048, 7, 7]      37,750,784
     BatchNorm2d-107           [-1, 2048, 7, 7]           4,096
            ReLU-108           [-1, 2048, 7, 7]               0
          Conv2d-109           [-1, 2048, 7, 7]      37,750,784
     BatchNorm2d-110           [-1, 2048, 7, 7]           4,096
            ReLU-111           [-1, 2048, 7, 7]               0
          Conv2d-112           [-1, 2048, 7, 7]      37,750,784
     BatchNorm2d-113           [-1, 2048, 7, 7]           4,096
            ReLU-114           [-1, 2048, 7, 7]               0
          Conv2d-115           [-1, 2048, 7, 7]      37,750,784
  ConvSequential-116           [-1, 2048, 7, 7]               0
        Down2dBB-117           [-1, 2048, 7, 7]               0
       Dropout2d-118           [-1, 2048, 7, 7]               0
     BatchNorm2d-119           [-1, 2048, 7, 7]           4,096
 ConvTranspose2d-120         [-1, 1024, 15, 15]      18,875,392
     BatchNorm2d-121         [-1, 1024, 15, 15]           2,048
            ReLU-122         [-1, 1024, 15, 15]               0
          Conv2d-123         [-1, 1024, 15, 15]       9,438,208
     BatchNorm2d-124         [-1, 1024, 15, 15]           2,048
            ReLU-125         [-1, 1024, 15, 15]               0
          Conv2d-126         [-1, 1024, 15, 15]       9,438,208
     BatchNorm2d-127         [-1, 1024, 15, 15]           2,048
            ReLU-128         [-1, 1024, 15, 15]               0
          Conv2d-129         [-1, 1024, 15, 15]       9,438,208
     BatchNorm2d-130         [-1, 1024, 15, 15]           2,048
            ReLU-131         [-1, 1024, 15, 15]               0
          Conv2d-132         [-1, 1024, 15, 15]       9,438,208
     BatchNorm2d-133         [-1, 1024, 15, 15]           2,048
            ReLU-134         [-1, 1024, 15, 15]               0
          Conv2d-135         [-1, 1024, 15, 15]       9,438,208
  ConvSequential-136         [-1, 1024, 15, 15]               0
          Up2dBB-137         [-1, 1024, 15, 15]               0
       Dropout2d-138         [-1, 1024, 15, 15]               0
     BatchNorm2d-139         [-1, 2048, 15, 15]           4,096
 ConvTranspose2d-140          [-1, 512, 33, 33]      26,214,912
     BatchNorm2d-141          [-1, 512, 33, 33]           1,024
            ReLU-142          [-1, 512, 33, 33]               0
          Conv2d-143          [-1, 512, 33, 33]       2,359,808
     BatchNorm2d-144          [-1, 512, 33, 33]           1,024
            ReLU-145          [-1, 512, 33, 33]               0
          Conv2d-146          [-1, 512, 33, 33]       2,359,808
     BatchNorm2d-147          [-1, 512, 33, 33]           1,024
            ReLU-148          [-1, 512, 33, 33]               0
          Conv2d-149          [-1, 512, 33, 33]       2,359,808
     BatchNorm2d-150          [-1, 512, 33, 33]           1,024
            ReLU-151          [-1, 512, 33, 33]               0
          Conv2d-152          [-1, 512, 33, 33]       2,359,808
     BatchNorm2d-153          [-1, 512, 33, 33]           1,024
            ReLU-154          [-1, 512, 33, 33]               0
          Conv2d-155          [-1, 512, 33, 33]       2,359,808
  ConvSequential-156          [-1, 512, 33, 33]               0
          Up2dBB-157          [-1, 512, 33, 33]               0
       Dropout2d-158          [-1, 512, 33, 33]               0
     BatchNorm2d-159         [-1, 1024, 33, 33]           2,048
 ConvTranspose2d-160          [-1, 256, 69, 69]       6,553,856
     BatchNorm2d-161          [-1, 256, 69, 69]             512
            ReLU-162          [-1, 256, 69, 69]               0
          Conv2d-163          [-1, 256, 69, 69]         590,080
     BatchNorm2d-164          [-1, 256, 69, 69]             512
            ReLU-165          [-1, 256, 69, 69]               0
          Conv2d-166          [-1, 256, 69, 69]         590,080
     BatchNorm2d-167          [-1, 256, 69, 69]             512
            ReLU-168          [-1, 256, 69, 69]               0
          Conv2d-169          [-1, 256, 69, 69]         590,080
     BatchNorm2d-170          [-1, 256, 69, 69]             512
            ReLU-171          [-1, 256, 69, 69]               0
          Conv2d-172          [-1, 256, 69, 69]         590,080
     BatchNorm2d-173          [-1, 256, 69, 69]             512
            ReLU-174          [-1, 256, 69, 69]               0
          Conv2d-175          [-1, 256, 69, 69]         590,080
  ConvSequential-176          [-1, 256, 69, 69]               0
          Up2dBB-177          [-1, 256, 69, 69]               0
       Dropout2d-178          [-1, 256, 69, 69]               0
     BatchNorm2d-179          [-1, 512, 69, 69]           1,024
 ConvTranspose2d-180        [-1, 128, 139, 139]         589,952
     BatchNorm2d-181        [-1, 128, 139, 139]             256
            ReLU-182        [-1, 128, 139, 139]               0
          Conv2d-183        [-1, 128, 139, 139]         147,584
     BatchNorm2d-184        [-1, 128, 139, 139]             256
            ReLU-185        [-1, 128, 139, 139]               0
          Conv2d-186        [-1, 128, 139, 139]         147,584
     BatchNorm2d-187        [-1, 128, 139, 139]             256
            ReLU-188        [-1, 128, 139, 139]               0
          Conv2d-189        [-1, 128, 139, 139]         147,584
     BatchNorm2d-190        [-1, 128, 139, 139]             256
            ReLU-191        [-1, 128, 139, 139]               0
          Conv2d-192        [-1, 128, 139, 139]         147,584
     BatchNorm2d-193        [-1, 128, 139, 139]             256
            ReLU-194        [-1, 128, 139, 139]               0
          Conv2d-195        [-1, 128, 139, 139]         147,584
  ConvSequential-196        [-1, 128, 139, 139]               0
          Up2dBB-197        [-1, 128, 139, 139]               0
       Dropout2d-198        [-1, 128, 139, 139]               0
     BatchNorm2d-199        [-1, 256, 139, 139]             512
 ConvTranspose2d-200        [-1, 128, 281, 281]         819,328
     BatchNorm2d-201        [-1, 128, 281, 281]             256
            ReLU-202        [-1, 128, 281, 281]               0
          Conv2d-203        [-1, 128, 281, 281]         147,584
     BatchNorm2d-204        [-1, 128, 281, 281]             256
            ReLU-205        [-1, 128, 281, 281]               0
          Conv2d-206        [-1, 128, 281, 281]         147,584
     BatchNorm2d-207        [-1, 128, 281, 281]             256
            ReLU-208        [-1, 128, 281, 281]               0
          Conv2d-209        [-1, 128, 281, 281]         147,584
     BatchNorm2d-210        [-1, 128, 281, 281]             256
            ReLU-211        [-1, 128, 281, 281]               0
          Conv2d-212        [-1, 128, 281, 281]         147,584
     BatchNorm2d-213        [-1, 128, 281, 281]             256
            ReLU-214        [-1, 128, 281, 281]               0
          Conv2d-215        [-1, 128, 281, 281]         147,584
  ConvSequential-216        [-1, 128, 281, 281]               0
          Up2dBB-217        [-1, 128, 281, 281]               0
       Dropout2d-218        [-1, 128, 281, 281]               0
     BatchNorm2d-219        [-1, 256, 281, 281]             512
          Conv2d-220          [-1, 2, 281, 281]             514
      ConvOutput-221          [-1, 2, 281, 281]               0
================================================================
Total params: 404,686,466
Trainable params: 404,686,466
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.30
Forward/backward pass size (MB): 4503.46
Params size (MB): 1543.76
Estimated Total Size (MB): 6047.52
----------------------------------------------------------------
===================End of Net Architecture =====================

Info: program will use 4 GPUs.
Loss Functions List: FocalCELoss with weight of 1; 	BoundaryLoss with weight of 0; 
Hints: Test Dice_0 is the dice coeff for all non-zero labels
Hints: Test Dice_1 is for primary cancer(green), test Dice_2 is for metastasis(yellow), and test Dice_3 is for invaded lymph node(brown).
Hints: Test TPR_0 is the TPR for all non-zero labels
Hints: Test TPR_1 is for primary cancer(green), TPR_2 is for metastasis(yellow), and TPR_3 is for invaded lymph node(brown).

Epoch 	 TrainingLoss 	 TestLoss 	Dice_0	Dice_1	TPR_0	TPR_1
0 	 0.0950 	 0.0824 	0.272	0.272	0.998	0.998
1 	 0.0745 	 0.0799 	0.320	0.320	0.990	0.990
2 	 0.0677 	 0.0593 	0.411	0.411	0.929	0.929
3 	 0.0574 	 0.1407 	0.372	0.372	0.892	0.892
4 	 0.0619 	 0.1019 	0.380	0.380	0.983	0.983
5 	 0.0592 	 0.0879 	0.404	0.404	0.956	0.956
6 	 0.0507 	 0.0473 	0.469	0.469	0.977	0.977
7 	 0.0536 	 0.0540 	0.460	0.460	0.981	0.981
8 	 0.0493 	 0.0581 	0.445	0.445	0.959	0.959
9 	 0.0503 	 0.0622 	0.422	0.422	0.992	0.992
10 	 0.0490 	 0.0524 	0.494	0.494	0.953	0.953
11 	 0.0451 	 0.0645 	0.525	0.525	0.943	0.943
12 	 0.0475 	 0.0439 	0.552	0.552	0.959	0.959
13 	 0.0523 	 0.0484 	0.516	0.516	0.950	0.950
14 	 0.0461 	 0.0460 	0.481	0.481	0.982	0.982
15 	 0.0466 	 0.0997 	0.497	0.497	0.941	0.941
16 	 0.0461 	 0.0420 	0.428	0.428	0.987	0.987
17 	 0.0456 	 0.0493 	0.525	0.525	0.958	0.958
18 	 0.0421 	 0.0612 	0.460	0.460	0.966	0.966
19 	 0.0453 	 0.0924 	0.545	0.545	0.916	0.916
20 	 0.0434 	 0.0782 	0.598	0.598	0.917	0.917
21 	 0.0485 	 0.0478 	0.440	0.440	0.991	0.991
22 	 0.0420 	 0.0848 	0.444	0.444	0.961	0.961
23 	 0.0399 	 0.0863 	0.523	0.523	0.945	0.945
24 	 0.0356 	 0.0410 	0.562	0.562	0.955	0.955
25 	 0.0403 	 0.0340 	0.560	0.560	0.982	0.982
26 	 0.0409 	 0.1786 	0.619	0.619	0.886	0.886
27 	 0.0398 	 0.0683 	0.580	0.580	0.935	0.935
28 	 0.0375 	 0.0372 	0.519	0.519	0.988	0.988
29 	 0.0505 	 0.0780 	0.439	0.439	0.960	0.960
30 	 0.0510 	 0.0560 	0.407	0.407	0.963	0.963
31 	 0.0413 	 0.0500 	0.428	0.428	0.990	0.990
32 	 0.0399 	 0.0417 	0.469	0.469	0.986	0.986
33 	 0.0370 	 0.0438 	0.505	0.505	0.980	0.980
34 	 0.0325 	 0.0496 	0.518	0.518	0.945	0.945
35 	 0.0340 	 0.0497 	0.571	0.571	0.944	0.944
36 	 0.0335 	 0.0576 	0.604	0.604	0.928	0.928
37 	 0.0317 	 0.0415 	0.585	0.585	0.951	0.951
38 	 0.0354 	 0.0401 	0.492	0.492	0.995	0.995
39 	 0.0391 	 0.0618 	0.544	0.544	0.974	0.974
40 	 0.0424 	 0.0652 	0.592	0.592	0.936	0.936
41 	 0.0363 	 0.0415 	0.566	0.566	0.954	0.954
42 	 0.0325 	 0.0447 	0.563	0.563	0.945	0.945
43 	 0.0358 	 0.0913 	0.572	0.572	0.924	0.924
44 	 0.0341 	 0.0674 	0.609	0.609	0.940	0.940
45 	 0.0343 	 0.0675 	0.585	0.585	0.927	0.927
46 	 0.0347 	 0.0424 	0.553	0.553	0.962	0.962
47 	 0.0366 	 0.0690 	0.422	0.422	0.955	0.955
48 	 0.0483 	 0.0809 	0.564	0.564	0.931	0.931
49 	 0.0460 	 0.1951 	0.556	0.556	0.914	0.914
50 	 0.0381 	 0.0460 	0.529	0.529	0.959	0.959
51 	 0.0426 	 0.0519 	0.543	0.543	0.953	0.953
52 	 0.0370 	 0.0628 	0.602	0.602	0.926	0.926
53 	 0.0370 	 0.0481 	0.504	0.504	0.973	0.973
54 	 0.0336 	 0.0809 	0.583	0.583	0.925	0.925
55 	 0.0380 	 0.0362 	0.551	0.551	0.980	0.980
56 	 0.0430 	 0.0996 	0.580	0.580	0.925	0.925
57 	 0.0346 	 0.0376 	0.494	0.494	0.990	0.990
58 	 0.0323 	 0.0394 	0.580	0.580	0.967	0.967
59 	 0.0309 	 0.0519 	0.596	0.596	0.954	0.954
60 	 0.0287 	 0.0561 	0.574	0.574	0.961	0.961
61 	 0.0329 	 0.0575 	0.540	0.540	0.961	0.961
62 	 0.0320 	 0.0499 	0.545	0.545	0.969	0.969
63 	 0.0290 	 0.0742 	0.656	0.656	0.907	0.907
64 	 0.0287 	 0.0625 	0.595	0.595	0.939	0.939
65 	 0.0285 	 0.0677 	0.592	0.592	0.950	0.950
66 	 0.0290 	 0.0898 	0.562	0.562	0.903	0.903
67 	 0.0312 	 0.0764 	0.598	0.598	0.935	0.935
68 	 0.0299 	 0.0481 	0.566	0.566	0.962	0.962
69 	 0.0302 	 0.0765 	0.569	0.569	0.946	0.946
70 	 0.0274 	 0.0593 	0.591	0.591	0.952	0.952
71 	 0.0288 	 0.0882 	0.647	0.647	0.919	0.919
72 	 0.0336 	 0.0863 	0.594	0.594	0.932	0.932
73 	 0.0308 	 0.0752 	0.555	0.555	0.932	0.932
74 	 0.0297 	 0.0898 	0.611	0.611	0.896	0.896
75 	 0.0319 	 0.0764 	0.574	0.574	0.942	0.942
76 	 0.0268 	 0.0622 	0.611	0.611	0.937	0.937
77 	 0.0287 	 0.0432 	0.584	0.584	0.958	0.958
78 	 0.0293 	 0.0675 	0.586	0.586	0.937	0.937
79 	 0.0263 	 0.0530 	0.596	0.596	0.947	0.947
80 	 0.0373 	 0.0694 	0.377	0.377	0.976	0.976
81 	 0.0404 	 0.0623 	0.398	0.398	0.991	0.991
82 	 0.0371 	 0.1060 	0.543	0.543	0.933	0.933
83 	 0.0328 	 0.0858 	0.576	0.576	0.933	0.933
84 	 0.0316 	 0.0581 	0.599	0.599	0.941	0.941
85 	 0.0267 	 0.0748 	0.545	0.545	0.949	0.949
86 	 0.0304 	 0.0565 	0.541	0.541	0.961	0.961
87 	 0.0302 	 0.0551 	0.591	0.591	0.962	0.962
88 	 0.0276 	 0.0546 	0.588	0.588	0.960	0.960
89 	 0.0247 	 0.0633 	0.587	0.587	0.954	0.954
90 	 0.0258 	 0.0688 	0.597	0.597	0.950	0.950
91 	 0.0252 	 0.0682 	0.588	0.588	0.956	0.956
92 	 0.0246 	 0.0760 	0.609	0.609	0.933	0.933
93 	 0.0269 	 0.0781 	0.581	0.581	0.941	0.941
94 	 0.0292 	 0.0656 	0.603	0.603	0.941	0.941
95 	 0.0240 	 0.0664 	0.582	0.582	0.945	0.945
96 	 0.0235 	 0.0891 	0.581	0.581	0.872	0.872
97 	 0.0253 	 0.0839 	0.585	0.585	0.898	0.898
98 	 0.0282 	 0.0804 	0.590	0.590	0.938	0.938
99 	 0.0258 	 0.0744 	0.622	0.622	0.936	0.936
100 	 0.0258 	 0.0849 	0.599	0.599	0.939	0.939
101 	 0.0262 	 0.0859 	0.584	0.584	0.881	0.881
102 	 0.0231 	 0.1347 	0.561	0.561	0.789	0.789
103 	 0.0254 	 0.0958 	0.589	0.589	0.940	0.940
104 	 0.0236 	 0.0882 	0.565	0.565	0.881	0.881
105 	 0.0229 	 0.0719 	0.601	0.601	0.946	0.946
106 	 0.0255 	 0.0689 	0.590	0.590	0.943	0.943
107 	 0.0238 	 0.0878 	0.624	0.624	0.935	0.935
108 	 0.0248 	 0.0980 	0.585	0.585	0.942	0.942
109 	 0.0237 	 0.0794 	0.604	0.604	0.916	0.916
110 	 0.0252 	 0.1070 	0.539	0.539	0.842	0.842
111 	 0.0257 	 0.0876 	0.569	0.569	0.904	0.904
112 	 0.0226 	 0.1198 	0.523	0.523	0.816	0.816
113 	 0.0219 	 0.1258 	0.552	0.552	0.826	0.826
114 	 0.0221 	 0.1250 	0.560	0.560	0.809	0.809
115 	 0.0239 	 0.1161 	0.548	0.548	0.848	0.848
116 	 0.0235 	 0.1002 	0.618	0.618	0.918	0.918
117 	 0.0215 	 0.1368 	0.499	0.499	0.761	0.761
118 	 0.0228 	 0.1049 	0.561	0.561	0.876	0.876
119 	 0.0219 	 0.1064 	0.551	0.551	0.865	0.865
120 	 0.0206 	 0.1349 	0.532	0.532	0.820	0.820
121 	 0.0214 	 0.1363 	0.529	0.529	0.818	0.818
122 	 0.0207 	 0.1721 	0.484	0.484	0.762	0.762
123 	 0.0204 	 0.2052 	0.461	0.461	0.712	0.712
124 	 0.0211 	 0.1893 	0.489	0.489	0.748	0.748
125 	 0.0207 	 0.2265 	0.461	0.461	0.702	0.702
126 	 0.0219 	 0.0889 	0.602	0.602	0.907	0.907
127 	 0.0231 	 0.0794 	0.598	0.598	0.932	0.932
128 	 0.0215 	 0.0938 	0.574	0.574	0.874	0.874
129 	 0.0205 	 0.0994 	0.559	0.559	0.860	0.860
130 	 0.0216 	 0.1076 	0.591	0.591	0.882	0.882
131 	 0.0210 	 0.1274 	0.528	0.528	0.790	0.790
132 	 0.0232 	 0.1310 	0.562	0.562	0.856	0.856
133 	 0.0205 	 0.1194 	0.579	0.579	0.870	0.870
134 	 0.0206 	 0.1170 	0.539	0.539	0.837	0.837
135 	 0.0213 	 0.1355 	0.514	0.514	0.789	0.789
136 	 0.0195 	 0.1284 	0.508	0.508	0.790	0.790
137 	 0.0206 	 0.0988 	0.571	0.571	0.870	0.870
138 	 0.0198 	 0.1822 	0.494	0.494	0.731	0.731
139 	 0.0217 	 0.1853 	0.474	0.474	0.726	0.726
140 	 0.0197 	 0.1566 	0.512	0.512	0.777	0.777
141 	 0.0219 	 0.1848 	0.480	0.480	0.712	0.712
142 	 0.0207 	 0.1622 	0.462	0.462	0.715	0.715
143 	 0.0199 	 0.1552 	0.503	0.503	0.755	0.755
144 	 0.0209 	 0.1737 	0.453	0.453	0.723	0.723
145 	 0.0223 	 0.1689 	0.469	0.469	0.732	0.732
146 	 0.0214 	 0.0997 	0.549	0.549	0.853	0.853
147 	 0.0212 	 0.1104 	0.530	0.530	0.843	0.843
148 	 0.0224 	 0.1127 	0.560	0.560	0.841	0.841
149 	 0.0207 	 0.1300 	0.531	0.531	0.819	0.819
150 	 0.0187 	 0.1668 	0.502	0.502	0.737	0.737
151 	 0.0186 	 0.1655 	0.519	0.519	0.786	0.786
152 	 0.0199 	 0.1994 	0.485	0.485	0.712	0.712
153 	 0.0190 	 0.1815 	0.491	0.491	0.730	0.730
154 	 0.0200 	 0.1460 	0.533	0.533	0.816	0.816
155 	 0.0183 	 0.1792 	0.497	0.497	0.746	0.746
156 	 0.0208 	 0.1259 	0.534	0.534	0.833	0.833
157 	 0.0217 	 0.1558 	0.499	0.499	0.768	0.768
158 	 0.0197 	 0.1863 	0.471	0.471	0.709	0.709
159 	 0.0188 	 0.1619 	0.503	0.503	0.752	0.752
160 	 0.0184 	 0.1817 	0.494	0.494	0.748	0.748
161 	 0.0177 	 0.1284 	0.522	0.522	0.824	0.824
162 	 0.0178 	 0.1861 	0.483	0.483	0.740	0.740
