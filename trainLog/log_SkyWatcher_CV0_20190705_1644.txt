Program ID of SkyWatcher Network training:13932

Program command: 
 ['TrainSkyWatcher.py', '/home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/images_npy', '/home/hxie1/data/OvarianCancerCT/Extract_ps2_2_5/labels_npy', '/home/hxie1/data/OvarianCancerCT/patientResponseDict.json', '0']

Major program changes: 
                      along deeper layer, increase filter number.
                      10 fold cross validation, 0 fold for test.
                      data partition with patient ID, instead of VOI.
                      in image3dResponseDataMgr, random Crop ROI in the fly.
                      erase all normalization layers in the fully connected layers.
                      Crop ROI around the mass center in each labeled slice. 
                      use reSampleForSameDistribution in training set, but keep original ditribution in the test set
                      First implement 1000 epochs in the segmentation path, and then freeze the encoder and decoder, only train the ResponseBranch.  
                      epoch < 1000, the loss is pure segmentation loss;
                      add FC layer width = 256*49 at first FC layer, and halves along deeper FC layer.
                      add  dropout of 0.5 in FC layers.
                      add data window level adjust, slice Normalization, gausssian noise, random flip, 90/180/270 rotation. 
                      reset learning rate patience after 1000 epochs.
                      disable data augmentation in the validation data;
                      in response prediction path, learning rate decacy patience set as 200 instead of 30.
                      when disable data augmentation, choose the fixed center labeled slice from a patient.
                      epoch >= 1000,  training Encoder and FC branch, and freeze decode. this training is base on  log_SkyWatcher_CV0_20190704_2129.txt
                      
                      train from 1000 epoch to continue.
                                                    
 
Discarded changes:                      
                      training response branch per 5 epoch after epoch 100, while continuing train the segmenation branch.
                      which means that before epoch 100, the accuray data is a mess.
                      Add L2 norm regularization in Adam optimizer with weight 5e-4. 
                      Add dropout at Fully connected layer with dropout rate of 50%.  
                      focus loss  with weight [3.3, 1.4] for [0,1] class separately, as [0,1] uneven distribution.   
                       the loss is pure response loss with reinitialized learning rate 1e-3.
                      

Experiment setting for Image3d ROI to response:
Input CT data: 29*140*140  3D CT raw image ROI with spacing size(5*2*2)
segmentation label: 23*127*127 with spacing size(5*2*2) segmentation label with value (0,1,2) which erases lymph node label


Predictive Model: 1,  first 3-layer dense conv block with channel size 128.
                  2,  and 3 dense conv DownBB blocks,  each of which includes a stride 2 conv and 3-layers dense conv block; 
                  3,  and 3 fully connected layers  changes the tensor into size 2*1;
                  4,  final a softmax for binary classification;
                  Total network learning parameters are 119 million.
                  Network architecture is referred at https://github.com/Hui-Xie/OvarianCancer/blob/master/SkyWatcherModel2.py

response Loss Function:  focus loss with weight 1:1 as training data use response balance distribution with resample with replacement. 
segmentation loss function: focus loss  with weight [1.0416883685076772, 39.37007874015748, 68.39945280437757] for label (0, 1, 2)

Data:   training data has 153 patients, and valdiation data has 16 patients, under 10 fold partition.
        We randomize all data, and then assign same distrubtion of treat reponse 0,1 into to training set;
        Validation data set keep original distribution
        

Training strategy:  50% probability of data are mixed up with beta distribution with alpha =0.4, to feed into network for training. 
                    No other data augmentation, and no dropout.  

                    Learning Scheduler for segmentation:  Reduce learning rate on  plateau, and learning rate patience is 300 epochs.
                                                    
                    Learning Scheduler for response:      Reduce learning rate on  plateau, and learning rate patience is 30 epochs.
            

Program starting Time: 2019-07-05 16:44:53.414863
Info: this is the 0th fold leave for test in the 10-fold cross-validation.

Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher

Now program get 169 input files.
Infor: In all data of 169 files, respone 0 has 51 files,
	  and response 1 has 118 files, where positive response rate = 0.6982248520710059 in full data
Infor: Validation Set has 16 files,and Training Set has 153 files
Infor: Validataion set has 11 1's, and positive response rate = 0.6875
Infor: trainning set has 107 1's, and positive response rate = 0.6993464052287581
Infor: the drop_last data in the dataMgr may lead the number of validation set and training set less than above number.
Info: program uses Mixup with alpha=0.4, and mixupProb = 0.5.
Infor: program is in multi samples running model.
TrainTestData Input:  batchSize=6, depth=29, height=140, width=140

Info: the size of bottle neck in the net = (512, 1, 7, 7)

Program loads net from /home/hxie1/temp_netParameters/OvarianCancer/SkyWatcher.
Current best test performance: 0
Network has total 119,954,053 parameters.
Infor: Segmentation Cross Entropy Weight: [1.0416883685076772, 39.37007874015748, 68.39945280437757] for label (0, 1, 2)
Info: program will use 3 GPUs.
Loss Functions List: FocalCELoss with weight of 1; 	FocalCELoss with weight of 1; 
when epoch < 1000, only train segmentation, which means response accuracy are meaningless at these epoch.
when epoch >= 1000, only training response branch, which means segmentation accuracy should keep unchange.
Hints: Test Dice_0 is the dice coeff for all non-zero labels
Hints: Test Dice_1 is for primary cancer(green), 	
 test Dice_2 is for metastasis(yellow), 	
 and test Dice_3 is for invaded lymph node(brown).
Hints: Test TPR_0 is the TPR for all non-zero labels
Hints: Test TPR_1 is for primary cancer(green), 	
 TPR_2 is for metastasis(yellow), 	
 and TPR_3 is for invaded lymph node(brown).

Dice is based on all 2D segmented slices in the volume from weak annotation, not real 3D dice.

Hints: Optimal_Result = Yes = 1,  Optimal_Result = No = 0 

Epoch	TrLoss	Dice0	Dice1	Dice2	TPR_0	TPR_1	TPR_2	Accura	TPR_r	TNR_r		TsLoss	Dice0	Dice1	Dice2	TPR_0	TPR_1	TPR_2	Accura	TPR_r	TNR_r
1000	0.2135	0.178	0.184	0.151	0.556	0.378	0.504	0.5514	0.8318	0.2710		0.1667	0.185	0.021	0.116	0.691	0.020	0.600	0.6875	1.0000	0.0000
1005	0.1745	0.131	0.126	0.067	0.567	0.333	0.188	0.4439	0.4299	0.4579		0.1708	0.175	0.170	0.098	0.643	0.461	0.201	0.6875	1.0000	0.0000
1010	0.1743	0.146	0.145	0.086	0.567	0.370	0.222	0.4486	0.5140	0.3832		0.1726	0.159	0.172	0.119	0.560	0.417	0.262	0.6875	1.0000	0.0000
1015	0.1744	0.123	0.100	0.069	0.539	0.289	0.185	0.4766	0.0561	0.8972		0.1762	0.178	0.163	0.114	0.575	0.370	0.222	0.3125	0.0000	1.0000
1020	0.1736	0.133	0.122	0.069	0.568	0.302	0.166	0.4720	0.1495	0.7944		0.1756	0.178	0.170	0.117	0.627	0.479	0.219	0.3125	0.0000	1.0000
1025	0.1732	0.144	0.134	0.075	0.579	0.360	0.196	0.5234	0.2150	0.8318		0.1741	0.173	0.156	0.110	0.627	0.470	0.231	0.3125	0.0000	1.0000
1030	0.1735	0.153	0.149	0.088	0.571	0.333	0.201	0.5140	0.2804	0.7477		0.1761	0.182	0.153	0.110	0.611	0.384	0.228	0.3125	0.0000	1.0000
1035	0.1736	0.138	0.118	0.087	0.572	0.313	0.239	0.5140	1.0000	0.0280		0.1685	0.176	0.150	0.080	0.573	0.388	0.154	0.6875	1.0000	0.0000
1040	0.1751	0.124	0.104	0.070	0.506	0.286	0.205	0.4112	0.1402	0.6822		0.1755	0.162	0.159	0.113	0.556	0.406	0.222	0.3125	0.0000	1.0000
1045	0.1735	0.129	0.124	0.081	0.563	0.337	0.207	0.4766	0.7290	0.2243		0.1714	0.171	0.170	0.116	0.782	0.576	0.207	0.6875	1.0000	0.0000
1050	0.1734	0.137	0.123	0.080	0.561	0.310	0.219	0.5374	0.3551	0.7196		0.1742	0.182	0.161	0.107	0.623	0.456	0.209	0.3125	0.0000	1.0000
1055	0.1748	0.144	0.143	0.063	0.572	0.340	0.175	0.4673	0.4766	0.4579		0.1728	0.161	0.171	0.124	0.577	0.464	0.249	0.6875	1.0000	0.0000
1060	0.1730	0.141	0.127	0.079	0.570	0.318	0.201	0.5187	0.4766	0.5607		0.1741	0.187	0.182	0.096	0.654	0.476	0.195	0.3125	0.0000	1.0000
1065	0.1739	0.149	0.143	0.068	0.537	0.353	0.171	0.5000	0.2991	0.7009		0.1740	0.186	0.160	0.116	0.635	0.474	0.208	0.3125	0.0000	1.0000
1070	0.1731	0.135	0.142	0.067	0.553	0.345	0.201	0.4953	0.4206	0.5701		0.1734	0.166	0.173	0.128	0.589	0.470	0.239	0.3125	0.0000	1.0000
1075	0.1738	0.142	0.141	0.070	0.545	0.318	0.216	0.4953	0.2617	0.7290		0.1730	0.177	0.163	0.088	0.619	0.468	0.174	0.6875	1.0000	0.0000
1080	0.1735	0.136	0.111	0.065	0.563	0.317	0.200	0.4953	0.5981	0.3925		0.1731	0.188	0.182	0.135	0.676	0.544	0.246	0.6875	1.0000	0.0000
1085	0.1736	0.139	0.139	0.078	0.562	0.352	0.236	0.4907	0.8598	0.1215		0.1715	0.154	0.193	0.135	0.546	0.410	0.274	0.6875	1.0000	0.0000
1090	0.1733	0.134	0.117	0.067	0.596	0.355	0.183	0.5514	0.4112	0.6916		0.1742	0.181	0.180	0.104	0.633	0.508	0.194	0.3125	0.0000	1.0000
1095	0.1734	0.142	0.125	0.083	0.540	0.366	0.199	0.5047	0.2056	0.8037		0.1747	0.173	0.163	0.086	0.629	0.469	0.173	0.3125	0.0000	1.0000
1100	0.1734	0.129	0.137	0.052	0.556	0.370	0.158	0.4907	0.0654	0.9159		0.1761	0.155	0.180	0.131	0.579	0.460	0.268	0.3125	0.0000	1.0000
1105	0.1740	0.133	0.122	0.062	0.567	0.333	0.161	0.4673	0.0093	0.9252		0.1756	0.152	0.181	0.123	0.560	0.484	0.215	0.3125	0.0000	1.0000
1110	0.1732	0.150	0.158	0.070	0.546	0.321	0.174	0.5374	0.6355	0.4393		0.1730	0.187	0.154	0.099	0.601	0.382	0.188	0.6875	1.0000	0.0000
1115	0.1741	0.142	0.135	0.070	0.552	0.341	0.190	0.4346	0.7196	0.1495		0.1713	0.182	0.163	0.120	0.589	0.386	0.222	0.6875	1.0000	0.0000
1120	0.3426	0.089	0.026	0.071	0.401	0.040	0.349	0.4953	0.4299	0.5607		0.1395	0.086	0.030	0.039	0.309	0.023	0.181	0.6875	1.0000	0.0000
1125	0.1754	0.086	0.068	0.046	0.319	0.116	0.185	0.4579	0.3084	0.6075		0.1723	0.093	0.085	0.064	0.309	0.096	0.256	0.6875	1.0000	0.0000
1130	0.1752	0.094	0.069	0.047	0.323	0.117	0.163	0.4299	0.1963	0.6636		0.1762	0.099	0.083	0.069	0.352	0.102	0.309	0.3125	0.0000	1.0000
1135	0.1731	0.080	0.073	0.048	0.323	0.113	0.177	0.5187	0.8224	0.2150		0.1704	0.091	0.077	0.056	0.302	0.093	0.244	0.6875	1.0000	0.0000
1140	0.1746	0.124	0.132	0.053	0.416	0.265	0.174	0.5093	0.1121	0.9065		0.1754	0.178	0.169	0.019	0.377	0.199	0.060	0.3125	0.0000	1.0000
1145	0.1737	0.133	0.126	0.062	0.418	0.279	0.179	0.4907	0.3925	0.5888		0.1746	0.172	0.193	0.014	0.364	0.216	0.035	0.3125	0.0000	1.0000
1150	0.1742	0.129	0.129	0.047	0.428	0.270	0.171	0.4860	0.8131	0.1589		0.1707	0.188	0.170	0.022	0.419	0.219	0.065	0.6875	1.0000	0.0000
1155	0.1745	0.132	0.124	0.054	0.387	0.190	0.200	0.4766	0.8598	0.0935		0.1689	0.179	0.173	0.013	0.410	0.216	0.039	0.6875	1.0000	0.0000
1160	0.1733	0.123	0.162	0.047	0.417	0.313	0.182	0.5514	0.4019	0.7009		0.1752	0.193	0.169	0.040	0.451	0.223	0.082	0.3125	0.0000	1.0000
1165	0.1743	0.141	0.165	0.045	0.416	0.287	0.160	0.4720	0.8692	0.0748		0.1708	0.177	0.218	0.022	0.419	0.264	0.074	0.6875	1.0000	0.0000
1170	0.1742	0.085	0.104	0.029	0.291	0.170	0.118	0.4953	0.4393	0.5514		0.1736	0.128	0.133	0.022	0.269	0.147	0.090	0.3125	0.0000	1.0000
1175	0.1743	0.148	0.178	0.043	0.407	0.210	0.186	0.4533	0.6168	0.2897		0.1728	0.210	0.207	0.023	0.451	0.178	0.134	0.6875	1.0000	0.0000
1180	0.1743	0.154	0.157	0.062	0.387	0.225	0.204	0.4486	0.4860	0.4112		0.1745	0.203	0.223	0.012	0.422	0.188	0.126	0.3125	0.0000	1.0000
1185	0.1732	0.139	0.169	0.054	0.386	0.201	0.185	0.5187	0.3551	0.6822		0.1745	0.205	0.246	0.015	0.392	0.207	0.125	0.3125	0.0000	1.0000
1190	0.1733	0.150	0.161	0.048	0.391	0.193	0.177	0.5140	0.8037	0.2243		0.1723	0.213	0.252	0.024	0.364	0.203	0.133	0.6875	1.0000	0.0000
1195	0.1734	0.138	0.135	0.050	0.372	0.191	0.164	0.4907	0.9533	0.0280		0.1684	0.211	0.246	0.018	0.381	0.206	0.133	0.6875	1.0000	0.0000
1200	0.1728	0.143	0.156	0.043	0.389	0.218	0.148	0.5140	0.2243	0.8037		0.1777	0.204	0.248	0.013	0.429	0.224	0.130	0.3125	0.0000	1.0000
1205	0.1735	0.147	0.169	0.061	0.421	0.216	0.209	0.4766	0.4673	0.4860		0.1736	0.204	0.251	0.033	0.331	0.191	0.141	0.3125	0.0000	1.0000
1210	0.1742	0.149	0.146	0.047	0.384	0.171	0.172	0.4533	0.5047	0.4019		0.1709	0.214	0.275	0.016	0.389	0.231	0.126	0.6875	1.0000	0.0000
1215	0.1725	0.145	0.159	0.055	0.384	0.205	0.172	0.5561	0.4766	0.6355		0.1748	0.221	0.248	0.012	0.406	0.200	0.125	0.3125	0.0000	1.0000
1220	0.1732	0.138	0.125	0.058	0.382	0.168	0.179	0.5467	0.4112	0.6822		0.1740	0.212	0.238	0.027	0.362	0.193	0.133	0.3125	0.0000	1.0000
1225	0.1741	0.163	0.179	0.061	0.412	0.227	0.204	0.4626	0.5327	0.3925		0.1733	0.198	0.184	0.018	0.296	0.131	0.125	0.6875	1.0000	0.0000
1230	0.1732	0.129	0.117	0.046	0.378	0.180	0.186	0.4626	0.1682	0.7570		0.1753	0.220	0.251	0.022	0.415	0.236	0.133	0.3125	0.0000	1.0000
1235	0.1737	0.146	0.161	0.058	0.419	0.234	0.220	0.4907	0.1963	0.7850		0.1754	0.206	0.245	0.014	0.414	0.208	0.127	0.3125	0.0000	1.0000
1240	0.1740	0.155	0.139	0.075	0.410	0.158	0.232	0.4953	0.4206	0.5701		0.1737	0.200	0.229	0.011	0.496	0.289	0.145	0.3125	0.0000	1.0000
1245	0.1731	0.144	0.132	0.071	0.428	0.171	0.248	0.5234	0.5701	0.4766		0.1726	0.203	0.246	0.012	0.439	0.222	0.127	0.6875	1.0000	0.0000
1250	0.1727	0.140	0.156	0.055	0.379	0.196	0.198	0.5421	0.5981	0.4860		0.1725	0.205	0.224	0.022	0.310	0.159	0.129	0.6875	1.0000	0.0000
1255	0.1732	0.139	0.183	0.055	0.367	0.239	0.166	0.4953	0.3832	0.6075		0.1740	0.209	0.195	0.015	0.349	0.146	0.126	0.3125	0.0000	1.0000
1260	0.1741	0.142	0.160	0.053	0.401	0.183	0.196	0.4299	0.2243	0.6355		0.1743	0.214	0.249	0.015	0.393	0.194	0.127	0.3125	0.0000	1.0000
1265	0.1734	0.125	0.145	0.048	0.365	0.204	0.170	0.5093	0.4579	0.5607		0.1737	0.213	0.237	0.015	0.393	0.196	0.129	0.3125	0.0000	1.0000
1270	0.1730	0.148	0.124	0.053	0.422	0.163	0.190	0.5467	0.6916	0.4019		0.1725	0.217	0.233	0.014	0.364	0.175	0.125	0.6875	1.0000	0.0000
1275	0.1729	0.145	0.155	0.060	0.405	0.188	0.218	0.5748	0.5421	0.6075		0.1737	0.209	0.237	0.010	0.442	0.237	0.125	0.3125	0.0000	1.0000
1280	0.1732	0.149	0.185	0.057	0.397	0.217	0.210	0.5234	0.4673	0.5794		0.1737	0.201	0.244	0.011	0.470	0.271	0.123	0.3125	0.0000	1.0000
1285	0.1736	0.146	0.163	0.051	0.407	0.200	0.219	0.5000	0.5981	0.4019		0.1726	0.207	0.261	0.014	0.422	0.238	0.131	0.6875	1.0000	0.0000
1290	0.1734	0.139	0.133	0.057	0.406	0.178	0.202	0.4953	0.9533	0.0374		0.1697	0.208	0.236	0.019	0.376	0.199	0.130	0.6875	1.0000	0.0000
1295	0.1735	0.144	0.149	0.061	0.378	0.188	0.208	0.4907	0.6542	0.3271		0.1730	0.209	0.241	0.015	0.388	0.194	0.126	0.6875	1.0000	0.0000
1300	0.1733	0.135	0.159	0.049	0.387	0.195	0.170	0.5047	0.6449	0.3645		0.1723	0.201	0.244	0.011	0.436	0.264	0.126	0.6875	1.0000	0.0000
1305	0.1731	0.155	0.157	0.061	0.391	0.192	0.199	0.5093	0.6729	0.3458		0.1726	0.212	0.237	0.014	0.405	0.210	0.130	0.6875	1.0000	0.0000
1310	0.1743	0.138	0.158	0.060	0.375	0.207	0.199	0.4346	0.6355	0.2336		0.1722	0.205	0.266	0.012	0.422	0.232	0.122	0.6875	1.0000	0.0000
1315	0.1724	0.150	0.167	0.062	0.400	0.204	0.220	0.5607	0.7664	0.3551		0.1718	0.215	0.273	0.020	0.375	0.237	0.130	0.6875	1.0000	0.0000
1320	0.1733	0.158	0.152	0.061	0.440	0.197	0.213	0.4953	0.7103	0.2804		0.1720	0.214	0.241	0.013	0.399	0.195	0.126	0.6875	1.0000	0.0000
1325	0.1734	0.129	0.154	0.056	0.368	0.171	0.184	0.4953	0.6636	0.3271		0.1721	0.210	0.231	0.014	0.372	0.190	0.126	0.6875	1.0000	0.0000
1330	0.1740	0.139	0.144	0.046	0.397	0.170	0.189	0.4813	0.7196	0.2430		0.1711	0.200	0.196	0.020	0.304	0.138	0.126	0.6875	1.0000	0.0000
1335	0.1734	0.152	0.184	0.062	0.437	0.217	0.232	0.5187	0.4112	0.6262		0.1741	0.208	0.252	0.012	0.432	0.224	0.126	0.3125	0.0000	1.0000
1340	0.1740	0.150	0.151	0.050	0.400	0.198	0.196	0.4953	0.3084	0.6822		0.1745	0.205	0.232	0.013	0.403	0.189	0.125	0.3125	0.0000	1.0000
1345	0.1733	0.144	0.134	0.065	0.393	0.181	0.200	0.5280	0.5047	0.5514		0.1735	0.223	0.226	0.040	0.429	0.198	0.146	0.3125	0.0000	1.0000
1350	0.1730	0.140	0.148	0.044	0.395	0.185	0.152	0.4953	0.4299	0.5607		0.1739	0.213	0.241	0.013	0.411	0.201	0.126	0.3125	0.0000	1.0000
1355	0.1728	0.154	0.170	0.040	0.401	0.223	0.163	0.5421	0.3458	0.7383		0.1740	0.206	0.217	0.022	0.335	0.168	0.129	0.3125	0.0000	1.0000
1360	0.1733	0.144	0.147	0.048	0.389	0.173	0.196	0.5093	0.4206	0.5981		0.1742	0.196	0.266	0.038	0.299	0.205	0.132	0.3125	0.0000	1.0000
1365	0.1737	0.136	0.145	0.059	0.367	0.181	0.194	0.4813	0.7009	0.2617		0.1723	0.203	0.226	0.014	0.428	0.202	0.133	0.6875	1.0000	0.0000
1370	0.1729	0.142	0.148	0.060	0.383	0.179	0.205	0.5280	0.5047	0.5514		0.1729	0.208	0.237	0.012	0.419	0.213	0.127	0.6875	1.0000	0.0000
1375	0.1737	0.152	0.116	0.054	0.404	0.162	0.176	0.5000	0.3271	0.6729		0.1750	0.203	0.250	0.014	0.428	0.232	0.128	0.3125	0.0000	1.0000
1380	0.1737	0.150	0.183	0.040	0.399	0.226	0.170	0.4813	0.5234	0.4393		0.1723	0.207	0.232	0.010	0.430	0.191	0.125	0.6875	1.0000	0.0000
1385	0.1739	0.119	0.102	0.057	0.373	0.117	0.234	0.5047	0.6636	0.3458		0.1715	0.197	0.166	0.016	0.389	0.157	0.149	0.6875	1.0000	0.0000
1390	0.1735	0.120	0.128	0.061	0.370	0.155	0.249	0.5000	0.4486	0.5514		0.1738	0.173	0.148	0.022	0.310	0.159	0.131	0.3125	0.0000	1.0000
1395	0.1730	0.123	0.088	0.058	0.335	0.093	0.214	0.5327	0.4766	0.5888		0.1731	0.201	0.195	0.022	0.371	0.186	0.144	0.6875	1.0000	0.0000
1400	0.1737	0.122	0.109	0.064	0.379	0.128	0.280	0.4533	0.5514	0.3551		0.1730	0.166	0.146	0.011	0.256	0.136	0.107	0.6875	1.0000	0.0000
1405	0.1730	0.128	0.106	0.061	0.393	0.144	0.272	0.5234	0.5514	0.4953		0.1732	0.186	0.176	0.024	0.357	0.178	0.146	0.6875	1.0000	0.0000
1410	0.1736	0.118	0.087	0.049	0.356	0.086	0.238	0.4860	0.6822	0.2897		0.1723	0.171	0.124	0.029	0.301	0.116	0.142	0.6875	1.0000	0.0000
1415	0.1741	0.123	0.124	0.055	0.367	0.136	0.237	0.4813	0.6355	0.3271		0.1725	0.181	0.146	0.028	0.359	0.128	0.160	0.6875	1.0000	0.0000
1420	0.1735	0.126	0.090	0.062	0.342	0.095	0.247	0.4533	0.7196	0.1869		0.1722	0.207	0.268	0.025	0.446	0.237	0.167	0.6875	1.0000	0.0000
1425	0.1732	0.117	0.092	0.053	0.332	0.103	0.223	0.5561	0.5234	0.5888		0.1731	0.187	0.176	0.030	0.371	0.153	0.162	0.6875	1.0000	0.0000
1430	0.1733	0.108	0.070	0.049	0.335	0.102	0.223	0.5467	0.4112	0.6822		0.1735	0.204	0.179	0.030	0.377	0.142	0.158	0.3125	0.0000	1.0000
1435	0.1732	0.113	0.117	0.053	0.325	0.142	0.209	0.5140	0.8037	0.2243		0.1718	0.168	0.152	0.028	0.299	0.132	0.137	0.6875	1.0000	0.0000
1440	0.1738	0.112	0.081	0.050	0.346	0.090	0.224	0.4626	0.4860	0.4393		0.1731	0.211	0.298	0.020	0.431	0.274	0.149	0.6875	1.0000	0.0000
1445	0.1740	0.109	0.080	0.052	0.312	0.098	0.192	0.4533	0.7570	0.1495		0.1721	0.203	0.243	0.023	0.372	0.213	0.141	0.6875	1.0000	0.0000
1450	0.1733	0.119	0.114	0.054	0.326	0.125	0.224	0.4953	0.6262	0.3645		0.1728	0.197	0.197	0.032	0.355	0.160	0.151	0.6875	1.0000	0.0000
1455	0.1734	0.127	0.109	0.064	0.363	0.126	0.261	0.4907	0.8505	0.1308		0.1718	0.184	0.133	0.028	0.316	0.125	0.146	0.6875	1.0000	0.0000
1460	0.1733	0.130	0.106	0.064	0.356	0.107	0.242	0.4907	0.8505	0.1308		0.1722	0.214	0.244	0.027	0.422	0.204	0.157	0.6875	1.0000	0.0000
1465	0.1734	0.130	0.104	0.070	0.352	0.110	0.283	0.5093	0.5981	0.4206		0.1730	0.180	0.185	0.030	0.336	0.156	0.152	0.6875	1.0000	0.0000
1470	0.1734	0.049	0.022	0.027	0.179	0.033	0.127	0.5187	0.2897	0.7477		0.1741	0.021	0.018	0.004	0.029	0.012	0.016	0.3125	0.0000	1.0000
1475	0.1728	0.047	0.010	0.038	0.190	0.016	0.161	0.5280	0.3925	0.6636		0.1743	0.057	0.062	0.005	0.091	0.045	0.026	0.3125	0.0000	1.0000
1480	0.1737	0.036	0.006	0.029	0.147	0.009	0.139	0.4720	0.2336	0.7103		0.1740	0.024	0.014	0.026	0.102	0.012	0.117	0.3125	0.0000	1.0000
1485	0.1736	0.032	0.005	0.034	0.140	0.007	0.152	0.4533	0.3645	0.5421		0.1742	0.026	0.014	0.024	0.133	0.013	0.152	0.3125	0.0000	1.0000
1490	0.1731	0.043	0.018	0.042	0.157	0.023	0.166	0.5421	0.3084	0.7757		0.1743	0.029	0.015	0.024	0.141	0.014	0.182	0.3125	0.0000	1.0000
1495	0.1735	0.039	0.018	0.036	0.169	0.029	0.168	0.4439	0.4019	0.4860		0.1734	0.024	0.011	0.026	0.092	0.010	0.096	0.3125	0.0000	1.0000
1500	0.1734	0.043	0.011	0.029	0.188	0.010	0.156	0.4720	0.3364	0.6075		0.1735	0.031	0.018	0.028	0.107	0.015	0.151	0.3125	0.0000	1.0000
1505	0.1733	0.040	0.016	0.028	0.154	0.019	0.117	0.4953	0.6449	0.3458		0.1745	0.051	0.016	0.041	0.223	0.012	0.279	0.6250	0.9091	0.0000
1510	0.1731	0.050	0.021	0.039	0.186	0.022	0.163	0.5514	0.7477	0.3551		0.1722	0.072	0.027	0.042	0.153	0.017	0.098	0.6875	1.0000	0.0000
1515	0.1735	0.057	0.019	0.037	0.209	0.031	0.149	0.5047	0.9159	0.0935		0.1715	0.084	0.053	0.058	0.272	0.044	0.213	0.6875	1.0000	0.0000
