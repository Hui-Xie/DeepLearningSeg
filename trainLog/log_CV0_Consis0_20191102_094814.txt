=============training from sratch============
Program ID: 24539

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/labels_npy', '0', '3', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
      Oct 23th, 2019
      1   change to MaxPool with 2*2*2 with stride 2;
      2   Loss use GeneralizedDiceLoss function;
      
      Oct 25th, 2019
      1   add trainAllData switch
      
      Oct 29th, 2019
      1   revise reduce learnging rate at training loss;
      2   reduce min_lr = 1e-9
      
      Nov 1st, 2019
      1   erase batchNorm, for training.
      
      Nov 2nd, 2019
      1   Use InstanceNormalization, instead of BatchNorm 
        
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-11-02 09:48:14.993259
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191102_094814

Info: this is the 0th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191102_094814.
All files are in one partition.

all dataset: total 35 image files.
Network has total 254,616,050 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		0.7794		0.21612
5	1.0000e-02		0.5311		0.42845
10	1.0000e-02		0.4871		0.48604
15	1.0000e-02		0.4602		0.50483
20	1.0000e-02		0.4714		0.49052
25	1.0000e-02		0.4753		0.49336
30	1.0000e-02		0.4524		0.50253
35	1.0000e-02		0.4052		0.55845
40	1.0000e-02		0.4105		0.55593
45	1.0000e-02		0.4121		0.55640
50	1.0000e-02		0.3529		0.61897
55	1.0000e-02		0.3778		0.59463
60	1.0000e-02		0.3787		0.59074
65	1.0000e-02		0.3358		0.63669
70	1.0000e-02		0.3860		0.57935
75	1.0000e-02		0.3727		0.60133
80	1.0000e-02		0.3340		0.64034
85	1.0000e-02		0.3670		0.60280
90	1.0000e-02		0.3351		0.64281
95	1.0000e-02		0.3398		0.63952
100	1.0000e-02		0.3258		0.65076
105	1.0000e-02		0.2838		0.69847
110	1.0000e-02		0.3070		0.67592
115	1.0000e-02		0.3223		0.65980
120	1.0000e-02		0.3439		0.63095
125	1.0000e-02		0.2887		0.69385
130	1.0000e-02		0.2961		0.68183
135	1.0000e-02		0.2909		0.68633
140	1.0000e-02		0.3082		0.67307
145	1.0000e-02		0.2901		0.69271
150	1.0000e-02		0.2930		0.68530
155	1.0000e-02		0.2306		0.75585
160	1.0000e-02		0.2292		0.75765
165	1.0000e-02		0.2857		0.69913
170	1.0000e-02		0.2574		0.72833
175	1.0000e-02		0.2660		0.71804
180	1.0000e-02		0.2629		0.72236
185	1.0000e-02		0.2415		0.74274
190	1.0000e-02		0.2725		0.71305
195	1.0000e-02		0.2568		0.73022
200	1.0000e-02		0.2368		0.75043
