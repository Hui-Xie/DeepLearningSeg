=============training from sratch============
Program ID: 32190

Program command: 
 ['TrainSegV3d_ROI.py', '/home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI', '1', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/training/nrrd_npy', '/home/hxie1/data/OvarianCancerCT/primaryROI1_1_3/training/labels_npy', '0', '1', '0']

Major program changes: 
      1  3D V model for primary cancer ROI;
      2  Uniform ROI size: 51*171*171 in z,y,x directon;
      3  Total 36 patient data, in which training data 24 patients, validation 6 patients, and test 6 patients;
      4  all 36 patients data have 50-80% 3D label;
      5  Dice coefficient is 3D dice coefficient against corresponding 3D ground truth;
      6  training data augmentation in the fly: affine in XY plane, translation in Z direction;
      7  In the bottle neck of V model, the latent vector has size of 512*2*9*9;
      Sep 16th, 2019:
      1   add dynamic loss weight according trainin  data;
      2   refine learning rate decay.
      Sep 21st, 2019
      1   add improved Boundary Loss2, and inherit the previous learningrate of network of pure CELoss;
      Sep 23rd, 2019:
      1   improve mean of boundary loss limited on the A,B regions;
      2   use log(segProb) instead of segProb in the boudary loss;
      3   CrossEntropy weight reduces 0.01 per 5 epochs from 1 to 0.01, while boundary Loss weight increase 0.01 per 5 epochs from 0.01 to 1. 
      Sep 24th, 2019
      1   Use boundaryLoss1, which is considering the whole volume. 
      Sep 25th, 2019
      1   use boundaryLoss3, which is a stronger gradient signal to improve loss.
      2   unbalanced weight for class is applied on logP,and just use boundaryLoss3 with CELoss.
      3   use CELoss and boundaryLoss together.
      4   Use truncated DistanceCrossEntropy Loss alone;
      5   change LRScheduler into reduce into Plateau with initial LR=0.1
      Sep 26th, 2019
      1   Add one layer in the bottom of V model;
      2   Add residual connnection in each layer;
      Sep 30th, 2019
      1   With size-reduced ROI of size 51*149*149;
      2   reduce the translation of data augmentation;
      3   reduce all data into 35 patients, excluding a very blur patient.
      Oct 5th, 2019
      1   use uniform physical size 147mm*147mm*147mm, input pixel size: 49*147*147 with spacing size 3mm*1mm*1mm;
      2   change V model with inputsize 49*147*147
      Oct 6th, 2019
      1   add filter number to 48 at the first layer. 
      Oct 7th, 2019
      1   restore to 32 of number of filters in the first layer;
      2   add bottom number of filters to 1024, and keep down sample and add filter number together. 
      Oct 8th, 2019
      1   discard the cancer with size exceeding 147mm*147mm*147mm; Now remains 29 patients data; 
      Oct 9th, 2019
      1   In the first layer of V model, remove the residual link; 
           with the residula link at first layer: Tr dice:54%, Validation Dice 27%, Test Dice 56%;  Not good.
      2   the final output layer, change into 1*1*1 convolution, instead of 3*3*3 convolution;
      3   add labelConsistencyLoss, it use 64 dimension feature extracted from 2 ends of V model:
           It gets stable Training Dice 61%, validation Dice 27%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      Oct 11th, 2019
      1   use feature tensor just from the output end of V model. It is 32 dimensions.
          It gets stable Training Dice 61%, validation Dice 23%, and test dice 49%, for fold 0 in the fixed physical size:147mm*147mm*147mm; 
      2   windows size for consistency loss changes to 3;
      Oct 12th, 2019
      1   change image window level to 100/50; relaunch training;
      2   change consistencyLoss to use ground truth for comparing diff of feature vector;
      Oct 13th, 2019
      1    use conistencyLoss3: ((G1-G2)-(P1-P2))**2 as loss.
      
      Oct 18th, 2019
      1   use 48 filters at the first layer with inputsize 49*147*147 with scaled ROI.
      
      Oct 20th, 2019
      1   at final output layer of V model, change 1*1*1 conv to 5*5*5 conv, in order to consider context for final output
      
      Oct 23th, 2019
      1   change to MaxPool with 2*2*2 with stride 2;
      2   Loss use GeneralizedDiceLoss function;
      
      Oct 25th, 2019
      1   add trainAllData switch
      
      Oct 29th, 2019
      1   revise reduce learnging rate at training loss;
      2   reduce min_lr = 1e-9
      
      Nov 1st, 2019
      1   erase batchNorm, for training.
      
      Nov 2nd, 2019
      1   Use InstanceNormalization, instead of BatchNorm
      
      Nov 25th, 2019
      1   Add boundary loss into training.  
      
      Dec 3rd, 2019
      1  cancel Boundary Loss;
      2  add 13 network generated files with accuracy >88% into 3D ground truth, then train;
      
      Dec 7th, 2019
      1  Change latent vector in the V model into 1*1 in feature map size.
      
      
        
      
       
      

          
         

Discarded changes:                  
          

Program starting Time: 2019-12-07 13:51:06.039363
Info: netPath = /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191207_135106

Info: this is the 0th fold leave for test in the 6-fold cross-validation.

Info: batchSize = 1

Info: useConsistencyLoss = False and searchWindowSize= 0

Net parameters is saved in  /home/hxie1/temp_netParameters/OvarianCancer/SegV3d_ROI/20191207_135106.
All files are in one partition.

all dataset: total 35 image files.
Network has total 155,528,690 parameters.


************** Table of Training Log **************
Epoch	LearningRate		TrainingLoss	Dice		ValidationLoss	Dice		TestLoss	Dice
0	1.0000e-02		0.7792		0.22582
5	1.0000e-02		0.5235		0.42207
10	1.0000e-02		0.5368		0.42164
15	1.0000e-02		0.4813		0.46554
20	1.0000e-02		0.5078		0.45409
25	1.0000e-02		0.4748		0.49129
30	1.0000e-02		0.4255		0.52403
35	1.0000e-02		0.4467		0.50878
40	1.0000e-02		0.3915		0.56769
45	1.0000e-02		0.4317		0.53616
50	1.0000e-02		0.3721		0.59799
55	1.0000e-02		0.3709		0.59481
60	1.0000e-02		0.3141		0.66464
65	1.0000e-02		0.3891		0.58567
70	1.0000e-02		0.3394		0.63395
75	1.0000e-02		0.3216		0.65135
80	1.0000e-02		0.3134		0.66762
85	1.0000e-02		0.3805		0.59043
90	1.0000e-02		0.3167		0.65571
95	1.0000e-02		0.3536		0.61944
100	1.0000e-02		0.3233		0.65201
105	1.0000e-02		0.3139		0.66469
110	1.0000e-02		0.2866		0.69533
115	1.0000e-02		0.3078		0.67003
120	1.0000e-02		0.2839		0.69500
125	1.0000e-02		0.3255		0.65408
130	1.0000e-02		0.2700		0.71532
135	1.0000e-02		0.2787		0.70342
140	1.0000e-02		0.2518		0.72620
145	1.0000e-02		0.2837		0.69986
150	1.0000e-02		0.3354		0.64342
155	1.0000e-02		0.2993		0.67948
160	1.0000e-02		0.2405		0.74768
165	1.0000e-02		0.2540		0.73078
170	1.0000e-02		0.2816		0.69644
175	1.0000e-02		0.2494		0.73602
180	1.0000e-02		0.2632		0.71693
185	1.0000e-02		0.2532		0.73058
190	1.0000e-02		0.2601		0.72210
195	1.0000e-02		0.2170		0.77029
200	1.0000e-02		0.2326		0.75098
205	1.0000e-02		0.2043		0.78020
210	1.0000e-02		0.2164		0.77097
215	1.0000e-02		0.2679		0.71633
220	1.0000e-02		0.2837		0.69779
225	1.0000e-02		0.2380		0.74743
230	1.0000e-02		0.2194		0.76637
235	1.0000e-02		0.2696		0.69742
240	1.0000e-02		0.2112		0.77614
245	1.0000e-02		0.2403		0.74099
250	1.0000e-02		0.2601		0.72639
255	1.0000e-02		0.1797		0.80733
260	1.0000e-02		0.1814		0.80898
265	1.0000e-02		0.2033		0.78258
270	1.0000e-02		0.1824		0.80739
275	1.0000e-02		0.1842		0.80555
280	1.0000e-02		0.1853		0.80410
285	1.0000e-02		0.2044		0.78649
290	1.0000e-02		0.2217		0.76453
295	1.0000e-02		0.1679		0.82262
300	1.0000e-02		0.1677		0.82422
305	1.0000e-02		0.2312		0.75627
310	1.0000e-02		0.2177		0.76541
315	1.0000e-02		0.2172		0.77121
320	1.0000e-02		0.1713		0.82002
325	1.0000e-02		0.1744		0.81350
330	1.0000e-02		0.1781		0.81101
335	1.0000e-02		0.1708		0.81856
340	1.0000e-02		0.1599		0.82613
345	1.0000e-02		0.1547		0.83778
350	1.0000e-02		0.1890		0.79703
355	1.0000e-02		0.1835		0.80315
360	1.0000e-02		0.1663		0.82437
365	1.0000e-02		0.1943		0.79728
370	1.0000e-02		0.1595		0.83050
375	1.0000e-02		0.1744		0.81438
380	1.0000e-02		0.1662		0.82426
385	1.0000e-02		0.1741		0.81398
390	1.0000e-02		0.1372		0.85446
395	1.0000e-02		0.1633		0.82844
400	1.0000e-02		0.1327		0.85991
405	1.0000e-02		0.1469		0.84427
410	1.0000e-02		0.1406		0.85043
415	1.0000e-02		0.1303		0.86210
420	1.0000e-02		0.1599		0.83168
425	1.0000e-02		0.1260		0.86664
430	1.0000e-02		0.1356		0.85542
435	1.0000e-02		0.1503		0.84016
440	1.0000e-02		0.1290		0.86387
445	1.0000e-02		0.1545		0.83705
450	1.0000e-02		0.1220		0.87072
455	1.0000e-02		0.1218		0.87045
460	1.0000e-02		0.1026		0.89045
465	1.0000e-02		0.1175		0.87612
470	1.0000e-02		0.1284		0.86425
475	1.0000e-02		0.1137		0.87914
480	1.0000e-02		0.1323		0.86106
485	1.0000e-02		0.1564		0.83480
490	1.0000e-02		0.1259		0.86495
495	1.0000e-02		0.1287		0.86350
500	1.0000e-02		0.1033		0.89109
505	1.0000e-02		0.1284		0.86404
510	1.0000e-02		0.1209		0.87274
515	1.0000e-03		0.1154		0.87794
520	1.0000e-03		0.1067		0.88794
525	1.0000e-03		0.1012		0.89238
530	1.0000e-03		0.1048		0.88960
535	1.0000e-03		0.0962		0.89869
540	1.0000e-03		0.0980		0.89637
545	1.0000e-03		0.1074		0.88634
550	1.0000e-03		0.0956		0.89864
555	1.0000e-03		0.0910		0.90392
560	1.0000e-03		0.0962		0.89873
565	1.0000e-03		0.1035		0.89122
570	1.0000e-03		0.0940		0.90059
575	1.0000e-03		0.0811		0.91419
580	1.0000e-03		0.0936		0.90171
585	1.0000e-03		0.0869		0.90723
590	1.0000e-03		0.0953		0.89865
595	1.0000e-03		0.0812		0.91356
600	1.0000e-03		0.0835		0.91091
605	1.0000e-03		0.0902		0.90411
610	1.0000e-03		0.0858		0.90927
615	1.0000e-03		0.0791		0.91648
620	1.0000e-03		0.0827		0.91234
625	1.0000e-03		0.0755		0.91960
630	1.0000e-03		0.0846		0.91051
635	1.0000e-03		0.0851		0.91041
640	1.0000e-03		0.0892		0.90553
645	1.0000e-03		0.0863		0.90850
650	1.0000e-03		0.0828		0.91230
655	1.0000e-03		0.0822		0.91225
660	1.0000e-03		0.0830		0.91185
665	1.0000e-03		0.0748		0.92018
670	1.0000e-03		0.0819		0.91331
675	1.0000e-03		0.0780		0.91671
680	1.0000e-03		0.0816		0.91326
685	1.0000e-03		0.0754		0.91925
690	1.0000e-03		0.0813		0.91396
695	1.0000e-03		0.0805		0.91456
700	1.0000e-03		0.0811		0.91383
705	1.0000e-03		0.0769		0.91855
710	1.0000e-03		0.0825		0.91276
715	1.0000e-03		0.0810		0.91420
720	1.0000e-04		0.0767		0.91842
725	1.0000e-04		0.0754		0.91996
730	1.0000e-04		0.0799		0.91548
735	1.0000e-04		0.0805		0.91473
740	1.0000e-04		0.0762		0.91874
745	1.0000e-04		0.0816		0.91315
750	1.0000e-04		0.0765		0.91817
755	1.0000e-04		0.0648		0.93133
760	1.0000e-04		0.0738		0.92210
765	1.0000e-04		0.0804		0.91477
770	1.0000e-04		0.0776		0.91806
775	1.0000e-04		0.0782		0.91742
780	1.0000e-04		0.0778		0.91683
785	1.0000e-04		0.0727		0.92294
790	1.0000e-04		0.0706		0.92511
795	1.0000e-04		0.0745		0.92106
800	1.0000e-04		0.0768		0.91843
805	1.0000e-04		0.0785		0.91640
810	1.0000e-05		0.0790		0.91617
815	1.0000e-05		0.0761		0.91911
820	1.0000e-05		0.0753		0.92067
825	1.0000e-05		0.0754		0.92012
830	1.0000e-05		0.0771		0.91899
835	1.0000e-05		0.0756		0.92032
840	1.0000e-05		0.0705		0.92548
845	1.0000e-05		0.0752		0.92053
850	1.0000e-05		0.0747		0.92104
855	1.0000e-05		0.0743		0.92079
860	1.0000e-05		0.0722		0.92330
865	1.0000e-06		0.0711		0.92457
870	1.0000e-06		0.0721		0.92370
875	1.0000e-06		0.0770		0.91840
880	1.0000e-06		0.0735		0.92217
885	1.0000e-06		0.0708		0.92532
890	1.0000e-06		0.0723		0.92302
895	1.0000e-06		0.0724		0.92282
900	1.0000e-06		0.0733		0.92204
905	1.0000e-06		0.0703		0.92521
910	1.0000e-06		0.0752		0.92000
915	1.0000e-06		0.0767		0.91880
920	1.0000e-07		0.0767		0.91859
925	1.0000e-07		0.0742		0.92119
930	1.0000e-07		0.0738		0.92189
935	1.0000e-07		0.0766		0.91885
940	1.0000e-07		0.0785		0.91584
945	1.0000e-07		0.0737		0.92166
950	1.0000e-07		0.0729		0.92331
955	1.0000e-07		0.0735		0.92266
960	1.0000e-07		0.0845		0.90955
965	1.0000e-07		0.0763		0.91905
970	1.0000e-07		0.0742		0.92152
975	1.0000e-08		0.0762		0.91908
980	1.0000e-08		0.0773		0.91784
985	1.0000e-08		0.0769		0.91886
990	1.0000e-08		0.0723		0.92330
995	1.0000e-08		0.0787		0.91890
1000	1.0000e-08		0.0693		0.92630
1005	1.0000e-08		0.0802		0.91528
1010	1.0000e-08		0.0790		0.91627
1015	1.0000e-08		0.0752		0.92031
1020	1.0000e-08		0.0733		0.92210
1025	1.0000e-08		0.0698		0.92554
1030	1.0000e-08		0.0777		0.91775
1035	1.0000e-08		0.0745		0.92124
1040	1.0000e-08		0.0705		0.92581
1045	1.0000e-08		0.0819		0.91281
1050	1.0000e-08		0.0755		0.91991
1055	1.0000e-08		0.0733		0.92190
1060	1.0000e-08		0.0799		0.91504
1065	1.0000e-08		0.0751		0.91997
1070	1.0000e-08		0.0716		0.92392
1075	1.0000e-08		0.0781		0.91733
1080	1.0000e-08		0.0734		0.92162
1085	1.0000e-08		0.0720		0.92345
1090	1.0000e-08		0.0707		0.92458
1095	1.0000e-08		0.0760		0.91963
1100	1.0000e-08		0.0748		0.92077
1105	1.0000e-08		0.0787		0.91589
1110	1.0000e-08		0.0715		0.92414
1115	1.0000e-08		0.0762		0.91905
1120	1.0000e-08		0.0759		0.91965
1125	1.0000e-08		0.0726		0.92263
1130	1.0000e-08		0.0732		0.92244
1135	1.0000e-08		0.0811		0.91397
1140	1.0000e-08		0.0789		0.91648
1145	1.0000e-08		0.0752		0.92051
1150	1.0000e-08		0.0757		0.92060
1155	1.0000e-08		0.0730		0.92321
1160	1.0000e-08		0.0751		0.91946
1165	1.0000e-08		0.0784		0.91708
1170	1.0000e-08		0.0738		0.92216
1175	1.0000e-08		0.0802		0.91479
1180	1.0000e-08		0.0762		0.91952
1185	1.0000e-08		0.0750		0.92145
1190	1.0000e-08		0.0786		0.91737
1195	1.0000e-08		0.0760		0.91932
1200	1.0000e-08		0.0699		0.92601
1205	1.0000e-08		0.0763		0.91957
1210	1.0000e-08		0.0746		0.92092
1215	1.0000e-08		0.0715		0.92423
1220	1.0000e-08		0.0757		0.91994
1225	1.0000e-08		0.0709		0.92485
1230	1.0000e-08		0.0726		0.92260
1235	1.0000e-08		0.0715		0.92372
1240	1.0000e-08		0.0808		0.91498
1245	1.0000e-08		0.0782		0.91723
1250	1.0000e-08		0.0727		0.92229
1255	1.0000e-08		0.0716		0.92422
1260	1.0000e-08		0.0726		0.92269
1265	1.0000e-08		0.0751		0.92086
1270	1.0000e-08		0.0715		0.92370
1275	1.0000e-08		0.0700		0.92549
1280	1.0000e-08		0.0756		0.92027
1285	1.0000e-08		0.0784		0.91668
1290	1.0000e-08		0.0767		0.92022
1295	1.0000e-08		0.0776		0.91801
1300	1.0000e-08		0.0751		0.92064
1305	1.0000e-08		0.0702		0.92517
1310	1.0000e-08		0.0782		0.91791
1315	1.0000e-08		0.0803		0.91472
1320	1.0000e-08		0.0811		0.91449
1325	1.0000e-08		0.0741		0.92121
1330	1.0000e-08		0.0796		0.91553
1335	1.0000e-08		0.0715		0.92394
1340	1.0000e-08		0.0733		0.92370
1345	1.0000e-08		0.0758		0.91955
1350	1.0000e-08		0.0741		0.92125
1355	1.0000e-08		0.0727		0.92277
1360	1.0000e-08		0.0727		0.92287
1365	1.0000e-08		0.0726		0.92279
1370	1.0000e-08		0.0787		0.91653
1375	1.0000e-08		0.0758		0.91982
1380	1.0000e-08		0.0774		0.91828
1385	1.0000e-08		0.0730		0.92323
1390	1.0000e-08		0.0781		0.91796
1395	1.0000e-08		0.0755		0.92007
1400	1.0000e-08		0.0768		0.91891
1405	1.0000e-08		0.0711		0.92442
1410	1.0000e-08		0.0771		0.91838
1415	1.0000e-08		0.0813		0.91395
1420	1.0000e-08		0.0752		0.92038
1425	1.0000e-08		0.0740		0.92137
1430	1.0000e-08		0.0749		0.92071
1435	1.0000e-08		0.0766		0.91828
1440	1.0000e-08		0.0728		0.92328
1445	1.0000e-08		0.0723		0.92299
1450	1.0000e-08		0.0772		0.91791
1455	1.0000e-08		0.0776		0.91829
1460	1.0000e-08		0.0755		0.92016
1465	1.0000e-08		0.0736		0.92157
1470	1.0000e-08		0.0739		0.92175
1475	1.0000e-08		0.0754		0.92111
1480	1.0000e-08		0.0787		0.91668
1485	1.0000e-08		0.0753		0.92012
1490	1.0000e-08		0.0767		0.91894
1495	1.0000e-08		0.0773		0.91834
1500	1.0000e-08		0.0785		0.91687
1505	1.0000e-08		0.0753		0.92025
1510	1.0000e-08		0.0763		0.91929
1515	1.0000e-08		0.0712		0.92405
1520	1.0000e-08		0.0785		0.91671
1525	1.0000e-08		0.0737		0.92193
1530	1.0000e-08		0.0763		0.91919
1535	1.0000e-08		0.0776		0.91789
1540	1.0000e-08		0.0696		0.92630
1545	1.0000e-08		0.0784		0.91716
1550	1.0000e-08		0.0762		0.91911
1555	1.0000e-08		0.0738		0.92180
1560	1.0000e-08		0.0723		0.92333
1565	1.0000e-08		0.0751		0.92026
1570	1.0000e-08		0.0728		0.92284
1575	1.0000e-08		0.0779		0.91729
1580	1.0000e-08		0.0686		0.92655
1585	1.0000e-08		0.0730		0.92271
1590	1.0000e-08		0.0753		0.92026
1595	1.0000e-08		0.0744		0.92088
1600	1.0000e-08		0.0787		0.91633
1605	1.0000e-08		0.0789		0.91727
1610	1.0000e-08		0.0793		0.91613
1615	1.0000e-08		0.0737		0.92187
1620	1.0000e-08		0.0736		0.92180
1625	1.0000e-08		0.0753		0.92012
1630	1.0000e-08		0.0751		0.91960
1635	1.0000e-08		0.0749		0.92109
1640	1.0000e-08		0.0758		0.91948
1645	1.0000e-08		0.0787		0.91639
1650	1.0000e-08		0.0800		0.91499
1655	1.0000e-08		0.0726		0.92271
1660	1.0000e-08		0.0726		0.92266
1665	1.0000e-08		0.0764		0.91875
1670	1.0000e-08		0.0730		0.92321
1675	1.0000e-08		0.0731		0.92295
1680	1.0000e-08		0.0754		0.92014
1685	1.0000e-08		0.0796		0.91509
1690	1.0000e-08		0.0727		0.92273
1695	1.0000e-08		0.0754		0.92067
1700	1.0000e-08		0.0742		0.92114
1705	1.0000e-08		0.0812		0.91513
1710	1.0000e-08		0.0774		0.91753
1715	1.0000e-08		0.0753		0.91986
1720	1.0000e-08		0.0751		0.92017
1725	1.0000e-08		0.0741		0.92136
1730	1.0000e-08		0.0750		0.92070
1735	1.0000e-08		0.0758		0.91968
1740	1.0000e-08		0.0626		0.93345
1745	1.0000e-08		0.0751		0.92004
1750	1.0000e-08		0.0754		0.92036
1755	1.0000e-08		0.0765		0.91945
1760	1.0000e-08		0.0706		0.92525
1765	1.0000e-08		0.0751		0.92021
1770	1.0000e-08		0.0731		0.92232
1775	1.0000e-08		0.0758		0.92039
1780	1.0000e-08		0.0824		0.91228
1785	1.0000e-08		0.0709		0.92481
1790	1.0000e-08		0.0691		0.92705
1795	1.0000e-08		0.0717		0.92363
1800	1.0000e-08		0.0740		0.92142
1805	1.0000e-08		0.0764		0.91875
1810	1.0000e-08		0.0763		0.91878
1815	1.0000e-08		0.0766		0.91849
1820	1.0000e-08		0.0782		0.91723
1825	1.0000e-08		0.0724		0.92270
1830	1.0000e-08		0.0727		0.92321
1835	1.0000e-08		0.0793		0.91633
1840	1.0000e-08		0.0757		0.92012
1845	1.0000e-08		0.0743		0.92125
1850	1.0000e-08		0.0766		0.91877
1855	1.0000e-08		0.0736		0.92167
1860	1.0000e-08		0.0741		0.92154
1865	1.0000e-08		0.0781		0.91797
1870	1.0000e-08		0.0738		0.92150
1875	1.0000e-08		0.0741		0.92144
1880	1.0000e-08		0.0710		0.92434
1885	1.0000e-08		0.0709		0.92456
1890	1.0000e-08		0.0749		0.92039
1895	1.0000e-08		0.0737		0.92187
1900	1.0000e-08		0.0701		0.92602
1905	1.0000e-08		0.0756		0.92000
1910	1.0000e-08		0.0708		0.92511
1915	1.0000e-08		0.0754		0.92048
1920	1.0000e-08		0.0784		0.91780
1925	1.0000e-08		0.0720		0.92365
1930	1.0000e-08		0.0780		0.91869
1935	1.0000e-08		0.0705		0.92521
1940	1.0000e-08		0.0793		0.91574
1945	1.0000e-08		0.0729		0.92203
1950	1.0000e-08		0.0765		0.91911
1955	1.0000e-08		0.0722		0.92309
1960	1.0000e-08		0.0739		0.92111
1965	1.0000e-08		0.0760		0.92009
1970	1.0000e-08		0.0729		0.92249
1975	1.0000e-08		0.0814		0.91360
1980	1.0000e-08		0.0736		0.92244
1985	1.0000e-08		0.0807		0.91459
1990	1.0000e-08		0.0725		0.92302
1995	1.0000e-08		0.0779		0.91785
2000	1.0000e-08		0.0729		0.92372
2005	1.0000e-08		0.0805		0.91462
2010	1.0000e-08		0.0786		0.91648
2015	1.0000e-08		0.0725		0.92323
2020	1.0000e-08		0.0746		0.92132
2025	1.0000e-08		0.0753		0.92001
2030	1.0000e-08		0.0735		0.92227
2035	1.0000e-08		0.0761		0.91952
2040	1.0000e-08		0.0778		0.91778
2045	1.0000e-08		0.0689		0.92668
2050	1.0000e-08		0.0772		0.91823
2055	1.0000e-08		0.0752		0.92023
2060	1.0000e-08		0.0728		0.92297
2065	1.0000e-08		0.0774		0.91809
2070	1.0000e-08		0.0730		0.92242
2075	1.0000e-08		0.0803		0.91501
2080	1.0000e-08		0.0764		0.91835
2085	1.0000e-08		0.0771		0.91882
2090	1.0000e-08		0.0743		0.92139
2095	1.0000e-08		0.0772		0.91834
2100	1.0000e-08		0.0723		0.92335
2105	1.0000e-08		0.0778		0.91769
2110	1.0000e-08		0.0762		0.91914
2115	1.0000e-08		0.0729		0.92297
2120	1.0000e-08		0.0725		0.92333
2125	1.0000e-08		0.0715		0.92398
2130	1.0000e-08		0.0706		0.92496
2135	1.0000e-08		0.0728		0.92246
2140	1.0000e-08		0.0757		0.92003
2145	1.0000e-08		0.0729		0.92255
2150	1.0000e-08		0.0728		0.92254
