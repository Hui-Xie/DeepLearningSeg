# Nov 19th, 2020
Add Hausdorf distance compuation on test on AMD and Control data separately.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Compute performance in AMD and Control group separately
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([2.9164, 1.0329, 2.0026])
muSurfaceError = tensor([1.3764, 1.9611, 3.3444])
HauddorffDistance = [[50. 31. 44.]]
stdError = 2.267122268676758
muError = 2.2273151874542236
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.4579, 1.1336, 0.4604])
muSurfaceError = tensor([0.6594, 1.7114, 2.1802])
HauddorffDistance = [[33.  9.  8.]]
stdError = 0.9794862270355225
muError = 1.516985535621643
===============
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


# Nov 17th, 2020
Compute Accuracy separately in AMD and Control of Duke_AMD data:
Analysis:
1  Comparing ReLU constraints with SoftSepartion constraints, control group accuracy improves more with SoftSeparation;
2  Control group improves 0.04, while AMD group improves 0.02;
   So I have some suspicion that more disease data may will get better result.
3  Comparing our result with Leixin's result on the Table 1 of https://arxiv.org/pdf/2007.01217.pdf;
   our surface 3 corresponds with OBM, our surface 2 correpondis with IRPE. Our Control correspond its normal.
   Both our result in both control and AMD groups are better than Leixin's best result "W/".
   Leixin's used neighbor smooth constraints, while our method used soft separation constraints.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Compute performance in AMD and Control group separately in expDuke_20201113A_FixLambda2Unet Network:
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SoftSepar3Unet/expDuke_20201113A_FixLambda2Unet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([2.9144, 1.0877, 1.8948])
muSurfaceError = tensor([1.3283, 1.9940, 3.2859])
stdError = 2.239591121673584
muError = 2.202744960784912
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.4251, 1.0081, 0.4460])
muSurfaceError = tensor([0.6201, 1.6549, 2.1552])
stdError = 0.9295611381530762
muError = 1.4767508506774902
===============
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Compute performance in AMD and Control group separately in pure SurfaceSubnet with ReLU constraint without SoftSeparation:
predictDir= /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/xml
gtDir = /home/hxie1/data/OCT_Duke/numpy_slices/test
===============
GroupName: AMD
case number = 41
stdSurfaceError = tensor([2.9164, 1.0329, 2.0026])
muSurfaceError = tensor([1.3764, 1.9611, 3.3444])
stdError = 2.267122268676758
muError = 2.2273151874542236
===============
GroupName: Control
case number = 18
stdSurfaceError = tensor([0.4579, 1.1336, 0.4604])
muSurfaceError = tensor([0.6594, 1.7114, 2.1802])
stdError = 0.9794862270355225
muError = 1.516985535621643
===============
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

# Nov 17th, 2020
Meeting with professor:
1  we need to use JHU method to test Duke_AMD data;
2  make sure JHU method same with its paper, without using our gradient;
3  JHU's method has a Journel verson;

My think:
1  Leixin's method is single surface optimization, so its lambda only fit its single surface;
2  Our method is multi-surface paralllel optimization, so its final lambda is the balance of multi-surface;
3  we may need a special disease for show our method;
4  soft separation cost function does not consider the lambda for top and bottom surfaces.

# Nov 16th, 2020
Response to professor Wu:
 "I'd suggest you to calculate the segmentation accuracy for the normal and AMD subjects separately. I am still puzzling on the tiny \lambda."
 "We should compare to the model without any separation constraints. Also please compute the segmentation accuracy for the normal and AMD subjects separately.

1  OK.  I will write a script to divide normal and AMD case, and statistics its accuracy separately in test set;
2  "Compare model without any separation constraints" is a wonderful/insightful idea; Thank you,professor. ReLU is a hard constraint.
    OK. I need to retrain a network to test it;
3  I am totally not surprise this tiny lambda result:
   A. Grid searching lambda and network learning lambda both got same tiny lambda result, which should not be an occasional result;
   B. Grid Searching lambda on both Tongren and Duke_AMD data clearly demonstrated that muError is linear correlation with lambda; smaller lambda, smaller muError;
   C. Our theoretical analysis clearly demonstrated that adding lambda in cost function may improve error, may worse error; In total effect, lambda acts 0 effect.
      In other words, lambda may act as a positive or negative noise in different sample, therefore tiny lambda is network's natural choice;
      We also can explain that in proper data set, lambda may act a good effect; In other data set, lambda may act a negative effect;
   D. And we have used backward gradient, Cplex, manual computation 3 methods verifying that our IPM is correct;
   All there results from different dataset, from different methods, and from theory and experiments, all point to a tiny lambda.

# Nov 16th, 2020
Fine Tune Surface and Rift Subnet with fixed Lambda:
Exp config:  /home/sheen/projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/testConfig_Duke/expDuke_20201113A_FixLambda2Unet.yaml
Result:
1   fine Tune training didn't improve accuracy in validation test; (when lambda is small, this is possible)
2   result with fixed lambda on test data:
    config file: /home/sheen/projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/testConfig_Duke/expDuke_20201113A_FixLambda2Unet.yaml
    stdSurfaceError = tensor([2.4533, 1.0671, 1.6763], device='cuda:0')
    muSurfaceError = tensor([1.1122, 1.8906, 2.9410], device='cuda:0')
    stdError = 1.9619948863983154
    muError = 1.9812551736831665
    pixel number of violating surface-separation constraints: 0
3   compare with pure SurfaceSubnet result:
    test expDuke_20200902A_SurfaceSubnet.yaml experiment on test data:
    stdSurfaceError = tensor([2.4572, 1.0611, 1.7664], device='cuda:1')
    muSurfaceError = tensor([1.1576, 1.8849, 2.9892], device='cuda:1')
    stdError = 1.9898078441619873
    muError = 2.0106043815612793
4   use fixed small lambda, mu error improved 0.03 micrometer, while stdError improved 0.028 micrometer;

# Nov 13th, 2020
Professor ask 3 questions on initial grid search Lambda result on Duke_AMD data:
1) What is the method you used to get the error of 2.196 micrometer?
    2.196 is from pure surfaceSubent network with ReLU,without seperation rift as below.
    expDuke_20200902A_SurfaceSubnet.yaml experiment on validation data got:
    meanMuerror = 2.196 micrometer, stdError=2.171, mciepoch=74, learningRate=5e-3,
    surfaceMuError: surface0 = 1.017, surface1=2.45, surface2= 3.141

 2) Compared to your previous experiments, why this dataset takes such a long time?
    1 Duke_AMD data is very big, 12.7 times bigger than Tongren data;
    2 As Tongren data is small, I load all training and validation data in memory at beginning, and use them for each epoch without read disk again;
      As Duke_AMD data is too big, we can not load all training and validation data in memory at beginning,therefore each epoch needs to read each file from disk;
      And SATA harddisk reading speed in our GPU Server is about 1000 times slower than memory access speed.

3) What are the relative magnitudes of the two terms in the objective function?
   In my Aug 29 email report, I computed 1/(2sigma^2) for Tongren data like below. In other words, 1/(2sigma^2) is at 0.1 magnitude .
    1/(2sigma^2) in different OCT data at about 0.1 magnitude should not change too much, as it is surface error variance against same Guassian GT with same sigma =20 in training.
    While our grid search lambda is 2.9e-7, very small.
    Its relative magnitude of 2 terms in the object function is 300K times.
    Duke_AMD experiment is same with Tongren experiment, grid searched lambda is very small, which is consistant with leanring lambda result.
    This result also is consist with our theoretical analysis: Lambda can not improve mu (maybe better, maybe worse), so a smaller lambda is the choice of the grid search, and also the choice of learning lambda.


Attached 0829 email report:
# compare 1/(2*sigma2) and lambda, which represent the magnitude of unary terms and pairwise terms in the cost function of IPM:
reciprocalTwoSigma2.shape = torch.Size([5, 9, 512])
mean of reciprocalTwoSigma2 = [1.7199, 0.8725, 0.0993, 0.2319, 0.9011, 0.5338, 0.0335, 0.2529, 1.7592]
min of reciprocalTwoSigma2 = [0.0334, 0.0288, 0.0193, 0.0106, 0.0186, 0.0059, 0.0060, 0.0185, 0.0345]
max of reciprocalTwoSigma2 = [2.7259, 1.6216, 0.1681, 0.4646, 1.3337, 1.1049, 0.0949, 0.3692, 3.4337]



# Nov 13th, Friday, 2020
1  test expDuke_20200902A_SurfaceSubnet.yaml experiment on test data:
    stdSurfaceError = tensor([2.4572, 1.0611, 1.7664], device='cuda:1')
    muSurfaceError = tensor([1.1576, 1.8849, 2.9892], device='cuda:1')
    stdError = 1.9898078441619873
    muError = 2.0106043815612793

2  output visual prediction result;  Done.
   all visul images at : /home/hxie1/data/OCT_Duke/numpy_slices/log/SurfaceSubnet/expDuke_20200902A_SurfaceSubnet/testResult/images

3  using predicted R, and expDuke_20200902A_SurfaceSubnet.yaml to search a better lambda, from 1e-5 downward, on validation data:
   config file: /home/sheen/projects/DeepLearningSeg/OCTMultiSurfaces/SoftSepar3Unet/testConfig_Duke/expDuke_20201109A_SearchLambda2Unet.yaml
   lambda search initial value: 1.0e-5, step: 1.0e-8; It search 1000 points.
   Result: at the search iteration 971, it get its minimum error
   nSearch,meanError,      meanStd,            lambda_0,               lambda_1,               surfErr_0,      surfErr_1,              surfErr_2,      surfStd_0,          surfStd_1,          surfStd_2,
   971,2.166304588317871,2.102299451828003,2.900840172515018e-07,2.900840172515018e-07, 0.9951231479644775,2.3246231079101562,3.1791670322418213, 1.3870776891708374,2.5179460048675537,1.637975811958313,

   the relation curve of meanError and Lambda: /home/hxie1/data/OCT_Duke/numpy_slices/log/SearchLambda2Unet/expDuke_20201109A_SearchLambda2Unet/testResult/searchLambda_replaceRwithGT_0_gridSearch_1e-5downward.png
   Comparing with case without lambda on validation data:
              meanError improve from 2.196 micrometer to 2.1663 micrometer;
              meanStd   improve from 2.171 micorometer to 2.102 micrometer;
   And lambda_0 = 2.900840172515018e-07, and lambda_1= 2.900840172515018e-07, very small.

4  using fixed lambda to fine tune Rift+SurfaceNet network:
   It has launched training, but very slow. 40 min per epoch.




5  use fixed lambda + fineTune network to get result on test data;



# Nov 10th, Tuesday.
Professor Wu point out: The duke data is from JHU's paper, it is not AMD data.  I need to find this data, may redo experiment.

# Nov 9th, Monday, 2020
Analyze previous experiment data:
1  expDuke_20200902A_SurfaceSubnet.yaml experiment on validation data got:
   meanMuerror = 2.196 micrometer, stdError=2.171, mciepoch=74, learningRate=5e-3,
   surfaceMuError: surface0 = 1.017, surface1=2.45, surface2= 3.141

2  replace R with GT without smoothness, using expDuke_20200902A_SurfaceSubnet search Lambda on validaton data:
   nSearch	meanError	    meanStd	            lambda_0	        lambda_1	        surfErr_0	        surfErr_1	        surfErr_2	        surfStd_0	        surfStd_1	        surfStd_2
   99	2.16719126701355	2.09991383552551	9.9994576885365E-06	9.9994576885365E-06	0.996140241622925	2.31729507446289	3.18813920021057	1.39484310150146	2.49415946006775	1.6541827917099





# Sep 14th, Tuesday, 2020
Professor directed to use CPlex for lambda search to verify IPM again at convenient time.


# Sep 10th, Thursday, 2020
# Duke data grid search result:
Analysis:
1  Please refer to csv data and its relation between lambda and meanError;
2  Grid search uses un-smooth ground truth as r;
3  Duke data is 12.7 times bigger than Tongren data, and grid search is very time-consuming;
4  I searched 2 regions, each region needs about 16 hours with 3 GPUs;
   Region A: from 0.1 to 0.001, with grid step 0.001;
   Region B: from 0.001 to 1e-5, with grid step 1e-5;
5  Both searches show that it is basically a linear relation between meanError and lambda;
   Smaller lambda, better accuracy of IPM optimized surface location.
6  Recall the stationary condition formula like below of IPM cost function,
   it shows that the sum effect of mu and r uses lambda as weight to correct mu,
   mu and r occupy 50% weight respectively before multiplying lambda, and mu generally is far bigger than r.
   Therefore even perfect r from ground truth does not add benefit to correct mu,
   as this correcting process is dominated by mu/s self.


# Sep 4th, Friday, 2020
# on validation set with pixel resolution of 3.24 micrometer, at 11:30pm of Friday.
expDuke_20200902A_SurfaceSubnet: meanMuerror = 2.196 micrometer, epoch=74, learningRate=5e-3,
                                 surfaceMuError: surface0 = 1.017, surface1=2.45, surface2= 3.141
                                 Now valiation loss is increasing.

expDuke_20200902A_RiftSubnet:    meanSepartionerror = 3.881 micrometer, epoch=101, learningRate=1.25e-3,
                                 riftError: rift0 = 3.089, rift1=4.673
                                 Now validation loss is still decreasing.


# Sep 2nd, 2020
1 Duke data is training:
  A   data set statistics:
  training set: 266 volumes x 51 slices per volume, where 187 AMD + 79 control = 266 volumes;
  validation set: 59 volumes x 51 slice per volume, where 41 AMD + 18 control  =  59 volumes;
  test set:  59 volumes x 51 slice per volume, where 41 AMD + 18 control  =  59 volumes;
  B   Now SurfaceSubset and SeparationSubset are in training separately, very slow, about 33 min/epoch;
      Duke data is about 12.7 times of Tongren data in the slices number: (266+59)*51/(42*31) = 12.7;
  C   After 32 epochs, surfaceSubset got mean surface error 2.6 micrometer in resolution of 3.24 micrometer per pixel;
      After 28 epochs, separationSubst got mean separation error 4.3 micrometer;
      both networks are still in training;
  D   At the moment, surface 0 get 1.4 micrometer, surface 1 gets 2.8 micrometer, surface 2 gets 3.7 micrometer errors;
      It shows the surface0 is very easy, while surface2 is harder to segment, which is led by the difference of different surface information;

2 Grid search-lambda script is ready, waiting the Duke network finishing pretraining;
  An intuitive analysis from below A and B may predict that our further grid search may still get a very small lambda.
  A  learning lambda also gets very small lambda  of 1e-11 level;
  B  Binary search: when lambda =0.01, muError =13; when lambda=0.0001, muError = 2.07 for Tongren data;
  As search-lambda network needs 3 GPUs in which SurfaceSubset and SeparationSubset run 2 GPUs, current GPU resource is busy on Duke.
  We need to wait for Duke to finish training, and then launch grid search lambda.

Plan:
1 As Duke data training is very slow, I plan to launch ovarian cancer project tomorrow;
2 When Duke data training finish, I will come back to continue Duke data work;

