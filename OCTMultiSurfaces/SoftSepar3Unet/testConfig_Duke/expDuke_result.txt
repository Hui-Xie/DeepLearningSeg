# Nov 9th, Monday, 2020
Analyze previous experiment data:
1  expDuke_20200902A_SurfaceSubnet.yaml experiment on validation data got:
   meanMuerror = 2.196 micrometer, stdError=2.171, mciepoch=74, learningRate=5e-3,
   surfaceMuError: surface0 = 1.017, surface1=2.45, surface2= 3.141

2  replace R with GT without smoothness, using expDuke_20200902A_SurfaceSubnet search Lambda on validaton data:
   nSearch	meanError	    meanStd	            lambda_0	        lambda_1	        surfErr_0	        surfErr_1	        surfErr_2	        surfStd_0	        surfStd_1	        surfStd_2
   99	2.16719126701355	2.09991383552551	9.9994576885365E-06	9.9994576885365E-06	0.996140241622925	2.31729507446289	3.18813920021057	1.39484310150146	2.49415946006775	1.6541827917099

Further work:
1  test expDuke_20200902A_SurfaceSubnet.yaml experiment on test data:
    stdSurfaceError = tensor([2.4572, 1.0611, 1.7664], device='cuda:1')
    muSurfaceError = tensor([1.1576, 1.8849, 2.9892], device='cuda:1')
    stdError = 1.9898078441619873
    muError = 2.0106043815612793

2  output visual prediction result;
    
3  using predicted R, and expDuke_20200902A_SurfaceSubnet.yaml to search a better lambda, from 1e-5 downward;

4  using fixed lambda to run Rift+SurfaceNet to get result on test data.




# Sep 14th, Tuesday, 2020
Professor directed to use CPlex for lambda search to verify IPM again at convenient time.


# Sep 10th, Thursday, 2020
# Duke data grid search result:
Analysis:
1  Please refer to csv data and its relation between lambda and meanError;
2  Grid search uses un-smooth ground truth as r;
3  Duke data is 12.7 times bigger than Tongren data, and grid search is very time-consuming;
4  I searched 2 regions, each region needs about 16 hours with 3 GPUs;
   Region A: from 0.1 to 0.001, with grid step 0.001;
   Region B: from 0.001 to 1e-5, with grid step 1e-5;
5  Both searches show that it is basically a linear relation between meanError and lambda;
   Smaller lambda, better accuracy of IPM optimized surface location.
6  Recall the stationary condition formula like below of IPM cost function,
   it shows that the sum effect of mu and r uses lambda as weight to correct mu,
   mu and r occupy 50% weight respectively before multiplying lambda, and mu generally is far bigger than r.
   Therefore even perfect r from ground truth does not add benefit to correct mu,
   as this correcting process is dominated by mu/s self.


# Sep 4th, Friday, 2020
# on validation set with pixel resolution of 3.24 micrometer, at 11:30pm of Friday.
expDuke_20200902A_SurfaceSubnet: meanMuerror = 2.196 micrometer, epoch=74, learningRate=5e-3,
                                 surfaceMuError: surface0 = 1.017, surface1=2.45, surface2= 3.141
                                 Now valiation loss is increasing.

expDuke_20200902A_RiftSubnet:    meanSepartionerror = 3.881 micrometer, epoch=101, learningRate=1.25e-3,
                                 riftError: rift0 = 3.089, rift1=4.673
                                 Now validation loss is still decreasing.


# Sep 2nd, 2020
1 Duke data is training:
  A   data set statistics:
  training set: 266 volumes x 51 slices per volume, where 187 AMD + 79 control = 266 volumes;
  validation set: 59 volumes x 51 slice per volume, where 41 AMD + 18 control  =  59 volumes;
  test set:  59 volumes x 51 slice per volume, where 41 AMD + 18 control  =  59 volumes;
  B   Now SurfaceSubset and SeparationSubset are in training separately, very slow, about 33 min/epoch;
      Duke data is about 12.7 times of Tongren data in the slices number: (266+59)*51/(42*31) = 12.7;
  C   After 32 epochs, surfaceSubset got mean surface error 2.6 micrometer in resolution of 3.24 micrometer per pixel;
      After 28 epochs, separationSubst got mean separation error 4.3 micrometer;
      both networks are still in training;
  D   At the moment, surface 0 get 1.4 micrometer, surface 1 gets 2.8 micrometer, surface 2 gets 3.7 micrometer errors;
      It shows the surface0 is very easy, while surface2 is harder to segment, which is led by the difference of different surface information;

2 Grid search-lambda script is ready, waiting the Duke network finishing pretraining;
  An intuitive analysis from below A and B may predict that our further grid search may still get a very small lambda.
  A  learning lambda also gets very small lambda  of 1e-11 level;
  B  Binary search: when lambda =0.01, muError =13; when lambda=0.0001, muError = 2.07 for Tongren data;
  As search-lambda network needs 3 GPUs in which SurfaceSubset and SeparationSubset run 2 GPUs, current GPU resource is busy on Duke.
  We need to wait for Duke to finish training, and then launch grid search lambda.

Plan:
1 As Duke data training is very slow, I plan to launch ovarian cancer project tomorrow;
2 When Duke data training finish, I will come back to continue Duke data work;

