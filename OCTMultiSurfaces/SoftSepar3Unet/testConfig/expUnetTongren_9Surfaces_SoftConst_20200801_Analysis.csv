# Experiment on 20200831
# Config: use binary search for one lambda scalar for all surface

# Lambda value after 40 iteration of binary search:
# Here, the meanError is validation data for Binary search.
learned R:      meanError=2.0169577598571777, meanStd=0.6771263480186462, lambda=1.6371e-08;
RUseNoSmoothGT: meanError=2.016946792602539, meanStd=0.6770985126495361, lambda=4.9113e-08;
RUseSmoothGT:   meanError=2.0169360637664795, meanStd=0.677091658115387, lambda=9.8225e-09;

# Now redo model_3 and model_4 with new one lambda.
Model Description:
Model_1: unary without hard separation: SoftConst_20200819A_SurfaceSubnet
Model_2: unary + hard separation:  20200829A_SurfaceSubnet_HardSeparation
Model_3: unary + pairwise + fixed lambda +learned R:  20200831A_FixLambda2Unet_CV0 (one lambda)
Model_4: unary + pairwise + fixed lambda + ground truth R: 20200831A_FixLambda2Unet_UseGTReplaceR_CV0 (one lambda)
Model_5: unary + pairwise + learned lambda + learned R:  20200821A_3Unet_CV0

# Surface Location Error:(micrometer) on test data
Model,      Experiment_Name,                            fold,   Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
Model_1,    SoftConst_20200819A_SurfaceSubnet,          CV0,    1.0055, 3.1687, 2.8236, 2.5908, 3.1875, 1.3984, 2.9978, 1.5677, 1.2584, 1.0588494539260864, 2.2220582962036133,
Model_2,    20200829A_SurfaceSubnet_HardSeparation,     CV0,    1.1042, 3.2179, 2.8373, 2.5845, 3.1151, 1.4258, 2.6963, 1.7313, 1.3402, 0.9733524322509766, 2.2280783653259277,
Model_3,    20200831A_FixLambda2Unet_CV0,               CV0,    1.1042, 3.2179, 2.8373, 2.5845, 3.1150, 1.4258, 2.6971, 1.7314, 1.3402, 0.9733793139457703, 2.2281432151794434,
Model_4,    20200831A_FixLambda2Unet_UseGTReplaceR_CV0, CV0,    1.1041, 3.2179, 2.8373, 2.5846, 3.1151, 1.4258, 2.6969, 1.7313, 1.3402, 0.9734154939651489, 2.228121519088745,
Model_5,    20200821A_3Unet_CV0,                        CV0,    1.0622, 3.1820, 2.8789, 2.5810, 3.1813, 1.3911, 2.9961, 1.6142, 1.2184, 1.0553975105285645, 2.233919143676758,

# Hausdorff Distance Error: (pixel) on test data
Model,      Experiment_Name,                            fold,    0,  1,  2,  3,  4,  5,  6,  7,  8,  Max,
Model_1,    SoftConst_20200819A_SurfaceSubnet,          CV0,     4, 12, 11,  8, 10,  9,  7,  4,  5,  12,
Model_2,    20200829A_SurfaceSubnet_HardSeparation,     CV0,     6, 15, 10,  7,  9,  9,  7,  8,  9,  15
Model_3,    20200831A_FixLambda2Unet_CV0,               CV0,     6, 15, 10,  7,  9,  9,  7,  8,  9,  15
Model_4,    20200831A_FixLambda2Unet_UseGTReplaceR_CV0, CV0,     6, 15, 10,  7,  9,  9,  7,  8,  9,  15
Model_5,    20200821A_3Unet_CV0,                        CV0,     7, 13, 10,  8, 10,  9,  8,  8, 12,  13,

Analysis:
1   Using binary search for one lambda scalar for all surfaces also gets a very small lambda at 1e-8 level;
    It shows network optimization expects the separation branch has very small weight;
2   I checked the binary search process records:
    if lambda=0.012 which is a similar level with 1/(2*sigma2), the validation error of surface location is 13.0 micrometer, and mean std of error is 7.0 micrometer.
3   Comparing A). one lambda for all surfaces,  and B). 8 lambda each for different surface:
     A and B get same HD, and A gets a little(0.0003 micormeter) worse than B in surface location error;
4   As lambda is so small ignorable value,  that fine tune network with fixed lambda equals further training surface branch, which will not bring improvement, instead of overfitting;


# Experiments on all models: 20200829
# Config: 5 models all on test data CVO.

Model Description:
Model_1: unary without hard separation: SoftConst_20200819A_SurfaceSubnet
Model_2: unary + hard separation:  20200829A_SurfaceSubnet_HardSeparation
Model_3: unary + pairwise + fixed lambda +learned R:  20200829A_FixLambda2Unet_CV0 (8 lambdas)
Model_4: unary + pairwise + fixed lambda + ground truth R: 20200829A_FixLambda2Unet_UseGTReplaceR (8 lamddas)
Model_5: unary + pairwise + learned lambda + learned R:  20200821A_3Unet_CV0

# Surface Location Error:(micrometer) on test data
Model,      Experiment_Name,                        fold,   Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
Model_1,    SoftConst_20200819A_SurfaceSubnet,      CV0,    1.0055, 3.1687, 2.8236, 2.5908, 3.1875, 1.3984, 2.9978, 1.5677, 1.2584, 1.0588494539260864, 2.2220582962036133,
Model_2,    20200829A_SurfaceSubnet_HardSeparation, CV0,    1.1042, 3.2179, 2.8373, 2.5845, 3.1151, 1.4258, 2.6963, 1.7313, 1.3402, 0.9733524322509766, 2.2280783653259277,
Model_3,    20200829A_FixLambda2Unet_CV0,           CV0,    1.1040, 3.2171, 2.8370, 2.5846, 3.1154, 1.4259, 2.6954, 1.7313, 1.3402, 0.9732388257980347, 2.227872371673584,
Model_4,    20200829A_FixLambda2Unet_UseGTReplaceR, CV0,    1.1040, 3.2171, 2.8372, 2.5844, 3.1154, 1.4259, 2.6951, 1.7313, 1.3402, 0.9732391834259033, 2.2278451919555664
Model_5,    20200821A_3Unet_CV0,                    CV0,    1.0622, 3.1820, 2.8789, 2.5810, 3.1813, 1.3911, 2.9961, 1.6142, 1.2184, 1.0553975105285645, 2.233919143676758,

# Hausdorff Distance Error: (pixel)  on test data
Model,      Experiment_Name,                        fold,    0,  1,  2,  3,  4,  5,  6,  7,  8,  Max,
Model_1,    SoftConst_20200819A_SurfaceSubnet,      CV0,     4, 12, 11,  8, 10,  9,  7,  4,  5,  12,
Model_2,    20200829A_SurfaceSubnet_HardSeparation, CV0,     6, 15, 10,  7,  9,  9,  7,  8,  9,  15   
Model_3,    20200829A_FixLambda2Unet_CV0,           CV0,     6, 15, 10,  7,  9,  9,  7,  8,  9,  15,
Model_4,    20200829A_FixLambda2Unet_UseGTReplaceR, CV0,     6, 15, 10,  7,  9,  9,  7,  8,  9,  15,   
Model_5,    20200821A_3Unet_CV0,                    CV0,     7, 13, 10,  8, 10,  9,  8,  8, 12,  13,

Analysis:
1   In order to save long IPM training time(2-3 days), model_2 just uses hardIPM to replace model_1's ReLU, and apply it on the test CV0, wthout retraining model;
2   The surface location errors of 5 models are similar;
3   All these result are in same level with 20200317 MICCAI model experiment, and subtle difference is from random conv initialization;
4   As fixed Lambda is very small, replacing predicted R with ground truth R does not bring improvement;
5   Binary search choosing Lambda and learning Lambda both achieve very small level value, showing that optimzation network expects R has minimum effect on its final loss;


# experiment: expTongren_9Surfaces_SoftConst_20200829A_FixLambda2Unet_CV0 on Test CV0
# config:
  A fix 8 scalars lambda values searched from validation data of CV0;
  B Use 2 Unet: SurfaceSubnet, RiftSubset;
  C uses predicted result from RiftSubnet;
  D test on test CV0 data, an untouched data;

Experiment_Name, Error(micrometer),     Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean, Notes,
20200829A_FixLambda2Unet_CV0,       CV0, 1.1040, 3.2171, 2.8370, 2.5846, 3.1154, 1.4259, 2.6954, 1.7313, 1.3402, 0.9732388257980347, 2.227872371673584,  (latest), fixed Lambda,with predicte R
20200821A_3Unet_CV0,                CV0, 1.0622, 3.1820, 2.8789, 2.5810, 3.1813, 1.3911, 2.9961, 1.6142, 1.2184, 1.0553975105285645, 2.233919143676758,  (8 days ago: learning lambda[0,1] on validaiton data and learning R
SoftConst_20200819A_SurfaceSubnet,  CV0, 1.0055, 3.1687, 2.8236, 2.5908, 3.1875, 1.3984, 2.9978, 1.5677, 1.2584, 1.0588494539260864, 2.2220582962036133, 1Unet, only unary item

# Measure Hausdorff distance of all 9 surface for different network results: (Unit:pixel)
Surface Index,                      0,  1,  2,  3,  4,  5,  6,  7,  8,  Max,   Notes
20200829A_FixLambda2Unet_CV0,       6, 15, 10,  7,  9,  9,  7,  8,  9,  15,    On test data, with predicted R, fixed Lambda;
20200821A_3Unet_CV0,                7, 13, 10,  8, 10,  9,  8,  8, 12,  13,    On test data, with predicted R, learning lambda;
SoftConst_20200819A_SurfaceSubnet,  4, 12, 11,  8, 10,  9,  7,  4,  5,  12,    On test data, with only 1 Unet, without Lambda, without R;

# compare 1/(2*sigma2) and lambda, which represent the magnitude of unary terms and pairwise terms in the cost function of IPM:
reciprocalTwoSigma2.shape = torch.Size([5, 9, 512])
mean of reciprocalTwoSigma2 = [1.7199, 0.8725, 0.0993, 0.2319, 0.9011, 0.5338, 0.0335, 0.2529, 1.7592]
min of reciprocalTwoSigma2  = [0.0334, 0.0288, 0.0193, 0.0106, 0.0186, 0.0059, 0.0060, 0.0185, 0.0345]
max of reciprocalTwoSigma2  = [2.7259, 1.6216, 0.1681, 0.4646, 1.3337, 1.1049, 0.0949, 0.3692, 3.4337]

Lambda_detach.shape = torch.Size([5, 8, 512]), for a fixed lambda value.
mean of Lambda_detach = [7.9563e-07, 9.8225e-09, 4.5475e-08, 1.0914e-07, 3.0013e-07, 2.4556e-07, 1.8008e-07, 8.8403e-08]
min of Lambda_detach  = [7.9563e-07, 9.8225e-09, 4.5475e-08, 1.0914e-07, 3.0013e-07, 2.4556e-07,1.8008e-07, 8.8403e-08]
max of Lambda_detach  = [7.9563e-07, 9.8225e-09, 4.5475e-08, 1.0914e-07, 3.0013e-07, 2.4556e-07,1.8008e-07, 8.8403e-08]


Analysis:
1  Yesterday(20200828) result is on validation data just for searching Lambda on validation data;
2  Today(20200829) result is on untouched test data CV), which represents a real generalization result;
3  A fixed Lambda searched from binary search on validation data, or a various Lambda learning from a Unet, both get very small lambda value, and both does not improve test result in segmentation and HD,
   comparing with our pure SurfaceSubnet;
4  Comparing coefficients of unary terms and pairwise terms, unary terms has 0.7116 mean magnitude, while pairwise terms has 2.2178e-07 mean magnitude.
   Their magnitude rate is 3208434 times, which shows pairwise term has very very tiny effect on whole optimization of IPM.
5  Initial theoretical analysis can support above experiment result;


# experiment 20200828: Brute Force Search Best Lambda on validation CV0
# expereiment Name: expTongren_9Surfaces_SoftConst_20200828A_SearchLambda2Unet_CV0
# config:
  A use binary search to search best lambda to make muError smallest;
  B lambda is 8 scalars for 8 separations in 9 surfaces context;
  C data: validation data, 5 patients, total 155 slices;
  D Lambda intial value set as 200 for all;

Experiment_Name, Error(micrometer),  Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
Use_Predicted_R,                CV0, 1.0938, 2.7808, 2.5530, 2.2972, 2.5502, 1.3062, 2.5774, 1.7242, 1.2709, 0.6771336793899536, 2.0170607566833496,
Use_UnsmoothedGTasR,            CV0, 1.0939, 2.7808, 2.5546, 2.2967, 2.5508, 1.3088, 2.5777, 1.7243, 1.2708, 0.6769289374351501, 2.0176026821136475,
Use_SmoothedGTasR,              CV0, 1.0939, 2.7808, 2.5546, 2.2995, 2.5507, 1.3088, 2.5781, 1.7242, 1.2708, 0.6771811842918396, 2.017927646636963

# Measure Hausdorff distance of all 9 surface for different network results on this validation data: Unit.Pixel
 Surface Index,          0, 1, 2, 3, 4, 5, 6, 7, 8,  Max,
 Use_Predicted_R,        5. 8. 7. 6. 8. 7. 6. 3. 3.  8
 Use_UnsmoothedGTasR,    5. 8. 7. 6. 8. 7. 6. 3. 3.  8
 Use_SmoothedGTasR,      5. 8. 7. 6. 8. 7. 6. 3. 3.  8

It corresponded Lambda after binary search iterations:
                                lambda0,   lambda1,    lambda2,    lambda3,    lambda4,    lambda5,    lambda6,    lambda7,
Use_Predicted_R,               7.9563e-07, 9.8225e-09, 4.5475e-08, 1.0914e-07, 3.0013e-07, 2.4556e-07, 1.8008e-07, 8.8403e-08,
Use_UnsmoothedGTasR,           9.4296e-07, 1.0477e-07, 2.9104e-07, 2.9104e-07, 6.1118e-06, 5.2387e-07, 3.8417e-07, 3.1432e-07,
Use_SmoothedGTasR,             9.4296e-07, 1.0477e-07, 2.9104e-07, 8.7311e-07, 6.1118e-06, 5.2387e-07, 3.8417e-07, 3.1432e-07

Analysis:
1  After 35 binary search iterations, 3 networks all achieved about 2.017 micrometer mean surface error with subtle difference, and all lambda are at 1e-7 level value, very very small;
2  This experiment shows that even perfect predicted R has very small weight to help improve final surface segmentation accuracy;
   When mu is Not at high accuracy, a good R may help; In our case, the mu is really good.
   Some updated theoretical analysis is below:


3  Brute force search method and yesterday learning method  both get similar lambda result, as their optimization essences are same which is to reduce muError;
   Brute force used 35 iteration. If we use more iterations, lambda will be smaller;
4  Detailed binary search record and its each surface error and standard deviation are in attachments. You may use Excel to read them.
   *_0.csv: Use_Predicted_R,
   *_1.csf: Use_UnsmoothedGTasR,
   *_2.csv: Use_SmoothedGTasR


# experiment expTongren_9Surfaces_SoftConst_20200827A_3Unet_CV0
# config:
  1  add lambda into (0, 200) range from previous (0,1)
  2  lambda training from training data.
experiment_Name, Error(micrometer),        Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
20200821A_3Unet_CV0,                  CV0, 1.0622, 3.1820, 2.8789, 2.5810, 3.1813, 1.3911, 2.9961, 1.6142, 1.2184, 1.0553975105285645, 2.233919143676758,  lambda \in (0,1), learning R;
20200827A_3Unet_CV0,                  CV0, 0.9885, 3.2768, 2.9824, 2.6563, 3.3461, 1.4610, 3.0790, 1.6235, 1.2132, 1.1312294006347656, 2.291862964630127,  lambda \in (0,200), learning R;
20200827A_3Unet_CV0_RUseNoSmoothedGT, CV0, 0.9885, 3.2768, 2.9825, 2.6563, 3.3462, 1.4610, 3.0789, 1.6235, 1.2132, 1.1312602758407593, 2.291879415512085, lambda \in (0,200), NoSmoothedGTasR;
20200827A_3Unet_CV0_RUsesmoothedGT,   CV0, 0.9885, 3.2766, 2.9825, 2.6563, 3.3461, 1.4610, 3.0791, 1.6235, 1.2132, 1.1312209367752075, 2.2918736934661865, lambda \in (0,200), SmoothedGTasR;

Experiment: expTongren_9Surfaces_SoftConst_20200819A_LambdaSubnet_CV0  : train from validatioin data, and lambda limits in (0,1)
Lambda.shape = torch.Size([155, 8, 512])
mean of Lambda = tensor([0.0025, 0.0026, 0.0022, 0.0021, 0.0021, 0.0021, 0.0020, 0.0020], device='cuda:2')
min of Lambda  =
torch.return_types.min(values=tensor([4.1372e-14, 2.4867e-06, 2.2267e-07, 6.2960e-07, 9.1657e-07, 3.3532e-06,  4.9061e-07, 5.0531e-07], device='cuda:2'),
indices=tensor([0, 0, 0, 0, 0, 0, 0, 0], device='cuda:2'))
max of Lambda  =
torch.return_types.max(values=tensor([0.0122, 0.0038, 0.0028, 0.0024, 0.0042, 0.0027, 0.0020, 0.0025], device='cuda:2'),
indices=tensor([452, 443, 432, 163,  57, 120, 110, 163], device='cuda:2'))


Experiment: expTongren_9Surfaces_SoftConst_20200826A_LambdaSubnet_CV0 : train from training data, and lambda limit in (0,200)
Lambda.shape = torch.Size([155, 8, 512])
mean of Lambda = tensor([1.0461e-12, 3.2994e-11, 2.9176e-12, 9.1121e-12, 8.9318e-13, 4.6553e-15, 3.4685e-14, 1.1694e-14], device='cuda:2')
min of Lambda  =
torch.return_types.min(values=tensor([4.1818e-30, 5.0134e-27, 2.5970e-29, 2.6495e-28, 1.3683e-30, 4.6286e-37, 4.0834e-34, 1.2803e-35], device='cuda:2'),
indices=tensor([  4, 505, 506,   4, 505, 506, 505, 505], device='cuda:2'))
max of Lambda  =
torch.return_types.max(values=tensor([1.0036e-08, 2.3132e-07, 2.5180e-08, 7.8057e-08, 8.1684e-09, 5.9064e-11, 4.1676e-10, 1.5237e-10], device='cuda:2'),
indices=tensor([511, 511, 511, 511, 511, 511, 511, 511], device='cuda:2'))


Analysis:
1   Learning lambda from training data make lambda at 1e-13 level, which leads R almost ineffectively.
    As our learning lambda is basing on training data while fixed surface and separaition branches,
    Network thinks the surface branch has 0.99+ weight to match with ground truth, so it let separation branch weight equal to zero;
    This verified that we can not use training data to learning lambda;
2   learning lambda from validation data still make lambda at 2e-3 level; Nework still give surface branch 0.99+ weight;
3   Further plan:
    A  change Lambda into [0, 50] range;
    B  learn Lambda from validation data with R directly adapted from ground truth;
       risk: our learning R may can not achieve this level;


# sigma2 analysis along different surfaces on August 26th, 2020
python3.7 ./SurfaceSubnet_Test.py ./testConfig/expTongren_9Surfaces_SoftConst_20200819A_SurfaceSubnet_CV0.yaml

sigma2.shape = torch.Size([155, 9, 512])
mean of sigma2 = tensor([0.1589, 0.2499, 2.3791, 2.1644, 0.4526, 0.3018, 5.4519, 1.6994, 0.1721],  device='cuda:1')
1.0/mean = [6.2933, 4.0016, 0.4203, 0.4620, 2.2095, 3.3135, 0.1834, 0.5884, 5.8106]

min of sigma2  = torch.return_types.min(
values=tensor([0.0129, 0.0618, 0.7259, 0.7362, 0.0833, 0.1022, 0.7014, 0.6878, 0.0182], device='cuda:1'),
indices=tensor([187, 193, 131,  85,  58, 223,  44, 108,  15], device='cuda:1'))
1.0/min = [77.5194, 16.1812,  1.3776,  1.3583, 12.0048,  9.7847,  1.4257,  1.4539, 54.9451]

max of sigma2  = torch.return_types.max(
values=tensor([ 42.1497, 177.9110,  37.7500,  35.5107,  33.8680,  63.1024,  71.3775, 47.5299,  17.0574], device='cuda:1'),
indices=tensor([  0,   0,   0,   0,   0, 511, 481, 205, 208], device='cuda:1'))
1/max = [0.0237, 0.0056, 0.0265, 0.0282, 0.0295, 0.0158, 0.0140, 0.0210, 0.0586]

Analysis:
1   1/sigma2 is range [0, 200]

===========================================================


# Measure Hausdorff distance of all 9 surface for different network results: On August 26th, 2020, on test data.
 Surface Index,                         0,  1,  2,  3,  4,  5,  6,  7,  8,  Max,
 3Unet_Network_with_learning_R,         7, 13, 10,  8, 10,  9,  8,  8, 12,  13,
 3Unet_Use_SmoothedGT_to_replace_R,     7, 13, 10,  8, 10,  9,  8,  7, 12,  13,
 3Unet_Use_UnsmoothedGT_to_replace_R,   7, 13, 10,  8, 10,  9,  8,  8, 12,  13,
 1Unet_Only_unary_term,                 4, 12, 11,  8, 10,  9,  7,  4,  5,  12,

 Analysis:
 1  This Hausdorff distance measures the max absolute segmentation deviation between ground truth and prediction for all 5 test volumes in CV0;
 2  Hausdorff distance is in pixel unit above;
 3  Introducing R increases Hausdorff distance overall;
 4  Introducing R increases the Hausdorff distance of surface 0 and surface 8 which are the most easy segmented surfaces,
    which means R acts as a disturbance to the original unary prediction mu in worse cases;
 5  It will be really helpful to further analyze reason behind this from the cost function form;


# experiment: use ground truth of R to replace predicted R:
Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
20200821A_3Unet_CV0,                  CV0, 1.0622, 3.1820, 2.8789, 2.5810, 3.1813, 1.3911, 2.9961, 1.6142, 1.2184, 1.0553975105285645, 2.233919143676758,  3Unet Network, with learning R;
20200821A_3Unet_CV0_RUseSmoothedGT,   CV0, 1.0627, 3.1763, 2.8555, 2.5546, 3.1697, 1.3900, 2.9056, 1.5939, 1.2182, 1.0365197658538818, 2.2140555381774902, 3Unet, Use SmoothedGT to replace R;
20200821A_3Unet_CV0_RUseNoSmoothedGT, CV0, 1.0623, 3.1761, 2.8482, 2.5502, 3.1691, 1.3877, 2.8932, 1.5866, 1.2175, 1.0357218980789185, 2.210094928741455,  3Unet, use UnsmoothedGT to replace R;
SoftConst_20200819A_SurfaceSubnet,    CV0, 1.0055, 3.1687, 2.8236, 2.5908, 3.1875, 1.3984, 2.9978, 1.5677, 1.2584, 1.0588494539260864, 2.2220582962036133, 1Unet, only unary item

Analysis:
1  The above 4 results have no essential difference as network computes pixel locations;
   A physical error 0.01 micrometer equals 0.00258 pixel by (pixel resolution 3.870 micrometer);
   The error 0.01 micrometer is inside 1% of their one standard deviation;
2  Even using ground truth to directly get R, the whole accuracy does not explicitly improve;
   Leixin told me before that smooth constraint does not explictly improve the whole accuracy;
   In order to sell your method, maybe it needs to choose some special cases from a special data set.
   Today's experiment verified Leixin's words from a separation constraint dimension.
3  In intuition, Learning R(Separation) may get lower accuracy than learning S(surface Locations) as S involve local regions only  while R involves cross regions.
   Our cost function f(S) + \lambda * f(R) tries to use R's information to adjust S, which looks that using lower accuracy information calibrates a higher accruacy information.
   Tomorrow, I am planning to try to use mathematic formulas derived from our cost function to verify above statement.


# experiment: expTongren_9Surfaces_SoftConst_20200821A_3Unet_CV0
# experiment config:
  A  3 Unets;
  B  Pretraind SurfaceSubnet with validation error 2.0, and prtrained SeparationSubnet with validation error 3.0;
  C  Pretrain LambdaSubnet on validation data with validation error 1.845 with IPM module;
  D  fine tune SurfaeSubnet and SeparationSubnet with IPM module, while fixing LambdaSubset, getting vadlidation error 1.9;
Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
20200821A_3Unet_CV0, CV0, 1.0622, 3.1820, 2.8789, 2.5810, 3.1813, 1.3911, 2.9961, 1.6142, 1.2184, 1.0553975105285645, 2.233919143676758,

Analysis:
1  This result with IPM module is in same level of 0317 result;
2  It means the lambda learning on validation also can generalize; while a possible problem is that validation set is too small with only 5 patients;
3  learning a Lambdda with a (N-1)xW size does not overkill this problem, as its training loss is about 6, while its validation loss is about 16 at the stable stage;
4  Lambda learning on the training data may be also a good idea, which is worth to try; Training data is 8 times of validation data, which should get better representation and learning;
5  Further improving the SeparationSubnet is also needed:
   A  refined Gaussian yesterday improved error stability without jump up at tail;
   B  SurfaceSubnet has gradient signal at its surface; While separationSubnet learns the separation between 2 surfaces, which has no directly gradient guide;
   C  Proper network architecture adjustment:
       C1. reduce layers of Unet as network has some overfiiting (training loss 6 vs validation loss 68);
       C2. add size of the after Unet module to grasp the relations among adjacent surfaces;





# experiment 20200813C on CV0
# config: Pretain Unet+ SurfaceConv+RiftConv with ReLU instead of IPM, the gradient of RiftConv does not go backward, and then use IPM to train;
Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,  epochs of saved network,
20200813C_CV0, CV0, 1.0738, 3.0038, 2.5419, 2.4632, 3.1692, 1.4311, 2.5503, 1.5731, 1.2229, 0.9549962878227234, 2.1143689155578613,  148(pretrain),
20200813C_CV0, CV0, 1.5937, 3.1403, 2.8097, 2.5230, 3.6935, 1.7414, 2.9564, 1.7157, 1.9480, 0.9750833511352539, 2.45796799659729,    199(IPM)

Analysis:
1   The accuracy of pretrained network is better than the previous best 0317 CV0 result, in both error and variance;
2   After pretraining, adding  IPM reduces accuracy, which reason may be that  the not good predicted R affected accuracy;
3   Please refer to randomly chosen 6 slices from 5 patients in test CV0 for visual R, in pretrain and IPM 2 network results, as attachment;
     A Even for the pretrained network with a good accuracy, its predicted R is not very accurate; which mean there are  some space to further improve accuracy;
     B The R's in both networks capture big pattern, but the detail is not enough;
     C Notes: our current network didn't apply smooth onto predicted R;
4  further improvements:
    A  Build a separate Unet for a better R prediction;
    B  add the number of layers of RiftConv module to further raise R's accuracy;


# experiment 20200806B experiments on test fold 0:
Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,  epochs of saved network,
20200806B_CV0_8Grad_Calibrate,                CV0, 1.0285, 3.0024, 2.6635, 2.4130, 3.1927, 1.4863, 2.8564, 1.6278, 1.2272, 0.9899391531944275, 2.166411876678467,  150,
20200806B_CV0_8Grad_MergeMuR,                 CV0, 1.0074, 3.2170, 2.7835, 2.5455, 3.0714, 1.4552, 3.0687, 1.6066, 1.2720, 1.058144450187683,  2.2252607345581055, 144,
20200806B_CV0_8Grad_MergeMuR_NoPretrain_ReLU, CV0, 1.1218, 3.4047, 2.9818, 2.9162, 3.2488, 1.5357, 2.8776, 1.9314, 1.5722, 0.9876338839530945, 2.398918628692627,  167,
20200806B_CV0_8Grad_MergeMuR_ReLU,            CV0, 1.0151, 2.9194, 2.6627, 2.4457, 3.0521, 1.3989, 2.5869, 1.6352, 1.2256, 0.9155402779579163, 2.104637622833252,  121,

Summary:
1  After using learning rate=0.01 instead of 0.0001, mean error got similar result with 20200317 9-surface experiment on CV0; 2.16 micrometer = 0.56 pixel error;
   surface 4 error is also in same level with 0317 experiment;
2  In above experiments, ReLU means use ReLU to replace IPM module;
   Calibrate is using learning Conv to merge surface and separation(rift) branches,
   while MergeMuR is using directly variance weighted formula to merge surface and rift branches;
3  mean errors 2.10-2.22 have no essential difference, I think their differences are decided by random conv parameter initialization;
4  Further improvement:
   A  Learning pairwise terms weight is a good idea, which is a better selling point than directly variance formula.
   B  Use learning pairwise terms weight to replace above MergeMuR and Calibrate module;






#experiment on 20200805 and 2020806:
Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,  epochs of saved network,
20200805_CV0_8Grad_NoCalibrate,      CV0, 1.2319, 3.7467, 4.6088, 3.8662, 4.2378, 2.5786, 3.3929, 2.7176, 2.3010, 1.2838613986968994, 3.186833143234253, 117,
20200805_CV0_8Grad_Calibrate,        CV0, 1.0082, 3.3101, 3.8892, 3.6267, 4.1405, 1.8832, 2.6409, 1.7958, 1.7094, 1.2928392887115479, 2.6671106815338135,  115,
20200806_CV0_8Grad_MergeMuR_SoftIPM, CV0, 1.0005, 3.4731, 3.9608, 3.7947, 4.3676, 2.0261, 3.0408, 1.9843, 1.3892, 1.3145209550857544, 2.7819037437438965, 128,
20200806_CV0_8Grad_MergeMuR_ReLU,    CV0, 2.1571, 4.3067, 4.7070, 3.8894, 4.5863, 2.6521, 3.3053, 2.4307, 1.9068, 1.3382095098495483, 3.326826810836792, 131,

Summary:
1 After using bothside gradient aproximated formula, the error on surface 4 reduced; 20200317 experiment also showed surfacce 4 in CV0 has biggest error;
  please refer 2 volume  result as attachments.
  4511-OD in CV0 test set actually is a disease case, which is rare in our whole health data set.
2 Now the best error result is at about 2.7, about 0.7 pixel;
3 Further improvement to match 2020317 accuracy:
  A  increase learning rate to 0.01 in pretraining network; bigger learning rate may cross over some small local minima;
     current our initial learning rate is 0.0001 as our Rift Module has a linear layer, while our 20300317 experiment used an initial learning rate 0.01;




# experiment 20200804  with pretrain Unet 100 epochs, Not finished all trianing.
# Stop continue to training, as I found 100 epochs is not enough for pretrain.
Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,  epochs of saved network,
20200804_CV0_4Grad_Calibrate,   CV0, 2.1203, 3.7660, 4.0184, 3.8653, 3.9619, 2.5890, 3.1513, 2.3209, 2.1960, 1.089045524597168, 3.1098978519439697, 145,
20200804_CV0_4Grad_NoCalibrate, CV0, 1.0987, 3.3781, 3.7961, 3.3487, 3.8930, 2.6478, 3.0714, 2.6476, 2.2043, 1.1317063570022583, 2.8984198570251465, 99,
20200804_CV0_8Grad_Calibrate,   CV0, 1.0767, 3.2357, 3.5840, 3.3257, 3.6338, 1.8984, 2.8446, 2.1110, 1.7174, 1.1122558116912842, 2.6030385494232178,  99,
20200804_CV0_8Grad_NoCalibrate, CV0, 1.3719, 3.7417, 3.6301, 3.1433, 3.1354, 1.9387, 3.1761, 2.5851, 2.2167, 1.0072109699249268, 2.7710072994232178,  84,


# all below experiments with calibrate module have error on R_sigma2. it is a typo logic error.
# 4 experiments of 20200801, 9 surfaces

Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean, 
20200727_CV0_NoPairWeight_NoSmoothR,CV0,1.0783, 3.4766, 5.1293, 3.0762, 4.3432, 1.6945, 3.6113, 2.0895, 1.4815, 1.4384905099868774, 2.886721611022949,
20200801_CV0_baseline,             CV0, 1.4789, 3.6746, 3.7277, 3.5032, 3.7350, 1.8910, 3.0305, 2.1778, 1.7048, 1.0902187824249268, 2.7692790031433105,
20200801_CV0_pairweight_NoSmoothR, CV0, 1.5752, 3.7907, 3.3962, 3.1296, 6.7390, 2.3973, 3.6560, 2.8751, 1.5040, 1.6278992891311646, 3.2292423248291016,
20200801_CV0_NoPairweight_smoothR, CV0, 2.6533, 3.6903, 4.0416, 3.0337, 7.4564, 2.5187, 3.1922, 3.0804, 2.9461, 1.6918044090270996, 3.623622179031372,
20200801_CV0_pairweight_smoothR,   CV0, 1.0003, 3.4893, 2.9682, 3.0585, 6.9987, 1.7902, 3.1088, 2.0272, 1.6026, 1.763200283050537,  2.893749237060547,




# July 27th, 2020: smooth R test result with calibrate module, 9 Surfaces
Mean,,,,,,,,,,,,,
Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
CV0, 1.6727, 4.9475, 3.6303, 3.1904, 3.7603, 1.7515, 3.5944, 1.9640, 1.830,  1.3126462697982788, 2.926811933517456,
CV1, 1.3512, 3.3299, 5.5761, 4.6254, 6.2051, 1.8498, 3.3311, 1.8883, 1.6756, 1.8573707342147827, 3.314743995666504,
CV2, 
CV3, 
CV4,
CV5,
CV6,
CV7,
CV8,
CV9,
Avg,,,,,,,,,,,,,

# July 27th, 2020: smooth R test result without calibrate module, 9 Surfaces
Mean,,,,,,,,,,,,,
Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
CV0, 1.0783, 3.4766, 5.1293, 3.0762, 4.3432, 1.6945, 3.6113, 2.0895, 1.4815, 1.4384905099868774, 2.886721611022949,
CV1, 1.0746, 3.3951, 6.5350, 4.7573, 5.6961, 1.3269, 2.8676, 1.7422, 1.2510, 2.0658605098724365, 3.182877779006958
CV2, 
CV3, 
CV4,
CV5,
CV6,
CV7,
CV8,
CV9,
Avg,,,,,,,,,,,,,


# July 23th, 2020: Unsmooth R test result:
Mean,,,,,,,,,,,,,
Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, Surface9, totalStdDeviation, TotalMean,
CV0, 2.0657,  2.9303,  5.0221,  3.3406,  3.5618, 21.8507,  1.4649,  3.1653, 1.9080,  1.8951, 5.880496978759766, 4.720462799072266,
CV1, 1.3910,  3.3992, 13.2400,  3.2026,  4.0714,  4.4664,  1.3907,  2.6394, 1.8954,  1.4324, 3.606452465057373, 3.7128543853759766,
CV2, 2.2907,  3.3303,  4.6102,  2.9819,  2.7429,  3.5374,  2.3148,  7.1827, 3.8949,  3.3476, 1.688064455986023, 3.6233389377593994,
CV3, 1.1070,  2.9585,  4.0373,  2.4108,  2.4722, 21.0884,  1.3758,  2.1098, 1.3135,  1.3856, 5.857232093811035, 4.025874614715576
CV4,
CV5,
CV6,
CV7,
CV8,
CV9,
Avg,,,,,,,,,,,,,

# Tongren 9 Surfaces 20200317 Result.
Mean,
Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
CV0,1.0277, 2.9717, 2.8991, 2.5270, 3.1183, 1.5009, 2.5892, 1.5466, 1.2636, 0.9881571531295776,  2.160457134246826,
CV1,0.9242, 2.8337, 2.3255, 2.3878, 2.9070, 1.2594, 2.7502, 1.6587, 1.1253, 0.8148196339607239, 2.019092559814453,
CV2,1.1448, 3.1013, 2.7401, 2.5027, 2.9838, 1.3249, 2.6064, 1.6657, 1.2819, 0.8666604161262512, 2.15016770362854,
CV3,1.0444, 3.1325, 2.7502, 2.1961, 2.4697, 2.5186, 3.7787, 1.6613, 1.3668, 1.538432240486145, 2.3242485523223877,
CV4,0.9338, 3.3748, 2.8589, 2.6442, 2.9990, 1.5227, 2.6696, 1.4614, 1.2721, 0.9579789638519287, 2.192950963973999,
CV5,0.9336, 2.9447, 2.2988, 2.2961, 2.4235, 1.2243, 2.4619, 1.4099, 1.1992, 0.7368082404136658, 1.910221815109253,
CV6,1.0042, 2.6217, 2.3991, 2.1859, 2.5918, 1.3071, 2.8253, 1.5100, 1.1774, 0.7464631795883179, 1.958055019378662,
CV7,0.9936, 2.7479, 2.6015, 2.4974, 2.6378, 1.2698, 2.8727, 1.3831, 1.1645, 0.8926709890365601, 2.0186996459960938,
CV8,0.9259, 3.0914, 2.4603, 2.2445, 2.4688, 1.2057, 2.6317, 1.4208, 1.1207, 0.772925853729248, 1.9521963596343994,
CV9,0.9199, 3.0723, 2.5827, 2.3841, 2.4552, 1.2489, 3.0223, 1.6287, 1.1552, 0.8183929920196533, 2.0521445274353027,
Avg,0.98521,2.9892,2.59162,2.38658,2.70549,1.43823,2.8208, 1.53462,1.21267,0.913330966234207,2.07382342815399,

# May 12th, All good scan test result:
Mean,,,,,,,,,,,,,
Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, Surface9, totalStdDeviation, TotalMean,
CV0,0.965,2.5134,4.2586,2.3608,2.148,2.7457,1.3134,2.5419,1.5718,1.2211,1.02833092212677,2.16396355628967,
CV1,1.1703,2.9513,4.9718,3.2939,3.2564,3.6315,1.3674,2.2946,1.955,1.3673,1.31670713424683,2.62594127655029,
CV2,1.1424,2.9867,4.4777,2.717,2.7619,3.3026,1.4408,3.1698,1.4689,1.2865,1.18448543548584,2.47543287277222,
CV3,1.162,2.8031,4.2937,2.2719,2.1595,2.2279,1.1511,2.2538,1.4574,1.12,1.01635825634003,2.09003400802612,
CV4,0.8863,2.8751,5.1499,2.2673,2.2492,2.5261,1.2343,2.408,1.5256,1.2191,1.2550460100174,2.23409557342529,
CV5,0.97,2.6604,3.9583,1.9881,2.0328,2.3534,1.1675,2.522,1.4325,1.175,0.925507307052612,2.02600932121277,
CV6,0.8866,2.4064,4.6916,2.5252,2.6391,3.4617,1.2501,2.8816,1.4754,1.0636,1.40518307685852,2.3281147480011,
CV7,1.097,2.417,4.6682,2.1009,2.0137,2.666,1.1524,2.7574,1.4081,1.0447,1.10369324684143,2.13252139091492,
CV8,1.0136,2.6136,4.542,2.094,2.201,2.447,1.179,2.2493,1.4948,1.277,1.13219571113586,2.11113023757935,
CV9,1.0525,2.8498,3.896,2.1859,2.5326,2.7448,1.4233,2.9006,1.6597,1.2182,1.02195656299591,2.24634647369385, 
Avg,,,,,,,,,,,,,

