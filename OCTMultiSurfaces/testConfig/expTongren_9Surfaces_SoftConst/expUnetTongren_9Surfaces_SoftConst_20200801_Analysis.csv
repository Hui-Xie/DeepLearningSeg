# experiment expTongren_9Surfaces_SoftConst_20200827A_3Unet_CV0
# config:
  1  add lambda into (0, 200) range from previous (0,1)
experiment_Name, Error(micrometer),        Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
20200821A_3Unet_CV0,                  CV0, 1.0622, 3.1820, 2.8789, 2.5810, 3.1813, 1.3911, 2.9961, 1.6142, 1.2184, 1.0553975105285645, 2.233919143676758,  lambda \in (0,1), learning R;
20200827A_3Unet_CV0,                  CV0, 0.9885, 3.2768, 2.9824, 2.6563, 3.3461, 1.4610, 3.0790, 1.6235, 1.2132, 1.1312294006347656, 2.291862964630127,  lambda \in (0,200), learning R;
20200827A_3Unet_CV0_RUseNoSmoothedGT, CV0, 0.9885, 3.2768, 2.9825, 2.6563, 3.3462, 1.4610, 3.0789, 1.6235, 1.2132, 1.1312602758407593, 2.291879415512085, lambda \in (0,200), NoSmoothedGTasR;
20200827A_3Unet_CV0_RUsesmoothedGT,   CV0, 0.9885, 3.2766, 2.9825, 2.6563, 3.3461, 1.4610, 3.0791, 1.6235, 1.2132, 1.1312209367752075, 2.2918736934661865, lambda \in (0,200), SmoothedGTasR;
Analysis:
1

# sigma2 analysis along different surfaces on August 26th, 2020
python3.7 ./SurfaceSubnet_Test.py ./testConfig/expTongren_9Surfaces_SoftConst_20200819A_SurfaceSubnet_CV0.yaml

sigma2.shape = torch.Size([155, 9, 512])
mean of sigma2 = tensor([0.1589, 0.2499, 2.3791, 2.1644, 0.4526, 0.3018, 5.4519, 1.6994, 0.1721],  device='cuda:1')
1.0/mean = [6.2933, 4.0016, 0.4203, 0.4620, 2.2095, 3.3135, 0.1834, 0.5884, 5.8106]

min of sigma2  = torch.return_types.min(
values=tensor([0.0129, 0.0618, 0.7259, 0.7362, 0.0833, 0.1022, 0.7014, 0.6878, 0.0182], device='cuda:1'),
indices=tensor([187, 193, 131,  85,  58, 223,  44, 108,  15], device='cuda:1'))
1.0/min = [77.5194, 16.1812,  1.3776,  1.3583, 12.0048,  9.7847,  1.4257,  1.4539, 54.9451]

max of sigma2  = torch.return_types.max(
values=tensor([ 42.1497, 177.9110,  37.7500,  35.5107,  33.8680,  63.1024,  71.3775, 47.5299,  17.0574], device='cuda:1'),
indices=tensor([  0,   0,   0,   0,   0, 511, 481, 205, 208], device='cuda:1'))
1/max = [0.0237, 0.0056, 0.0265, 0.0282, 0.0295, 0.0158, 0.0140, 0.0210, 0.0586]

Analysis:
1   1/sigma2 is range [0, 200]

===========================================================


# Measure Hausdorff distance of all 9 surface for different network results: On August 26th, 2020
 Surface Index,                         0,  1,  2,  3,  4,  5,  6,  7,  8,  Max,
 3Unet_Network_with_learning_R,         7, 13, 10,  8, 10,  9,  8,  8, 12,  13,
 3Unet_Use_SmoothedGT_to_replace_R,     7, 13, 10,  8, 10,  9,  8,  7, 12,  13,
 3Unet_Use_UnsmoothedGT_to_replace_R,   7, 13, 10,  8, 10,  9,  8,  8, 12,  13,
 1Unet_Only_unary_term,                 4, 12, 11,  8, 10,  9,  7,  4,  5,  12,

 Analysis:
 1  This Hausdorff distance measures the max absolute segmentation deviation between ground truth and prediction for all 5 test volumes in CV0;
 2  Hausdorff distance is in pixel unit above;
 3  Introducing R increases Hausdorff distance overall;
 4  Introducing R increases the Hausdorff distance of surface 0 and surface 8 which are the most easy segmented surfaces,
    which means R acts as a disturbance to the original unary prediction mu in worse cases;
 5  It will be really helpful to further analyze reason behind this from the cost function form;


# experiment: use ground truth of R to replace predicted R:
Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
20200821A_3Unet_CV0,                  CV0, 1.0622, 3.1820, 2.8789, 2.5810, 3.1813, 1.3911, 2.9961, 1.6142, 1.2184, 1.0553975105285645, 2.233919143676758,  3Unet Network, with learning R;
20200821A_3Unet_CV0_RUseSmoothedGT,   CV0, 1.0627, 3.1763, 2.8555, 2.5546, 3.1697, 1.3900, 2.9056, 1.5939, 1.2182, 1.0365197658538818, 2.2140555381774902, 3Unet, Use SmoothedGT to replace R;
20200821A_3Unet_CV0_RUseNoSmoothedGT, CV0, 1.0623, 3.1761, 2.8482, 2.5502, 3.1691, 1.3877, 2.8932, 1.5866, 1.2175, 1.0357218980789185, 2.210094928741455,  3Unet, use UnsmoothedGT to replace R;
SoftConst_20200819A_SurfaceSubnet,    CV0, 1.0055, 3.1687, 2.8236, 2.5908, 3.1875, 1.3984, 2.9978, 1.5677, 1.2584, 1.0588494539260864, 2.2220582962036133, 1Unet, only unary item

Analysis:
1  The above 4 results have no essential difference as network computes pixel locations;
   A physical error 0.01 micrometer equals 0.00258 pixel by (pixel resolution 3.870 micrometer);
   The error 0.01 micrometer is inside 1% of their one standard deviation;
2  Even using ground truth to directly get R, the whole accuracy does not explicitly improve;
   Leixin told me before that smooth constraint does not explictly improve the whole accuracy;
   In order to sell your method, maybe it needs to choose some special cases from a special data set.
   Today's experiment verified Leixin's words from a separation constraint dimension.
3  In intuition, Learning R(Separation) may get lower accuracy than learning S(surface Locations) as S involve local regions only  while R involves cross regions.
   Our cost function f(S) + \lambda * f(R) tries to use R's information to adjust S, which looks that using lower accuracy information calibrates a higher accruacy information.
   Tomorrow, I am planning to try to use mathematic formulas derived from our cost function to verify above statement.


# experiment: expTongren_9Surfaces_SoftConst_20200821A_3Unet_CV0
# experiment config:
  A  3 Unets;
  B  Pretraind SurfaceSubnet with validation error 2.0, and prtrained SeparationSubnet with validation error 3.0;
  C  Pretrain LambdaSubnet on validation data with validation error 1.845 with IPM module;
  D  fine tune SurfaeSubnet and SeparationSubnet with IPM module, while fixing LambdaSubset, getting vadlidation error 1.9;
Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
20200821A_3Unet_CV0, CV0, 1.0622, 3.1820, 2.8789, 2.5810, 3.1813, 1.3911, 2.9961, 1.6142, 1.2184, 1.0553975105285645, 2.233919143676758,

Analysis:
1  This result with IPM module is in same level of 0317 result;
2  It means the lambda learning on validation also can generalize; while a possible problem is that validation set is too small with only 5 patients;
3  learning a Lambdda with a (N-1)xW size does not overkill this problem, as its training loss is about 6, while its validation loss is about 16 at the stable stage;
4  Lambda learning on the training data may be also a good idea, which is worth to try; Training data is 8 times of validation data, which should get better representation and learning;
5  Further improving the SeparationSubnet is also needed:
   A  refined Gaussian yesterday improved error stability without jump up at tail;
   B  SurfaceSubnet has gradient signal at its surface; While separationSubnet learns the separation between 2 surfaces, which has no directly gradient guide;
   C  Proper network architecture adjustment:
       C1. reduce layers of Unet as network has some overfiiting (training loss 6 vs validation loss 68);
       C2. add size of the after Unet module to grasp the relations among adjacent surfaces;





# experiment 20200813C on CV0
# config: Pretain Unet+ SurfaceConv+RiftConv with ReLU instead of IPM, the gradient of RiftConv does not go backward, and then use IPM to train;
Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,  epochs of saved network,
20200813C_CV0, CV0, 1.0738, 3.0038, 2.5419, 2.4632, 3.1692, 1.4311, 2.5503, 1.5731, 1.2229, 0.9549962878227234, 2.1143689155578613,  148(pretrain),
20200813C_CV0, CV0, 1.5937, 3.1403, 2.8097, 2.5230, 3.6935, 1.7414, 2.9564, 1.7157, 1.9480, 0.9750833511352539, 2.45796799659729,    199(IPM)

Analysis:
1   The accuracy of pretrained network is better than the previous best 0317 CV0 result, in both error and variance;
2   After pretraining, adding  IPM reduces accuracy, which reason may be that  the not good predicted R affected accuracy;
3   Please refer to randomly chosen 6 slices from 5 patients in test CV0 for visual R, in pretrain and IPM 2 network results, as attachment;
     A Even for the pretrained network with a good accuracy, its predicted R is not very accurate; which mean there are  some space to further improve accuracy;
     B The R's in both networks capture big pattern, but the detail is not enough;
     C Notes: our current network didn't apply smooth onto predicted R;
4  further improvements:
    A  Build a separate Unet for a better R prediction;
    B  add the number of layers of RiftConv module to further raise R's accuracy;


# experiment 20200806B experiments on test fold 0:
Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,  epochs of saved network,
20200806B_CV0_8Grad_Calibrate,                CV0, 1.0285, 3.0024, 2.6635, 2.4130, 3.1927, 1.4863, 2.8564, 1.6278, 1.2272, 0.9899391531944275, 2.166411876678467,  150,
20200806B_CV0_8Grad_MergeMuR,                 CV0, 1.0074, 3.2170, 2.7835, 2.5455, 3.0714, 1.4552, 3.0687, 1.6066, 1.2720, 1.058144450187683,  2.2252607345581055, 144,
20200806B_CV0_8Grad_MergeMuR_NoPretrain_ReLU, CV0, 1.1218, 3.4047, 2.9818, 2.9162, 3.2488, 1.5357, 2.8776, 1.9314, 1.5722, 0.9876338839530945, 2.398918628692627,  167,
20200806B_CV0_8Grad_MergeMuR_ReLU,            CV0, 1.0151, 2.9194, 2.6627, 2.4457, 3.0521, 1.3989, 2.5869, 1.6352, 1.2256, 0.9155402779579163, 2.104637622833252,  121,

Summary:
1  After using learning rate=0.01 instead of 0.0001, mean error got similar result with 20200317 9-surface experiment on CV0; 2.16 micrometer = 0.56 pixel error;
   surface 4 error is also in same level with 0317 experiment;
2  In above experiments, ReLU means use ReLU to replace IPM module;
   Calibrate is using learning Conv to merge surface and separation(rift) branches,
   while MergeMuR is using directly variance weighted formula to merge surface and rift branches;
3  mean errors 2.10-2.22 have no essential difference, I think their differences are decided by random conv parameter initialization;
4  Further improvement:
   A  Learning pairwise terms weight is a good idea, which is a better selling point than directly variance formula.
   B  Use learning pairwise terms weight to replace above MergeMuR and Calibrate module;






#experiment on 20200805 and 2020806:
Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,  epochs of saved network,
20200805_CV0_8Grad_NoCalibrate,      CV0, 1.2319, 3.7467, 4.6088, 3.8662, 4.2378, 2.5786, 3.3929, 2.7176, 2.3010, 1.2838613986968994, 3.186833143234253, 117,
20200805_CV0_8Grad_Calibrate,        CV0, 1.0082, 3.3101, 3.8892, 3.6267, 4.1405, 1.8832, 2.6409, 1.7958, 1.7094, 1.2928392887115479, 2.6671106815338135,  115,
20200806_CV0_8Grad_MergeMuR_SoftIPM, CV0, 1.0005, 3.4731, 3.9608, 3.7947, 4.3676, 2.0261, 3.0408, 1.9843, 1.3892, 1.3145209550857544, 2.7819037437438965, 128,
20200806_CV0_8Grad_MergeMuR_ReLU,    CV0, 2.1571, 4.3067, 4.7070, 3.8894, 4.5863, 2.6521, 3.3053, 2.4307, 1.9068, 1.3382095098495483, 3.326826810836792, 131,

Summary:
1 After using bothside gradient aproximated formula, the error on surface 4 reduced; 20200317 experiment also showed surfacce 4 in CV0 has biggest error;
  please refer 2 volume  result as attachments.
  4511-OD in CV0 test set actually is a disease case, which is rare in our whole health data set.
2 Now the best error result is at about 2.7, about 0.7 pixel;
3 Further improvement to match 2020317 accuracy:
  A  increase learning rate to 0.01 in pretraining network; bigger learning rate may cross over some small local minima;
     current our initial learning rate is 0.0001 as our Rift Module has a linear layer, while our 20300317 experiment used an initial learning rate 0.01;




# experiment 20200804  with pretrain Unet 100 epochs, Not finished all trianing.
# Stop continue to training, as I found 100 epochs is not enough for pretrain.
Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,  epochs of saved network,
20200804_CV0_4Grad_Calibrate,   CV0, 2.1203, 3.7660, 4.0184, 3.8653, 3.9619, 2.5890, 3.1513, 2.3209, 2.1960, 1.089045524597168, 3.1098978519439697, 145,
20200804_CV0_4Grad_NoCalibrate, CV0, 1.0987, 3.3781, 3.7961, 3.3487, 3.8930, 2.6478, 3.0714, 2.6476, 2.2043, 1.1317063570022583, 2.8984198570251465, 99,
20200804_CV0_8Grad_Calibrate,   CV0, 1.0767, 3.2357, 3.5840, 3.3257, 3.6338, 1.8984, 2.8446, 2.1110, 1.7174, 1.1122558116912842, 2.6030385494232178,  99,
20200804_CV0_8Grad_NoCalibrate, CV0, 1.3719, 3.7417, 3.6301, 3.1433, 3.1354, 1.9387, 3.1761, 2.5851, 2.2167, 1.0072109699249268, 2.7710072994232178,  84,


# all below experiments with calibrate module have error on R_sigma2. it is a typo logic error.
# 4 experiments of 20200801, 9 surfaces

Experiment_Name, Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean, 
20200727_CV0_NoPairWeight_NoSmoothR,CV0,1.0783, 3.4766, 5.1293, 3.0762, 4.3432, 1.6945, 3.6113, 2.0895, 1.4815, 1.4384905099868774, 2.886721611022949,
20200801_CV0_baseline,             CV0, 1.4789, 3.6746, 3.7277, 3.5032, 3.7350, 1.8910, 3.0305, 2.1778, 1.7048, 1.0902187824249268, 2.7692790031433105,
20200801_CV0_pairweight_NoSmoothR, CV0, 1.5752, 3.7907, 3.3962, 3.1296, 6.7390, 2.3973, 3.6560, 2.8751, 1.5040, 1.6278992891311646, 3.2292423248291016,
20200801_CV0_NoPairweight_smoothR, CV0, 2.6533, 3.6903, 4.0416, 3.0337, 7.4564, 2.5187, 3.1922, 3.0804, 2.9461, 1.6918044090270996, 3.623622179031372,
20200801_CV0_pairweight_smoothR,   CV0, 1.0003, 3.4893, 2.9682, 3.0585, 6.9987, 1.7902, 3.1088, 2.0272, 1.6026, 1.763200283050537,  2.893749237060547,




# July 27th, 2020: smooth R test result with calibrate module, 9 Surfaces
Mean,,,,,,,,,,,,,
Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
CV0, 1.6727, 4.9475, 3.6303, 3.1904, 3.7603, 1.7515, 3.5944, 1.9640, 1.830,  1.3126462697982788, 2.926811933517456,
CV1, 1.3512, 3.3299, 5.5761, 4.6254, 6.2051, 1.8498, 3.3311, 1.8883, 1.6756, 1.8573707342147827, 3.314743995666504,
CV2, 
CV3, 
CV4,
CV5,
CV6,
CV7,
CV8,
CV9,
Avg,,,,,,,,,,,,,

# July 27th, 2020: smooth R test result without calibrate module, 9 Surfaces
Mean,,,,,,,,,,,,,
Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
CV0, 1.0783, 3.4766, 5.1293, 3.0762, 4.3432, 1.6945, 3.6113, 2.0895, 1.4815, 1.4384905099868774, 2.886721611022949,
CV1, 1.0746, 3.3951, 6.5350, 4.7573, 5.6961, 1.3269, 2.8676, 1.7422, 1.2510, 2.0658605098724365, 3.182877779006958
CV2, 
CV3, 
CV4,
CV5,
CV6,
CV7,
CV8,
CV9,
Avg,,,,,,,,,,,,,


# July 23th, 2020: Unsmooth R test result:
Mean,,,,,,,,,,,,,
Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, Surface9, totalStdDeviation, TotalMean,
CV0, 2.0657,  2.9303,  5.0221,  3.3406,  3.5618, 21.8507,  1.4649,  3.1653, 1.9080,  1.8951, 5.880496978759766, 4.720462799072266,
CV1, 1.3910,  3.3992, 13.2400,  3.2026,  4.0714,  4.4664,  1.3907,  2.6394, 1.8954,  1.4324, 3.606452465057373, 3.7128543853759766,
CV2, 2.2907,  3.3303,  4.6102,  2.9819,  2.7429,  3.5374,  2.3148,  7.1827, 3.8949,  3.3476, 1.688064455986023, 3.6233389377593994,
CV3, 1.1070,  2.9585,  4.0373,  2.4108,  2.4722, 21.0884,  1.3758,  2.1098, 1.3135,  1.3856, 5.857232093811035, 4.025874614715576
CV4,
CV5,
CV6,
CV7,
CV8,
CV9,
Avg,,,,,,,,,,,,,

# Tongren 9 Surfaces 20200317 Result.
Mean,
Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, totalStdDeviation, TotalMean,
CV0,1.0277, 2.9717, 2.8991, 2.5270, 3.1183, 1.5009, 2.5892, 1.5466, 1.2636, 0.9881571531295776,  2.160457134246826,
CV1,0.9242, 2.8337, 2.3255, 2.3878, 2.9070, 1.2594, 2.7502, 1.6587, 1.1253, 0.8148196339607239, 2.019092559814453,
CV2,1.1448, 3.1013, 2.7401, 2.5027, 2.9838, 1.3249, 2.6064, 1.6657, 1.2819, 0.8666604161262512, 2.15016770362854,
CV3,1.0444, 3.1325, 2.7502, 2.1961, 2.4697, 2.5186, 3.7787, 1.6613, 1.3668, 1.538432240486145, 2.3242485523223877,
CV4,0.9338, 3.3748, 2.8589, 2.6442, 2.9990, 1.5227, 2.6696, 1.4614, 1.2721, 0.9579789638519287, 2.192950963973999,
CV5,0.9336, 2.9447, 2.2988, 2.2961, 2.4235, 1.2243, 2.4619, 1.4099, 1.1992, 0.7368082404136658, 1.910221815109253,
CV6,1.0042, 2.6217, 2.3991, 2.1859, 2.5918, 1.3071, 2.8253, 1.5100, 1.1774, 0.7464631795883179, 1.958055019378662,
CV7,0.9936, 2.7479, 2.6015, 2.4974, 2.6378, 1.2698, 2.8727, 1.3831, 1.1645, 0.8926709890365601, 2.0186996459960938,
CV8,0.9259, 3.0914, 2.4603, 2.2445, 2.4688, 1.2057, 2.6317, 1.4208, 1.1207, 0.772925853729248, 1.9521963596343994,
CV9,0.9199, 3.0723, 2.5827, 2.3841, 2.4552, 1.2489, 3.0223, 1.6287, 1.1552, 0.8183929920196533, 2.0521445274353027,
Avg,0.98521,2.9892,2.59162,2.38658,2.70549,1.43823,2.8208, 1.53462,1.21267,0.913330966234207,2.07382342815399,

# May 12th, All good scan test result:
Mean,,,,,,,,,,,,,
Error(micrometer), Surface0, Surface1, Surface2, Surface3,Surface4, Surface5, Surface6, Surface7, Surface8, Surface9, totalStdDeviation, TotalMean,
CV0,0.965,2.5134,4.2586,2.3608,2.148,2.7457,1.3134,2.5419,1.5718,1.2211,1.02833092212677,2.16396355628967,
CV1,1.1703,2.9513,4.9718,3.2939,3.2564,3.6315,1.3674,2.2946,1.955,1.3673,1.31670713424683,2.62594127655029,
CV2,1.1424,2.9867,4.4777,2.717,2.7619,3.3026,1.4408,3.1698,1.4689,1.2865,1.18448543548584,2.47543287277222,
CV3,1.162,2.8031,4.2937,2.2719,2.1595,2.2279,1.1511,2.2538,1.4574,1.12,1.01635825634003,2.09003400802612,
CV4,0.8863,2.8751,5.1499,2.2673,2.2492,2.5261,1.2343,2.408,1.5256,1.2191,1.2550460100174,2.23409557342529,
CV5,0.97,2.6604,3.9583,1.9881,2.0328,2.3534,1.1675,2.522,1.4325,1.175,0.925507307052612,2.02600932121277,
CV6,0.8866,2.4064,4.6916,2.5252,2.6391,3.4617,1.2501,2.8816,1.4754,1.0636,1.40518307685852,2.3281147480011,
CV7,1.097,2.417,4.6682,2.1009,2.0137,2.666,1.1524,2.7574,1.4081,1.0447,1.10369324684143,2.13252139091492,
CV8,1.0136,2.6136,4.542,2.094,2.201,2.447,1.179,2.2493,1.4948,1.277,1.13219571113586,2.11113023757935,
CV9,1.0525,2.8498,3.896,2.1859,2.5326,2.7448,1.4233,2.9006,1.6597,1.2182,1.02195656299591,2.24634647369385, 
Avg,,,,,,,,,,,,,

