# train and test JHU OCT data
# Use basic Unet + KLDLoss for OCT multisurface
# Unet with a residual link inside each block + Gaussian Ground truth for multi-surface + KLDivLoss + Soft argMax + SmoothL1Loss
# first train with KLDivLoss 1000 epochs, then train SmoothL1Loss 500 epochs.
dataDir: "/home/hxie1/data/OCT_JHU/numpy"
# log will save at: dataDir + "/log/" + network + "/" + experimentName
K_Folds: 0
fold_k:  -1   # -1 means do not use cross validation
# when use cross validation:
# training data:  dataDir + training + images_CV{k}.npy
#         label: dataDir + training + surfaces_CV{k}.npy
#         IDs  : dataDir + training + patientID_CV{k}.json

# validation data:  dataDir + validation + images_CV{k}.npy
#           label: dataDir + validation + surfaces_CV{k}.npy
#           IDs  : dataDir + validation + patientID_CV{k}.json

# test data:  dataDir + test + images_CV{k}.npy
#     label: dataDir + test + surfaces_CV{k}.npy
#     IDs  : dataDir + test + patientID_CV{k}.json

# when do not use cross validation:
# training data:  dataDir + training + images.npy
#         label: dataDir + training + surfaces.npy
#         IDs  : dataDir + training + patientID.json

# validation data:  dataDir + validation + images.npy
#           label: dataDir + validation + surfaces.npy
#           IDs  : dataDir + validation + patientID.json

# test data:  dataDir + test + images.npy
#     label: dataDir + test + surfaces.npy
#     IDs  : dataDir + test + patientID.json

groundTruthInteger: False
numSurfaces: 9  # num of surfaces in an image
sigma: 8.0  # For gaussian  ground truth, in float
device: torch.device('cuda:2')   #GPU ID
batchSize: 4
startFilters: 24  # the num of filters in first layer of Unet

# some physical parameter of images
slicesPerPatient: 49
hPixelSize: 3.9  # unit: micrometer, in y/height direction

# data augmentation
augmentProb: 0.4  #  data augmentation rate
gaussianNoiseStd: 0.1 # gausssian nosie std with mean =0
# for salt-pepper noise
saltPepperRate: 0.05  # rate = (salt+pepper)/allPixels
saltRate: 0.5  # saltRate = salt/(salt+pepper)
# data augmentation

network: "OCTUnetJHU"
netPath: "/home/hxie1/data/OCT_JHU/numpy/netParameters"  # net is saved at netpath / network / self_filename
# if loadNetPath !="", netPath will be replaced by loadNetPath
# below loadNetPath is copied from "/home/hxie1/data/OCT_JHU/numpy/netParameters/OCTUnetJHU/expUnetJHU_20200130" on 20200201 17:40
loadNetPath:  "/home/hxie1/data/OCT_JHU/numpy/netParameters/OCTUnetJHU/expUnetJHU_DP_20200130"  # this is pretrained network
outputDir: ""


# use loadNetpath, below loss function may need revise.
lossFunc0: "nn.KLDivLoss(reduction='batchmean').to(device)"   # the input given is expected to contain log-probabilities
lossFunc0Epochs: 1000   # the epoch number of using lossFunc0
lossFunc1: "nn.SmoothL1Loss().to(device)"
lossFunc1Epochs: 500

# Proximal IPM Optimization
useProxialIPM: False  # Flase means use gauranteeSurfaceOrder module.
learningStepIPM: 0.1
maxIterationIPM: 100
criterionIPM: 0.1 # the criterion of average difference of 2 adjacent iterations in IPM: less than this value, IPM exits.

useDynamicProgramming: True

usePrimalDualIPM: False


