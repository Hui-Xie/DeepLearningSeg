# Oct 6th, 2020
# A  add mobileNet v3 weight initialization
# B  replace ReLU6 with HS in last 2 layer of mobileNet v3
# C  add conv2dFeatureNet architecture choice
# D  use a small dataset for training and validation

# Oct 7th, 2020
# use single middle slice as input for each patient
# dropout rate 0.7



debug: True

# train config
device: torch.device('cuda:1')   #GPU ID
batchSize: 31  # fixed batchsize, make sure at least 2 sample are negative.
learningRate: 0.1

#data source
existGTLabel: True
dataDir:  "/home/hxie1/data/BES_3K/W512MidSlices"

# _delNonExist: delete ID nonexist, and repeated ID;
# _final: delete the ID whose slice number does not equal to 31.
trainingDataPath: "/home/hxie1/data/BES_3K/GTs/trainID_delNonExist_final.csv"
validationDataPath: "/home/hxie1/data/BES_3K/GTs/validationID_delNonExist_final.csv"
testDataPath: "/home/hxie1/data/BES_3K/GTs/testID_delNonExist_final.csv"

GTPath: "/home/hxie1/data/BES_3K/GTs/BESClinicalGT_Analysis.csv"
ODOS: "OD"  # right(OD) or left(OS) eye


# Network config
network: "OCT2SysD_Net_A"
imageH: 496
imageW: 512
sliceSuffix: "_midSlice.npy"
bothEyes: False  # choose all OD eyes
inputChannels: 3 # rawImage + gradChannels(W,H)
gradChannels: 2   # the added gradient channels beside raw image channel
bothSideGrad: True # False use singleSideGrad

useGlobalMean: True   # True uses global mean, and False will use IQR+std
dropoutRate: 0.8 # dropout after global average pooling

classifierWidth: [512, 256, 1] # [0] element is better is the times of 62
hypertensionClassPercent: [0.4441, 0.5559] # for 2031 big training set
#hypertensionClassPercent: [0.36, 0.64] # for 200 small training set

featureNet: "MobileNetV3_OCT2SysD" #"Conv2DFeatureNet" #
nStartFilters: 16
nLayers: 7
outputChannels: 1024  # output feature channels before the classifier for moibleNet v3
                      # nStartFilter * (2**(nLayers-1)) = 16 *(2**(7-1))= 1024

# data augmentation
augmentation: True
augmentProb: 0  #  data augmentation rate
flipProb: 0.5
gaussianNoiseStd: 0.1 # gausssian nosie std with mean =0
# for salt pepper noise
saltPepperRate: 0.05  # rate = (salt+pepper)/allPixels
saltRate: 0.5  # saltRate = salt/(salt+pepper)

# save network
netPath: "/home/hxie1/data/BES_3K/netParameters"  # net is saved at netpath / network / self_filename
loadNetPath: ""
outputDir: ""
logDir: "/home/hxie1/data/BES_3K/log"  # net is saved at logPath / network / self_filename

# module config:
# Test-time augmentation:
# refer to AlexNet's idea,and ResNet also adapted this 10-crop TTA:
#  "At test time, the network makes a prediction by extracting five 224×224 patches
#  (the four corner patches and the center patch) as well as their horizontal reflections (hence ten patches in all),
#  and averaging the predictions made by the network’s softmax layer on the ten patches."

# GoogleNet 2014:
#  "The softmax probabilities are averaged over multiple crops and
#   over all the individual classifiers to obtain the final prediction."





# VGG use pretrained network:
#  One approach described involved first training a model with a fixed but smaller image size,
#  retaining the model weights, then using them as a starting point for training a new model
#  with a larger but still fixed-sized image. This approach was designed
#  in an effort to speed up the training of the larger (second) model.


# decision threshold is problem-dependent.

